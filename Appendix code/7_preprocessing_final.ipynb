{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import the language specific models \n",
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download pl_core_news_sm\n",
    "!python -m spacy download da_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "Set the working directory and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "os.chdir(r'C:\\Users\\maril\\Documents\\20-21 KU\\block 4\\DM\\twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Germany: import data\n",
    "de = pd.read_csv(r'final_data_prepare1\\de_original_text.csv')\n",
    "print(de.shape)\n",
    "de.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denmark: import data\n",
    "da = pd.read_csv('final_data_prepare1\\da_original_text.csv')\n",
    "print(da.shape)\n",
    "da.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poland: import data\n",
    "pl = pd.read_csv('final_data_prepare1\\pl_original_text.csv')\n",
    "print(pl.shape)\n",
    "pl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We create two slightly different preprocessed datasets: One that contains @mentions and a second one that does not contain mentions. This is because we are not entirely sure yet if we want to use the mentions or not for the topic modelling.\n",
    "  \n",
    "We conduct the following steps:\n",
    "\n",
    "* Remove **URLs**.\n",
    "* Remove **emojis** using the ``emoji`` package.\n",
    "* **For the dataset without @mentions: Remove the mentions.**\n",
    "* Remove ``&amp;`` (the HTML code for the **ampersand** symbol)\n",
    "* **Replace '-' by an empty string:** This is important to keep words together that belong together. E.g. the German 'Impf-Reihenfolge' should be merge into one word 'ImpfReihenfolge' in order to not distort it's meaning. This makes lemmatization more difficult esp. because 'ImpfReihenfolge' is not the correct spelling of the word (it should be 'Impfreihenfolge'), but since this will only affect a very small number of words, we deem it acceptable.\n",
    "* **Remove ':', '\\*' and *_*:** Again, this is mostly relevant for German. In German, nouns describing people (e.g. the word for 'doctor') usually come in a male ('Doktor') and female form ('Doktorin'). In recent years, there has been a movement to include both spelling as either 'Doktor_in', 'Doktor:in' or 'Doktor\\*in' in an attempt at more gender neutral language. If we replace these symbols by spaces, then we would distort the words meaning since 'Doktor in' is not the same as 'Doktorin'. We therefore just remove these symbols. This is not relevant for Polish or Danish, but since removing these symbols does not cause any other issue there, we do this for all three datasets.\n",
    "* Only **keep the remaining alphanumeric characters** (incl. ``#`` for hashtags and ``@`` for the dataset containing @mentions).\n",
    "* Remove **numbers**.\n",
    "* Remove **single characters.** They are not usually not particularly meaningful: In Polish and German, there are (meaningful) words that only consist of one character. In Danish, there is the 'I' (the plural 'you'; as in 'Hvordan har I det?'). But this character will be removed in the stopword list anyway, we might as well already remove it here.\n",
    "* Remove **double, triple etc. whitespaces**.\n",
    "* Remove **leading and trailing whitespaces**.\n",
    "\n",
    "**Important:** We do not lowercase words yet. In German, nouns are spelled with a capital first letter and the German lemmatizer does not work well on lowercased words (capitalization does not make a difference for the Polish and Danish lemmatizers). We therefore lowercase all words after we've conducted the lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess: keep @mentions\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\",\n",
    "                  ' ', text)\n",
    "    \n",
    "    # remove emojis: we use the 'emoji' package to do so\n",
    "    # the function .get_emoji_regexp() returns a regex pattern for all unicode emoji characters\n",
    "    # we use this pattern to match emojis and then replace them with a whitespace\n",
    "    text = re.sub(emoji.get_emoji_regexp(), ' ', text)\n",
    "    \n",
    "    # remove all '&amp;' (the HTML code for the ampersand symbol)\n",
    "    text = re.sub('&amp;', '', text)\n",
    "   \n",
    "    # replace '-' by an empty string\n",
    "    text = re.sub('-', '', text)\n",
    "   \n",
    "    # replace '_' by an empty string\n",
    "    text = re.sub('_', '', text)\n",
    " \n",
    "    # replace '*' by an empty string\n",
    "    text = re.sub('\\*', '', text)\n",
    " \n",
    "    # replace ':' by an empty string\n",
    "    text = re.sub(':', '', text)\n",
    " \n",
    "    # keep all alphanumeric characters (i.e. [a-zA-Z0-9_]) incl. @ and #\n",
    "    # this removes all other weird/funny characters\n",
    "    text = ' '.join(re.findall(r'[\\w@#]+', text))\n",
    " \n",
    "    # remove numbers; note: this will remove the '19' in Covid19, but we do not see this as an issue\n",
    "    text = re.sub('\\d+', ' ', text)\n",
    " \n",
    "    # remove single characters (because they are not particularly meaningful)\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', ' ', text)\n",
    " \n",
    "    # remove whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    " \n",
    "    # remove leading and trailing whitespace\n",
    "    text= text.strip()\n",
    " \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess: remove @mentions\n",
    "\n",
    "def preprocess_without_mentions(text):\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\",\n",
    "                  ' ', text)\n",
    "    \n",
    "    # remove emojis: we use the 'emoji' package to do so\n",
    "    # the function .get_emoji_regexp() returns a regex pattern for all unicode emoji characters\n",
    "    # we use this pattern to match emojis and then replace them with a whitespace\n",
    "    text = re.sub(emoji.get_emoji_regexp(), ' ', text)\n",
    "    \n",
    "    # remove @mentions\n",
    "    text = re.sub(r'@\\w+ ', ' ', text)\n",
    "    \n",
    "    # replace all '&amp;' (the HTML code for the ampersand symbol) by &\n",
    "    text = re.sub('&amp;', '', text)\n",
    "   \n",
    "    # replace '-' by an empty string\n",
    "    text = re.sub('-', '', text)\n",
    "   \n",
    "    # replace '_' by an empty string\n",
    "    text = re.sub('_', '', text)\n",
    " \n",
    "    # replace '*' by an empty string\n",
    "    text = re.sub('\\*', '', text)\n",
    " \n",
    "    # replace ':' by an empty string\n",
    "    text = re.sub(':', '', text)\n",
    " \n",
    "    # keep all alphanumeric characters (i.e. [a-zA-Z0-9_])\n",
    "    # that removes all weird/funny characters\n",
    "    text = ' '.join(re.findall(r'[\\w#]+', text))\n",
    " \n",
    "    # remove numbers; note: this will remove the '19' in Covid19, but we do not see this as an issue\n",
    "    text = re.sub('\\d+', ' ', text)\n",
    " \n",
    "    # remove single characters (because they are not particularly meaningful)\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', ' ', text)\n",
    " \n",
    "    # remove whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    " \n",
    "    # remove leading and trailing whitespace\n",
    "    text= text.strip()\n",
    " \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Germany: apply to df\n",
    "de['preprocess'] = de['text'].apply(preprocess)\n",
    "de['preprocess_no_mention'] = de['text'].apply(preprocess_without_mentions)\n",
    "\n",
    "# check the dataframe\n",
    "de.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denmark: apply to df\n",
    "da['preprocess'] = da['text'].apply(preprocess)\n",
    "da['preprocess_no_mention'] = da['text'].apply(preprocess_without_mentions)\n",
    "\n",
    "# check the dataframe\n",
    "da.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poland: apply to df\n",
    "pl['preprocess'] = pl['text'].apply(preprocess)\n",
    "pl['preprocess_no_mention'] = pl['text'].apply(preprocess_without_mentions)\n",
    "\n",
    "# check the dataframe\n",
    "pl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if there are any completely empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the dataframes\n",
    "check_list = [de, da, pl]\n",
    "countries = ['German', 'Danish', 'Polish']\n",
    "    \n",
    "for i in range(3):\n",
    "    \n",
    "    # counter variable\n",
    "    c = 0\n",
    "\n",
    "    # iterate through the text without mentions (since that is the more strict preprocessing)\n",
    "    for text in check_list[i]['preprocess_no_mention']:\n",
    "\n",
    "        # if there is an empty string, update the counter variable by 1\n",
    "        if not text:\n",
    "            c += 1\n",
    "\n",
    "    print(f\"{countries[i]} data: There are {c} issues with empty strings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading (and customizing) spaCy models\n",
    "\n",
    "We load the models 'de_core_news_sm', 'da_core_news_sm' and 'pl_core_news_sm' model. We customize the tokenizer so that it does not split hashtags so that we keep information about which hashtags are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GERMAN CORPUS\n",
    "\n",
    "# loading the model\n",
    "de_nlp = spacy.load('de_core_news_sm')\n",
    "    \n",
    "# make sure that hashtags won't be split\n",
    "\n",
    "# retrieve the default token-matching regex pattern\n",
    "de_re_token_match = spacy.tokenizer._get_regex_pattern(de_nlp.Defaults.token_match)\n",
    "\n",
    "# add #hashtag pattern\n",
    "de_re_token_match = f\"({de_re_token_match}|#\\\\w+)\"\n",
    "de_nlp.tokenizer.token_match = re.compile(de_re_token_match).match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANISH CORPUS\n",
    "da_nlp = spacy.load('da_core_news_sm')\n",
    "\n",
    "# make sure that hashtags won't be split\n",
    "\n",
    "# retrieve the default token-matching regex pattern\n",
    "da_re_token_match = spacy.tokenizer._get_regex_pattern(da_nlp.Defaults.token_match)\n",
    "\n",
    "# add #hashtag pattern\n",
    "da_re_token_match = f\"({da_re_token_match}|#\\\\w+)\"\n",
    "da_nlp.tokenizer.token_match = re.compile(da_re_token_match).match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLISH CORPUS\n",
    "pl_nlp = spacy.load('pl_core_news_sm')\n",
    "    \n",
    "# make sure that hashtags won't be split\n",
    "\n",
    "# retrieve the default token-matching regex pattern\n",
    "pl_re_token_match = spacy.tokenizer._get_regex_pattern(pl_nlp.Defaults.token_match)\n",
    "\n",
    "# add #hashtag pattern\n",
    "pl_re_token_match = f\"({pl_re_token_match}|#\\\\w+)\"\n",
    "pl_nlp.tokenizer.token_match = re.compile(pl_re_token_match).match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GERMANY: define tokenizer and lemmatizer function using spaCy\n",
    "def de_tokenize(text):\n",
    "    \n",
    "    # apply the pipeline to dataset\n",
    "    doc = de_nlp(text)\n",
    "\n",
    "    # removing stopwords and retrieving tokens: the tokens have an \n",
    "    # attribute .is_stop and in order to filter out stopwords, we need to remove all words where this keyword is False\n",
    "    tok = [str(token).lower() for token in doc if token.is_stop == False]\n",
    "\n",
    "    return tok\n",
    "\n",
    "def de_lemmatize(text):\n",
    "    \n",
    "    # apply the pipeline to dataset\n",
    "    doc = de_nlp(text)\n",
    "    \n",
    "    # removing stopwords and retrieving lemmas\n",
    "    # now we lowercase all words\n",
    "    lem = [str(token.lemma_).lower() for token in doc if token.is_stop == False]\n",
    "    \n",
    "    return lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENMARK: define tokenizer function using spaCy\n",
    "\n",
    "# tokenizer\n",
    "def da_tokenize(text):\n",
    "    \n",
    "    # apply the pipeline to dataset\n",
    "    doc = da_nlp(text)\n",
    "\n",
    "    # removing stopwords and retrieving tokens: the tokens have an \n",
    "    # attribute .is_stop and in order to filter out stopwords, we need to remove all words where this keyword is False\n",
    "    tok = [str(token).lower() for token in doc if token.is_stop == False]\n",
    "\n",
    "    return tok\n",
    "\n",
    "\n",
    "# lemmatizer\n",
    "def da_lemmatize(text):\n",
    "    \n",
    "    # apply the pipeline to dataset\n",
    "    doc = da_nlp(text)\n",
    "    \n",
    "    # removing stopwords and retrieving lemmas\n",
    "    # now we lowercase all words\n",
    "    lem = [str(token.lemma_).lower() for token in doc if token.is_stop == False]\n",
    "    \n",
    "    return lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLAND: define tokenizer and lemmatizer function using spaCy\n",
    "\n",
    "# tokenizer\n",
    "def pl_tokenize(text):\n",
    "    \n",
    "    # apply the pipeline to dataset\n",
    "    doc = pl_nlp(text)\n",
    "\n",
    "    # removing stopwords and retrieving tokens: the tokens have an \n",
    "    # attribute .is_stop and in order to filter out stopwords, we need to remove all words where this keyword is False\n",
    "    tok = [str(token).lower() for token in doc if token.is_stop == False]\n",
    "\n",
    "    return tok\n",
    "\n",
    "# lemmatizer\n",
    "def pl_lemmatize(text):\n",
    "    \n",
    "    # apply the pipeline to dataset\n",
    "    doc = pl_nlp(text)\n",
    "    \n",
    "    # removing stopwords and retrieving lemmas\n",
    "    # now we lowercase all words\n",
    "    lem = [str(token.lemma_).lower() for token in doc if token.is_stop == False]\n",
    "    \n",
    "    return lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the preprocessed tweets incl. mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply functions\n",
    "de['token'] = de['preprocess'].apply(de_tokenize)\n",
    "de['lemma'] = de['preprocess'].apply(de_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the preprocessed tweets without mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply functions\n",
    "de['token_no_mention'] = de['preprocess_no_mention'].apply(de_tokenize)\n",
    "de['lemma_no_mention'] = de['preprocess_no_mention'].apply(de_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the preprocessed tweets incl. mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply functions\n",
    "da['token'] = da['preprocess'].apply(da_tokenize)\n",
    "da['lemma'] = da['preprocess'].apply(da_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the preprocessed tweets without mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply functions\n",
    "da['token_no_mention'] = da['preprocess_no_mention'].apply(da_tokenize)\n",
    "da['lemma_no_mention'] = da['preprocess_no_mention'].apply(da_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the preprocessed tweets without mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply functions\n",
    "pl['token'] = pl['preprocess'].apply(pl_tokenize)\n",
    "pl['lemma'] = pl['preprocess'].apply(pl_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the preprocessed tweets without mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply functions\n",
    "pl['token_no_mention'] = pl['preprocess_no_mention'].apply(pl_tokenize)\n",
    "pl['lemma_no_mention'] = pl['preprocess_no_mention'].apply(pl_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(de.head(3))\n",
    "display(da.head(3))\n",
    "display(pl.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the different dataframes as files\n",
    "df_list = [de, da, pl]\n",
    "\n",
    "fil_name_list = ['de_preprocess', 'da_preprocess', 'pl_preprocess']\n",
    "\n",
    "for i in range(len(df_list)):\n",
    "    df_list[i].to_csv(f\"final_data_preprocess\\\\{fil_name_list[i]}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
