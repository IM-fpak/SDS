{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Preprocessing for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import the language specific models \n",
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download pl_core_news_sm\n",
    "!python -m spacy download da_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "Set the working directory and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "os.chdir(r'C:\\Users\\maril\\Documents\\20-21 KU\\block 4\\DM\\twitter\\raw data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Germany: import data\n",
    "de = pd.read_csv('de_original_text.csv')\n",
    "print(de.shape)\n",
    "de.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denmark: import data\n",
    "da = pd.read_csv('da_original_text.csv')\n",
    "print(da.shape)\n",
    "da.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poland: import data\n",
    "pl = pd.read_csv('pl_original_text.csv')\n",
    "print(pl.shape)\n",
    "pl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We conduct the following steps:\n",
    "\n",
    "* Remove URLs.\n",
    "* Remove @mentions.\n",
    "* Remove emojis using the ``emoji`` package\n",
    "* Replace ``&amp;`` (the HTML code for the ampersand symbol) by ``&``\n",
    "* Only keep the remaning alphanumeric characters incl. ``&``\n",
    "* Remove numbers\n",
    "* Remove single characters (because we anticipate that single characters won't be relevant to find interesting new keywords for our Twitter query. \n",
    "* Lowercase all words.\n",
    "* Remove double, triple etc. whitespaces.\n",
    "* Remove leading and trailing whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\",\n",
    "                  ' ', text)\n",
    "     \n",
    "    # remove @mentions\n",
    "    text = re.sub(r'@\\w+ ', ' ', text)\n",
    "    \n",
    "    # remove emojis: we use the 'emoji' package to do so\n",
    "    # the function .get_emoji_regexp() returns a regex pattern for all unicode emoji characters\n",
    "    # we use this pattern to match emojis and then replace them with a whitespace\n",
    "    text = re.sub(emoji.get_emoji_regexp(), ' ', text)\n",
    "    \n",
    "    # replace all '&amp;' (the HTML code for the ampersand symbol) by &\n",
    "    text = re.sub('&amp;', '&', text)\n",
    "    \n",
    "    # keep all alphanumeric characters (i.e. [a-zA-Z0-9_]) and the & symbol\n",
    "    # that removes all weird/funny characters\n",
    "    text = ' '.join(re.findall(r'[\\w&]+', text))\n",
    "\n",
    "    # remove numbers\n",
    "    text = re.sub('\\d+', ' ', text)\n",
    "    \n",
    "    # remove single characters (because they are not particularly meaningful)\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    \n",
    "    # lowercase all words\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # remove leading and trailing whitespace\n",
    "    text= text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Germany: apply to df\n",
    "de['preprocess'] = de['text'].apply(preprocess)\n",
    "\n",
    "# check the dataframe\n",
    "de.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denmark: apply to df\n",
    "da['preprocess'] = da['text'].apply(preprocess)\n",
    "\n",
    "# check the dataframe\n",
    "da.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poland: apply to df\n",
    "pl['preprocess'] = pl['text'].apply(preprocess)\n",
    "\n",
    "# check the dataframe\n",
    "pl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading (and customizing) spaCy models\n",
    "\n",
    "We load the models 'de_core_news_sm', 'da_core_news_sm' and 'pl_core_news_sm' model. We customize the tokenizer so that it does not split hashtags so that we keep information about which hashtags are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GERMAN CORPUS\n",
    "\n",
    "# loading the models\n",
    "de_nlp = spacy.load('de_core_news_sm')\n",
    "    \n",
    "# make sure that hashtags won't be split\n",
    "\n",
    "# retrieve the default token-matching regex pattern\n",
    "de_re_token_match = spacy.tokenizer._get_regex_pattern(de_nlp.Defaults.token_match)\n",
    "\n",
    "# add #hashtag pattern\n",
    "de_re_token_match = f\"({de_re_token_match}|#\\\\w+)\"\n",
    "de_nlp.tokenizer.token_match = re.compile(de_re_token_match).match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANISH CORPUS\n",
    "da_nlp = spacy.load('da_core_news_sm')\n",
    "\n",
    "# make sure that hashtags won't be split\n",
    "\n",
    "# retrieve the default token-matching regex pattern\n",
    "da_re_token_match = spacy.tokenizer._get_regex_pattern(da_nlp.Defaults.token_match)\n",
    "\n",
    "# add #hashtag pattern\n",
    "da_re_token_match = f\"({da_re_token_match}|#\\\\w+)\"\n",
    "da_nlp.tokenizer.token_match = re.compile(da_re_token_match).match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLISH CORPUS\n",
    "pl_nlp = spacy.load('pl_core_news_sm')\n",
    "\n",
    "    \n",
    "# make sure that hashtags won't be split\n",
    "\n",
    "# retrieve the default token-matching regex pattern\n",
    "pl_re_token_match = spacy.tokenizer._get_regex_pattern(pl_nlp.Defaults.token_match)\n",
    "\n",
    "# add #hashtag pattern\n",
    "pl_re_token_match = f\"({pl_re_token_match}|#\\\\w+)\"\n",
    "pl_nlp.tokenizer.token_match = re.compile(pl_re_token_match).match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "For word2vec, we only tokenize the words but we don't lemmatize them. We want all words exactly as they are used by people in their tweets in order to improve our Twitter search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GERMANY: define tokenizer function using spaCy\n",
    "def de_tokenize(text):\n",
    "    \n",
    "    # apply the pipeline to dataset\n",
    "    doc = de_nlp(text)\n",
    "\n",
    "    # removing stopwords and retrieving tokens: the tokens have an \n",
    "    # attribute .is_stop and in order to filter out stopwords, we need to remove all words where this keyword is False\n",
    "    tok = [str(token) for token in doc if token.is_stop == False]\n",
    "\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENMARK: define tokenizer function using spaCy\n",
    "def da_tokenize(text):\n",
    "    \n",
    "    # apply the pipeline to dataset\n",
    "    doc = da_nlp(text)\n",
    "\n",
    "    # removing stopwords and retrieving tokens: the tokens have an \n",
    "    # attribute .is_stop and in order to filter out stopwords, we need to remove all words where this keyword is False\n",
    "    tok = [str(token) for token in doc if token.is_stop == False]\n",
    "\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLAND: define tokenizer function using spaCy\n",
    "def pl_tokenize(text):\n",
    "    \n",
    "    # apply the pipeline to dataset\n",
    "    doc = pl_nlp(text)\n",
    "\n",
    "    # removing stopwords and retrieving tokens: the tokens have an \n",
    "    # attribute .is_stop and in order to filter out stopwords, we need to remove all words where this keyword is False\n",
    "    tok = [str(token) for token in doc if token.is_stop == False]\n",
    "\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GERMANY: apply tokenizer function and add the lists back to the dataframe\n",
    "de['preprocess_token'] = de['preprocess'].apply(de_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENMARK: apply tokenizer function and add the lists back to the dataframe\n",
    "da['preprocess_token'] = da['preprocess'].apply(da_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLAND: apply tokenizer function and add the lists back to the dataframe\n",
    "pl['preprocess_token'] = pl['preprocess'].apply(pl_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(de.head(3), da.head(3), pl.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory\n",
    "os.chdir(r'C:\\Users\\maril\\Documents\\20-21 KU\\block 4\\DM\\twitter\\preprocess_word2vec')\n",
    "\n",
    "# saving the different dataframes as files\n",
    "df_list = [de, da, pl]\n",
    "\n",
    "fil_name_list = ['de_preprocess', 'da_preprocess', 'pl_preprocess']\n",
    "\n",
    "for i in range(len(df_list)):\n",
    "    df_list[i].to_csv(f\"{fil_name_list[i]}.csv\", index=False, encoding='utf8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
