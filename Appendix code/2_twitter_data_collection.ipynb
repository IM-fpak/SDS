{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix Code: 1\n",
    "\n",
    "\n",
    "## Twitter Data Collection\n",
    "### Using the Search API (RESTful endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook exemplifies the data collection script we used to scrape tweets. We ran the script three times with different specifications for the language setting (``da`` for Danish, ``de`` for German and ``pl`` for Polish).  \n",
    "\n",
    "We use the Python packages ``tweepy`` to collect the tweets. \n",
    "\n",
    "* _Tweet object:_ The resulting tweets come as tweepy status objects that contain a JSON property with all relevant info about the tweet (JSON = a nested ``dict`` of ``dicts``)\n",
    "\n",
    "* _Saving tweet objects:_ We'll write the entire JSON property to a csv file because csv is one of the most compact file formats. We do not parse the tweet object but keep it as a whole for now since we do not want to miss out on any information that might be relevant later on.\n",
    "\n",
    "* _Timeframe:_ We can retrieve tweets which were posted during the last seven days.\n",
    "\n",
    "* _Query:_ The query can be max. 500 characters long. This is a problem, because we have considerably more keywords than that. Therefore, we'll loop through all the keywords and construct a query for each keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import tweepy\n",
    "from tweepy import TweepError\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from contextlib import suppress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get credentials from local file \"AppCred.py\" located in the directory you are working in\n",
    "from AppCred import CONSUMER_KEY, CONSUMER_SECRET\n",
    "from AppCred import ACCESS_TOKEN, ACCESS_TOKEN_SECRET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authentification\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "# create instance of the API class\n",
    "api = tweepy.API(auth, \n",
    "          wait_on_rate_limit=True,\n",
    "          wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Retrieving tweets without further query specifications\n",
    "\n",
    "This will return a mixture of original tweets and retweets. Further down, we run variations of the query to only collect original tweets or to only collect retweets. Moreover, the Search API allows us to specify the ``result_type`` we want to retrieve: We can choose either ``recent``, ``popular`` or ``mixed``. In order to get the most comprehensive sample, we will run the scraper for all three result types.\n",
    "  \n",
    "This will most likely results in overlaps between the the different variations of the query. Before analysing the data, we therefore have to remove duplicates (we will do that in another notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading the keywords\n",
    "\n",
    "keywords = []\n",
    "\n",
    "with open('final_keywords.csv', 'r', encoding='utf') as file:\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    for row in reader:\n",
    "        keywords.append(row[0])\n",
    "\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA COLLECTION\n",
    "\n",
    "# list to store tweet object in\n",
    "data = []\n",
    "\n",
    "# result type: we can choose 'recent', 'popular' or 'mixed' and we want all tweets for all the different types\n",
    "result_type_list = ['recent', 'popular', 'mixed']\n",
    "\n",
    "# open the file in 'append' mode to store the tweets\n",
    "# specify encoding as 'utf8' (because of German letter such as ä, ö, ü)\n",
    "# specify newline=\"\": if this code is run on a Windows machine and we don't specify this parameter,\n",
    "# then the csv writer adds an empty line after each entry and we don't want that\n",
    "\n",
    "with open('tweets2.csv', 'a', encoding='utf8', newline=\"\") as outfile:\n",
    "    \n",
    "    # writer instance\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    # iterate through the result_type_list:\n",
    "    for result_type in result_type_list:\n",
    "        \n",
    "        # iterate through the keywords\n",
    "        for word in keywords:\n",
    "\n",
    "            # suppressing the TweepError (connection error)\n",
    "            # this allows the script to run without being interrupted by this random error, while also\n",
    "            # stopping once all tweets for this date are collected\n",
    "            with suppress(TweepError):\n",
    "\n",
    "                # we use the tweepy pagination (i.e. we get pages of tweet results back)\n",
    "\n",
    "                # Cursor specifications:\n",
    "                # 1- api.search: use the search endpoint\n",
    "                # 2- q: use the current iteration through the query list as the query term\n",
    "                # 3- tweet_mode: \"extended\", i.e. get tweets up to 280 chars\n",
    "                # 4- count: return 100 tweets per page (100 is the maximum)\n",
    "                # 5- result_type: get tweets based on the recent/popular/mixed specification\n",
    "                # 6- include_entities: include entities part of the tweet (just in case it might be relevant)\n",
    "                # 7- lang: set language to 'de' (= German), 'da' (= Danish) or 'pl' (= Polish)\n",
    "\n",
    "                for page in tweepy.Cursor(api.search,\n",
    "                                          q=word,\n",
    "                                          tweet_mode=\"extended\", \n",
    "                                          count=100,\n",
    "                                          result_type=result_type,\n",
    "                                          include_entities=True,\n",
    "                                          lang = 'de').pages(): \n",
    "\n",
    "                    # add tweet objects to the 'data' list\n",
    "                    data.extend(page)\n",
    "\n",
    "                    # iterate through the objects on the current page\n",
    "                    for item in page:\n",
    "\n",
    "                        # write the ._json part of the tweet object (i.e.the part that comes as dict of dicts) to file\n",
    "                        writer.writerow([str(item._json)])\n",
    "\n",
    "            # print the number of tweets we've collected so far\n",
    "            print(f\"Tweets in total for {word}: {len(data)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Retrieving original tweets (no retweets)\n",
    "\n",
    "This is where we scrape original tweets, i.e. we filter out all retweets. We will save the results in a different file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading the keywords\n",
    "# here, we filter out retweets\n",
    "\n",
    "keywords_filter_1 = []\n",
    "\n",
    "with open('final_keywords.csv', 'r', encoding='utf') as file:\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    for row in reader:\n",
    "        keywords_filter_1.append(row[0] + ' -filter:retweets')\n",
    "\n",
    "keywords_filter_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DATA COLLECTION\n",
    "\n",
    "# list to store tweet object in\n",
    "data = []\n",
    "\n",
    "# result type: we can choose 'recent', 'popular' or 'mixed' and we want all tweets for all the different types\n",
    "result_type_list = ['recent', 'popular', 'mixed']\n",
    "\n",
    "# open the file in 'append' mode to store the tweets\n",
    "# specify encoding as 'utf8' (because of German letter such as ä, ö, ü)\n",
    "# specify newline=\"\": if this code is run on a Windows machine and we don't specify this parameter\n",
    "# the csv writer adds an empty line after each entry and we don't want that\n",
    "\n",
    "with open('tweets_no_rt2.csv', 'a', encoding='utf8', newline=\"\") as outfile:\n",
    "    \n",
    "    # writer instance\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    # iterate through the result_type_list:\n",
    "    for result_type in result_type_list:\n",
    "        \n",
    "        # iterate through the keywords (incl. filter)\n",
    "        for word in keywords_filter_1:\n",
    "\n",
    "            # suppressing the TweepError (connection error)\n",
    "            # this allows the script to run without being interrupted by this random error, while also\n",
    "            # stopping once all tweets for this date are collected\n",
    "            with suppress(TweepError):\n",
    "\n",
    "                # we use the tweepy pagination (i.e. we get pages of tweet results back)\n",
    "\n",
    "                # Cursor specifications:\n",
    "                # 1- api.search: use the search endpoint\n",
    "                # 2- q: use the current iteration through the query list as the query term\n",
    "                # 3- tweet_mode: \"extended\", i.e. get tweets up to 280 chars\n",
    "                # 4- count: return 100 tweets per page (100 is the maximum)\n",
    "                # 5- result_type: get tweets based on the recent/popular/mixed specification\n",
    "                # 6- include_entities: include entities part of the tweet (just in case it might be relevant)\n",
    "                # 7- lang: set language to 'de' (= German), 'da' (= Danish) or 'pl' (= Polish)\n",
    "\n",
    "                for page in tweepy.Cursor(api.search,\n",
    "                                          q=word,\n",
    "                                          tweet_mode=\"extended\", \n",
    "                                          count=100,\n",
    "                                          result_type=result_type,\n",
    "                                          include_entities=True,\n",
    "                                          lang = 'de').pages(): \n",
    "\n",
    "                    # add tweet objects to the data list\n",
    "                    data.extend(page)\n",
    "\n",
    "                    # iterate through the objects on the current page\n",
    "                    for item in page:\n",
    "\n",
    "                        # write the ._json part of the tweet object (i.e.the part that comes as dict of dicts) to file\n",
    "                        writer.writerow([str(item._json)])\n",
    "\n",
    "            # print the number of tweets we've collected so far\n",
    "            print(f\"Tweets in total for {word}: {len(data)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 3: Retrieving only retweets\n",
    "Lastly, we'll scrape _only_ retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the keywords\n",
    "# here, we filter for just retweets\n",
    "\n",
    "keywords_filter_2 = []\n",
    "\n",
    "with open('final_keywords.csv', 'r', encoding='utf') as file:\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    for row in reader:\n",
    "        keywords_filter_2.append(row[0] + ' filter:retweets')\n",
    "\n",
    "keywords_filter_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA COLLECTION\n",
    "\n",
    "# list to store tweet object in\n",
    "data = []\n",
    "\n",
    "# result type: we can choose 'recent', 'popular' or 'mixed' and we want all tweets for all the different types\n",
    "result_type_list = ['recent', 'popular', 'mixed']\n",
    "\n",
    "# open the file in 'append' mode to store the tweets\n",
    "# specify encoding as 'utf8' (because of German letter such as ä, ö, ü)\n",
    "# specify newline=\"\": if this code is run on a Windows machine and we don't specify this parameter\n",
    "# the csv writer adds an empty line after each entry and we don't want that\n",
    "\n",
    "with open('tweets_only_rt2.csv', 'a', encoding='utf8', newline=\"\") as outfile:\n",
    "    \n",
    "    # writer instance\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    # iterate through the result_type_list:\n",
    "    for result_type in result_type_list:\n",
    "        \n",
    "        # iterate through the keywords (this time, filtered to just return retweets)\n",
    "        for word in keywords_filter_2:\n",
    "\n",
    "            # suppressing the TweepError (connection error)\n",
    "            # this allows the script to run without being interrupted by this random error, while also\n",
    "            # stopping once all tweets for this date are collected\n",
    "            with suppress(TweepError):\n",
    "\n",
    "                # we use the tweepy pagination (i.e. we get pages of tweet results back)\n",
    "\n",
    "                # Cursor specifications:\n",
    "                # 1- api.search: use the search endpoint\n",
    "                # 2- q: use the current iteration through the query list as the query term\n",
    "                # 3- tweet_mode: \"extended\", i.e. get tweets up to 280 chars\n",
    "                # 4- count: return 100 tweets per page (100 is the maximum)\n",
    "                # 5- result_type: get tweets based on the recent/popular/mixed specification\n",
    "                # 6- include_entities: include entities part of the tweet (just in case it might be relevant)\n",
    "                # 7- lang: set language to 'de' (= German), 'da' (= Danish) or 'pl' (= Polish)\n",
    "\n",
    "                for page in tweepy.Cursor(api.search,\n",
    "                                          q=word,\n",
    "                                          tweet_mode=\"extended\", \n",
    "                                          count=100,\n",
    "                                          result_type=result_type,\n",
    "                                          include_entities=True,\n",
    "                                          lang = 'de').pages(): \n",
    "\n",
    "                    # add tweet objects to the data list\n",
    "                    data.extend(page)\n",
    "\n",
    "                    # iterate through the objects on the current page\n",
    "                    for item in page:\n",
    "\n",
    "                        # write the ._json part of the tweet object (i.e.the part that comes as dict of dicts) to file\n",
    "                        writer.writerow([str(item._json)])\n",
    "\n",
    "            # print the number of tweets we've collected so far\n",
    "            print(f\"Tweets in total for {word}: {len(data)}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
