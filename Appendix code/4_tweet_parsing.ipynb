{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for further processing\n",
    "\n",
    "This notebook has two main goals:\n",
    "\n",
    "1. **Tweet text:** We extract the tweet text and some other basic information, such as the user tweeting, the date, and whether the tweet is an original or a retweet. This information is be saved in a tabular format (here: a pandas dataframe), so make further preprocessing steps easier.\n",
    "2. **Network edgelist:** We retrieve an edgelist of communicative interactions (retweets, quotes, mentions, replies). This information is also be saved in tabular format (here: a pandas dataframe) in order to allow for network analysis with ``networkx``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "Loading the data from the three different dataset files we've previously created and removing duplicate tweets. Each tweets comes with a unique tweet ID and we can use this information to find and remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\maril\\Documents\\20-21 KU\\block 4\\DM\\twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the tweet objects we've stored in a file before\n",
    "# we define a function that we can use for all three datasets\n",
    "\n",
    "def loading(file_tweets, file_no_rt, file_only_rt):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns set with tweet ID's and unique tweet objects in list.\n",
    "    \n",
    "        Input: Three file names as strings in the following order: \n",
    "                1. File containing mix of original and retweets.\n",
    "                2. File containing just original tweets.\n",
    "                3. File containing just retweets.\n",
    "\n",
    "        Output: List containing all unique tweets as a dict of dicts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty set to count the total number of tweets collected (incl. duplicates)\n",
    "    count = []\n",
    "\n",
    "    # empty set to filter out duplicate tweets \n",
    "    ids = set()\n",
    "\n",
    "    # empty list to store the tweets (as dict of dicts) in\n",
    "    tweets = []\n",
    "\n",
    "    # open the file in the 'read' mode, specify encoding as 'utf8'\n",
    "    with open(file_tweets, 'r', encoding='utf8') as infile_tweets:\n",
    "        with open(file_no_rt, 'r', encoding='utf8') as infile_no_rt:\n",
    "            with open(file_only_rt, 'r', encoding='utf8') as infile_only_rt:\n",
    "\n",
    "                # csv reader\n",
    "                reader_tweets = csv.reader(infile_tweets)\n",
    "                reader_no_rt = csv.reader(infile_no_rt)\n",
    "                reader_only_rt = csv.reader(infile_only_rt)\n",
    "\n",
    "                # iterate through the reader object\n",
    "                for row in reader_tweets:\n",
    "\n",
    "                    # each row is a list in which the first element contains the tweet object as a string resembling a dict\n",
    "                    # we want to turn this string back into a dict we use eval() to do so\n",
    "\n",
    "                    tweet = eval(row[0]) \n",
    "                    \n",
    "                    # collect the id of all tweets (incl. duplicate ids)\n",
    "                    count.append(row[0])\n",
    "                    \n",
    "                    # restrict the timeframe\n",
    "                    if 'retweeted_status' in tweet:\n",
    "                        date = datetime.datetime.strptime(tweet['retweeted_status']['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                        \n",
    "                    elif 'quoted_status' in tweet: \n",
    "                        date = datetime.datetime.strptime(tweet['quoted_status']['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                    \n",
    "                    else:\n",
    "                        date = datetime.datetime.strptime(tweet['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                    \n",
    "                    if (date > datetime.datetime(2021, 5, 21, 7, 0, 0)) & (date < datetime.datetime(2021, 5, 28, 7, 0, 0)):  \n",
    "                        \n",
    "                        # if the tweet id is not yet in the ids set, then add id to the 'ids' set \n",
    "                        # and add the entire dict to the 'tweets' list\n",
    "                        if tweet['id_str'] not in ids:\n",
    "                            ids.add(tweet['id_str'])\n",
    "                            tweets.append(tweet)\n",
    "\n",
    "                for row in reader_no_rt:\n",
    "\n",
    "                    # each row is a list in which the first element contains the tweet object as a string resembling a dict\n",
    "                    # we want to turn this string back into a dict we use eval() to do so\n",
    "\n",
    "                    tweet = eval(row[0])\n",
    "                    \n",
    "                    # restrict the timeframe\n",
    "                    if 'retweeted_status' in tweet:\n",
    "                        date = datetime.datetime.strptime(tweet['retweeted_status']['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                        \n",
    "                    elif 'quoted_status' in tweet: \n",
    "                        date = datetime.datetime.strptime(tweet['quoted_status']['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                    \n",
    "                    else:\n",
    "                        date = datetime.datetime.strptime(tweet['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                    \n",
    "                    if (date > datetime.datetime(2021, 5, 21, 7, 0, 0)) & (date < datetime.datetime(2021, 5, 28, 7, 0, 0)):  \n",
    "                        \n",
    "                        # if the tweet id is not yet in the ids set, then add id to the 'ids' set \n",
    "                        # and add the entire dict to the 'tweets' list\n",
    "                        if tweet['id_str'] not in ids:\n",
    "                            ids.add(tweet['id_str'])\n",
    "                            tweets.append(tweet)\n",
    "\n",
    "                for row in reader_only_rt:\n",
    "\n",
    "                    # each row is a list in which the first element contains the tweet object as a string resembling a dict\n",
    "                    # we want to turn this string back into a dict we use eval() to do so\n",
    "\n",
    "                    tweet = eval(row[0])\n",
    "                    \n",
    "                    # restrict the timeframe\n",
    "                    if 'retweeted_status' in tweet:\n",
    "                        date = datetime.datetime.strptime(tweet['retweeted_status']['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                        \n",
    "                    elif 'quoted_status' in tweet: \n",
    "                        date = datetime.datetime.strptime(tweet['quoted_status']['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                    \n",
    "                    else:\n",
    "                        date = datetime.datetime.strptime(tweet['created_at'], '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                    \n",
    "                    if (date > datetime.datetime(2021, 5, 21, 7, 0, 0)) & (date < datetime.datetime(2021, 5, 28, 7, 0, 0)):  \n",
    "                        \n",
    "                        # if the tweet id is not yet in the ids set, then add id to the 'ids' set \n",
    "                        # and add the entire dict to the 'tweets' list\n",
    "                        if tweet['id_str'] not in ids:\n",
    "                            ids.add(tweet['id_str'])\n",
    "                            tweets.append(tweet)        \n",
    "    \n",
    "    return tweets, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the function to the different datasets\n",
    "\n",
    "# German data\n",
    "de_tweets_unique, de_count = loading(r'final_raw_data\\de_tweets2.csv', r'final_raw_data\\de_tweets_no_rt2.csv', r'final_raw_data\\de_tweets_only_rt2.csv')\n",
    "\n",
    "# Danish data\n",
    "da_tweets_unique, da_count = loading(r'final_raw_data\\tweets_da2.csv', r'final_raw_data\\tweets_no_rt_da2.csv', r'final_raw_data\\tweets_only_rt_da2.csv')\n",
    "\n",
    "# Polish data\n",
    "pl_tweets_unique, pl_count = loading(r'final_raw_data\\tweets_pl_final3.csv', r'final_raw_data\\tweets_pl_final_no_rt3.csv', r'final_raw_data\\tweets_pl_final_rt3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Count how many duplicates there are\n",
    "de_pct_duplicates = (len(de_count) - len(de_tweets_unique)) / len(de_count)\n",
    "da_pct_duplicates = (len(da_count) - len(da_tweets_unique)) / len(da_count)\n",
    "pl_pct_duplicates = (len(pl_count) - len(pl_tweets_unique)) / len(pl_count)\n",
    "\n",
    "# print the results\n",
    "print(f'German data: {round(de_pct_duplicates,3)*100}% of the tweets were unique.')\n",
    "print(f'Danish data: {round(da_pct_duplicates,3)*100}% of the tweets were unique.')\n",
    "print(f'Polish data: {round(pl_pct_duplicates,3)*100}% of the tweets were unique.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out irrelevant tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter out all irrelevant tweets\n",
    "\n",
    "def filter_irrelevant(list_of_tweet_dicts, irrelevant_ids):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a list of relevant tweets. \n",
    "    \n",
    "        Input: The list of unique tweets which we created in the cell before. A list of tweet IDs that\n",
    "        should be removed because they are irrelevant.\n",
    "\n",
    "        Output: List containing all relevant, unique tweets as a dict of dicts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # list to store the filtered tweets in\n",
    "    filtered_tweets = []\n",
    "    \n",
    "    # iterate through the list containing tweet objects (as dict of dicts)\n",
    "    for tweet in list_of_tweet_dicts:\n",
    "        \n",
    "        # now we check if any unwanted tweets IDs are there \n",
    "        \n",
    "        # we only keep tweets in which all possible tweet IDs which we can find in a tweet object \n",
    "        # (= ID of the tweet in question, IDs of possible retweets or quotes) are relevant\n",
    "        \n",
    "        # if it's a retweet\n",
    "        if 'retweeted_status' in tweet:\n",
    "            id_tx = tweet['id_str']\n",
    "            id_rt = tweet['retweeted_status']['id_str']\n",
    "            \n",
    "            # make sure none of the IDs is in the 'irrelevant_ids' set\n",
    "            if id_tx not in irrelevant_ids:\n",
    "                if id_rt not in irrelevant_ids:\n",
    "                    \n",
    "                    # add to the list\n",
    "                    filtered_tweets.append(tweet)\n",
    "\n",
    "        # if it's quote\n",
    "        elif 'quoted_status' in tweet:\n",
    "            id_tx = tweet['id_str']\n",
    "            id_qt = tweet['quoted_status']['id_str']\n",
    "            \n",
    "            # make sure none of the IDs is in the 'irrelevant_ids' set\n",
    "            if id_tx not in irrelevant_ids:\n",
    "                if id_qt not in irrelevant_ids:\n",
    "                    \n",
    "                    # add to the list\n",
    "                    filtered_tweets.append(tweet)\n",
    "        \n",
    "        # if it's just a plain, original tweet\n",
    "        else:\n",
    "            id_tx = tweet['id_str']\n",
    "            \n",
    "             # make sure none of the IDs is in the 'irrelevant_ids' set\n",
    "            if id_tx not in irrelevant_ids:\n",
    "                \n",
    "                # add to the list\n",
    "                filtered_tweets.append(tweet)\n",
    "        \n",
    "    return filtered_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading irrelevant tweet IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've manually gone through keywords which were problematic in the data collection process (such as the use of the keyword 'j&j' since Twitter apparently treats the ampersand symbol as and ``AND`` operator). We've noted the IDs of all irrelevant tweets and remove these tweets here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Germany: loading irrelevant tweets and saving them in a set\n",
    "de_irr = set()\n",
    "\n",
    "with open(r'sanity_check\\remove_de.txt', 'r') as de_file:\n",
    "    de = de_file.readlines()\n",
    "    \n",
    "    for item in de: \n",
    "        item = re.findall('\\d+', item)[0]\n",
    "        de_irr.add(item)\n",
    "\n",
    "print(len(de_irr))\n",
    "de_irr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denmark: loading irrelevant tweets and saving them in a set\n",
    "da_irr = set()\n",
    "\n",
    "with open(r'sanity_check\\remove_da.txt', 'r') as da_file:\n",
    "    with open(r'sanity_check\\remove_da2.txt', 'r') as da_file2:\n",
    "         with open(r'sanity_check\\remove_da3.txt', 'r') as da_file3:\n",
    "        \n",
    "            da = da_file.readlines()\n",
    "            da2 = da_file2.readlines()\n",
    "            da3 = da_file3.readlines()\n",
    "\n",
    "            for item in da: \n",
    "                item = re.findall('\\d+', item)[0]\n",
    "                da_irr.add(item)\n",
    "\n",
    "            for item in da2: \n",
    "                item = re.findall('\\d+', item)[0]\n",
    "                da_irr.add(item)\n",
    "                \n",
    "            for item in da3: \n",
    "                item = re.findall('\\d+', item)[0]\n",
    "                da_irr.add(item)\n",
    "\n",
    "print(len(da_irr))\n",
    "da_irr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poland: loading irrelevant tweets and saving them in a set\n",
    "pl_irr = set()\n",
    "\n",
    "with open(r'sanity_check\\remove_pl.txt', 'r') as pl_file:\n",
    "    with open(r'sanity_check\\remove_pl2.txt', 'r') as pl_file2:\n",
    "        with open(r'sanity_check\\remove_pl3.txt', 'r') as pl_file3:\n",
    "            pl = pl_file.readlines()\n",
    "            pl2 = pl_file2.readlines()\n",
    "            pl3 = pl_file3.readlines()\n",
    "\n",
    "            for item in pl: \n",
    "                item = re.findall('\\d+', item)[0]\n",
    "                pl_irr.add(item)\n",
    "\n",
    "            for item in pl2: \n",
    "                item = re.findall('\\d+', item)[0]\n",
    "                pl_irr.add(item)\n",
    "\n",
    "            for item in pl3: \n",
    "                item = re.findall('\\d+', item)[0]\n",
    "                pl_irr.add(item)\n",
    "                \n",
    "print(len(pl_irr))\n",
    "pl_irr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the function\n",
    "de_relevant = filter_irrelevant(de_tweets_unique, de_irr)\n",
    "da_relevant = filter_irrelevant(da_tweets_unique, da_irr)\n",
    "pl_relevant = filter_irrelevant(pl_tweets_unique, pl_irr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out all tweets that produce empty strings after preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a modified version of the preprocessing function that is later on used to conduct the actual preprocessing.\n",
    "\n",
    "In this function, we conduct the following preprocessing steps:\n",
    "\n",
    "* Remove **URLs**.\n",
    "* Remove **emojis** using the ``emoji`` package.\n",
    "* **Remove the mentions.**\n",
    "* Remove ``&amp;`` (the HTML code for the **ampersand** symbol)\n",
    "* **Replace '-' by an empty string:** This is important to keep words together that belong together. E.g. the German 'Impf-Reihenfolge' should be merge into one word 'ImpfReihenfolge' in order to not distort it's meaning. This makes lemmatization more difficult esp. because 'ImpfReihenfolge' is not the correct spelling of the word (it should be 'Impfreihenfolge'), but since this will only affect a very small number of words, we deem it acceptable.\n",
    "* **Remove ':', '\\*' and *_*:** Again, this is mostly relevant for German. In German, nouns describing people (e.g. the word for 'doctor') usually come in a male ('Doktor') and female form ('Doktorin'). In recent years, there has been a movement to include both spelling as either 'Doktor_in', 'Doktor:in' or 'Doktor\\*in' in an attempt at more gender neutral language. If we replace these symbols by spaces, then we would distort the words meaning since 'Doktor in' is not the same as 'Doktorin'. We therefore just remove these symbols. This is not relevant for Polish or Danish, but since removing these symbols does not cause any other issue there, we do this for all three datasets.\n",
    "* Only **keep the remaining alphanumeric characters** (incl. ``#`` for hashtags).\n",
    "* Remove **numbers**.\n",
    "* Remove **single characters.** They are usually not particularly meaningful: In Polish and German, there are no (meaningful) words that only consist of one character. In Danish, there is the 'I' (the plural 'you'; as in 'Hvordan har I det?'). But this character will be removed in the stopword list anyway, so we might as well already remove it here.\n",
    "* Remove **double, triple etc. whitespaces**.\n",
    "* Remove **leading and trailing whitespaces**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess: remove @mentions\n",
    "\n",
    "def preprocess_without_mentions(text):\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\",\n",
    "                  ' ', text)\n",
    "    \n",
    "    # remove emojis: we use the 'emoji' package to do so\n",
    "    # the function .get_emoji_regexp() returns a regex pattern for all unicode emoji characters\n",
    "    # we use this pattern to match emojis and then replace them with a whitespace\n",
    "    text = re.sub(emoji.get_emoji_regexp(), ' ', text)\n",
    "    \n",
    "    # remove @mentions\n",
    "    text = re.sub(r'@\\w+ ', ' ', text)\n",
    "    \n",
    "    # replace all '&amp;' (the HTML code for the ampersand symbol) by &\n",
    "    text = re.sub('&amp;', '', text)\n",
    "   \n",
    "    # replace '-' by an empty string\n",
    "    text = re.sub('-', '', text)\n",
    "   \n",
    "    # replace '_' by an empty string\n",
    "    text = re.sub('_', '', text)\n",
    " \n",
    "    # replace '*' by an empty string\n",
    "    text = re.sub('\\*', '', text)\n",
    " \n",
    "    # replace ':' by an empty string\n",
    "    text = re.sub(':', '', text)\n",
    " \n",
    "    # keep all alphanumeric characters (i.e. [a-zA-Z0-9_])\n",
    "    # that removes all weird/funny characters\n",
    "    text = ' '.join(re.findall(r'[\\w#]+', text))\n",
    " \n",
    "    # remove numbers; note: this will remove the '19' in Covid19, but we do not see this as an issue\n",
    "    text = re.sub('\\d+', ' ', text)\n",
    " \n",
    "    # remove single characters (because they are not particularly meaningful)\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', ' ', text)\n",
    " \n",
    "    # remove whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    " \n",
    "    # remove leading and trailing whitespace\n",
    "    text= text.strip()\n",
    " \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove tweets that are empty after preprocessing\n",
    "\n",
    "def remove_empty_tweets(list_of_filtered_tweets):\n",
    "    \n",
    "    # list to store the final tweets in\n",
    "    final_tweets = []\n",
    "    \n",
    "    for tweet in list_of_filtered_tweets:\n",
    "        \n",
    "        # first we deal with retweets and original tweets (because they only have one tweet text in the end)\n",
    "        \n",
    "        # if it's a retweet\n",
    "        if 'retweeted_status' in tweet:\n",
    "            text = tweet['retweeted_status']['full_text']\n",
    "            \n",
    "            # apply the preprocessing function\n",
    "            text = preprocess_without_mentions(text)\n",
    "            \n",
    "            # if the string is not empty, add to the final_tweets list\n",
    "            if text:\n",
    "                final_tweets.append(tweet)\n",
    "        \n",
    "        # if it's quote\n",
    "        elif 'quoted_status' in tweet:\n",
    "            text = tweet['full_text']\n",
    "            quote = tweet['quoted_status']['full_text']\n",
    "            \n",
    "            # apply the preprocessing function\n",
    "            text = preprocess_without_mentions(text)\n",
    "            quote = preprocess_without_mentions(quote)\n",
    "            \n",
    "            # if the string is not empty, add to the final_tweets list\n",
    "            if text:\n",
    "                if quote:\n",
    "                    final_tweets.append(tweet)\n",
    "        \n",
    "        # if it's just a plain, original tweet\n",
    "        else:\n",
    "            text = tweet['full_text']\n",
    "            \n",
    "            # apply the preprocessing function\n",
    "            text = preprocess_without_mentions(text)\n",
    "            \n",
    "            # if the string is not empty, add to the final_tweets list\n",
    "            if text:\n",
    "                final_tweets.append(tweet)        \n",
    " \n",
    "    return final_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the function\n",
    "de_final = remove_empty_tweets(de_relevant)\n",
    "da_final = remove_empty_tweets(da_relevant)\n",
    "pl_final = remove_empty_tweets(pl_relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tweet text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the tweet text\n",
    "Here we extract the tweet text together with some meta-data on the tweet ID, the user and the date it was created. In particular, we create two dataframes: The first one contains information on all tweets (original and retweet). The second dataframe contains information on all original tweets.\n",
    "\n",
    "There are two tricky parts that need to be addressed:\n",
    "\n",
    "1. **Retweets:** We create the ``original`` dataframe that only contains original tweet texts (this will be used for the topic modelling and the active learninig classification (ASDS2) and the semantic network (DM)). For this dataframe, we discard all retweets. However, we still want to retrieve the original text that was retweeted since that constitutes an original tweet. This information is hidden in the tweet object (in a dictionary called ``retweeted_status``) and we can retrieve all relevant information, but we have to add another check to make sure that there are no duplicates. The duplicate check we ran in the cells above was only concerned with the tweet ID of the overall tweet object, but it did not remove duplicates in the retweet ID that can be found in the ``retweeted_status`` dictionary.\n",
    "  \n",
    "To illustrate the point, here is a shortened version of what the a tweet object of a retweet looks like (we've made the information up to not accidentally display personal information protected by the GDPR, so it's not actually possible to look this tweet up.)\n",
    "\n",
    "```\n",
    "{'created_at': 'Fri May 28 19:46:51 +0000 2021',\n",
    " 'id': 1398312371249155565,\n",
    " 'id_str': '1398312371249155565',\n",
    " 'full_text': 'RT @hello_summer: Das ist ein Beispiel Tweet über Impfungen und Corona…',\n",
    " 'truncated': False,\n",
    " 'display_text_range': [0, 140],\n",
    "\n",
    " ...\n",
    " \n",
    " 'user': {'id': 1375388000040222754,\n",
    "  'id_str': '1375388000040222754',\n",
    "  'name': 'Erika Musterfrau',\n",
    "  'screen_name': 'EMusterfrau',\n",
    "  'location': '',\n",
    " ...\n",
    "          \n",
    " 'retweeted_status': {'created_at': 'Fri May 28 19:34:49 +0000 2021',\n",
    "  'id': 1333360002156410885,\n",
    "  'id_str': '1333360002156410885',\n",
    "  'full_text': 'Das ist ein Beispiel Tweet über Impfungen und Corona #Pfizer #Astra #Impfung #Stiko',\n",
    "  'truncated': False,\n",
    " ...\n",
    "  'user': {'id': 762755574677764929,\n",
    "   'id_str': '762755574677764929',\n",
    "   'name': 'Summer',\n",
    "   'screen_name': 'hello_summer',\n",
    "   'location': 'BW',\n",
    "\n",
    "   ...\n",
    "```\n",
    "\n",
    "2. **Quotes/quoted tweets**: Quotes are retweets, to which the user retweeting has added their own comment. These kind of tweets pose a particular challenge since we have to decide: Do we just keep the comment that was added by the user? Do we add the quoted text and the user's comment up in one long string? Do we just keep what the quoted tweet originally said (that would mean treating the quotes as retweets)?\n",
    "We've discussed the implications of the different approaches and in the end decided to only keep the user's comment since this will be most relevant for the topic modelling and the classification task. However, we are aware that this means missing out on the context of the user's comment since they usually refer to the contents of the quoted tweet. We therefore add a column that contains the original quoted tweet so that we can refer back to them if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving the tweet text (for the topic modelling which we'll do in another notebook)\n",
    "# we define a function that we can use for all three datasets\n",
    "\n",
    "def tweets(list_of_final_tweets):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns user, date, tweet text and retweet information as pandas df.\n",
    "    \n",
    "        Input: List containing tweets (tweet format: dict of dicts)\n",
    "\n",
    "        Output: Two pandas dataframes. The first one contains information on all tweets (original and retweet),\n",
    "        and has the columns user, date, text and retweet. The second dataframe contains information on all\n",
    "        original tweets, and has the columns user, date and text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty lists to store tweet text in, we create two different lists: \n",
    "    # all_tweets: one for all the original and the retweets\n",
    "    # original: and one for only original tweets\n",
    "    all_tweets = []\n",
    "    original = []\n",
    "\n",
    "    # empty sets to remove further duplicates\n",
    "    all_ids = set()\n",
    "    original_ids = set()\n",
    "    \n",
    "    # iterate through the tweets list\n",
    "    for tweet in list_of_final_tweets:\n",
    "\n",
    "        # if the tweet object has a dict key 'retweeted_status'\n",
    "        if 'retweeted_status' in tweet:\n",
    "\n",
    "            # retrieve the user retweeting and the retweeted text\n",
    "            id_text = tweet['id_str']\n",
    "            user = tweet['user']['screen_name']\n",
    "            date = tweet['created_at']\n",
    "            text = tweet['retweeted_status']['full_text']\n",
    "            \n",
    "            # add the retweeted text as an \"original\" tweet incl. user/id/date information as well\n",
    "            id_rt = tweet['retweeted_status']['id_str']\n",
    "            user_rt = tweet['retweeted_status']['user']['screen_name']\n",
    "            date_rt = tweet['retweeted_status']['created_at']\n",
    "            text_rt = tweet['retweeted_status']['full_text']\n",
    "            \n",
    "            # if the ID of the retweet is not in the all_ids set yet\n",
    "            if id_text not in all_ids:\n",
    "                \n",
    "                # append the information (id, user handle, date, tweet text, is retweet, is quote, quote text) \n",
    "                # as a tuple to the list\n",
    "                all_tweets.append((id_text,user,date,text,1,0,np.nan))\n",
    "            \n",
    "            # if the ID of the retweeted tweet is not yet in all_ids\n",
    "            if id_rt not in all_ids:\n",
    "                \n",
    "                # append the information (id, user handle, date, tweet text, is retweet, is quote, quote text) \n",
    "                # as a tuple to the list\n",
    "                all_tweets.append((id_rt,user_rt,date_rt,text_rt,0,0,np.nan))\n",
    "            \n",
    "            # if the ID of the retweeted tweet is not yet in the original_id set\n",
    "            if id_rt not in original_ids:\n",
    "                \n",
    "                # append the retweeted information (which we treat as \"original\" tweets as well) to the 'original' list\n",
    "                original.append((id_rt, user_rt, date_rt, text_rt))\n",
    "            \n",
    "            # add the ids to the sets\n",
    "            all_ids.update([id_text, id_rt])\n",
    "            original_ids.add(id_rt)\n",
    "        \n",
    "        elif 'quoted_status' in tweet:\n",
    "            \n",
    "            # retrieve the user quoting and the quoted text\n",
    "            id_text = tweet['id_str']\n",
    "            user = tweet['user']['screen_name']\n",
    "            date = tweet['created_at']\n",
    "            text = tweet['full_text']\n",
    "            \n",
    "            # add the retweeted text as an \"original\" tweet incl. user/id/date information as well\n",
    "            id_qt = tweet['quoted_status']['id_str']\n",
    "            user_qt = tweet['quoted_status']['user']['screen_name']\n",
    "            date_qt = tweet['quoted_status']['created_at']\n",
    "            text_qt = tweet['quoted_status']['full_text']\n",
    "            \n",
    "            # if the ID of the quote is not in the all_ids set yet\n",
    "            if id_text not in all_ids:\n",
    "                \n",
    "                # append the information (id, user handle, date, tweet text, is retweet, is quote, quote text) \n",
    "                # as a tuple to the list\n",
    "                all_tweets.append((id_text,user,date,text,0,1,text_qt))\n",
    "            \n",
    "            # if the ID of the retweeted tweet is not yet in all_ids\n",
    "            if id_qt not in all_ids:\n",
    "                \n",
    "                # append the information (id, user handle, date, tweet text, is retweet, is quote, quote text) \n",
    "                # as a tuple to the list\n",
    "                all_tweets.append((id_qt,user_qt,date_qt,text_qt,0,0,np.nan))\n",
    "            \n",
    "            # if the ID of the retweeted tweet is not yet in the original_id set\n",
    "            if id_qt not in original_ids:\n",
    "                \n",
    "                # append the retweeted information (which we treat as \"original\" tweets as well) to the 'original' list\n",
    "                original.append((id_qt,user_qt,date_qt,text_qt))\n",
    "            \n",
    "            # add the ids to the sets\n",
    "            all_ids.update([id_text, id_qt])\n",
    "            original_ids.add(id_qt)\n",
    "            \n",
    "\n",
    "        # if the tweet is not a retweet\n",
    "        else: \n",
    "\n",
    "            # save the user who posted this tweet and the text\n",
    "            id_text = tweet['id_str']\n",
    "            user = tweet['user']['screen_name']\n",
    "            date = tweet['created_at']\n",
    "            text = tweet['full_text']\n",
    "\n",
    "            # if the ID of the tweet is not yet in the all_ids set\n",
    "            if id_text not in all_ids:\n",
    "   \n",
    "                # append the information (user handle, date, tweet text, is retweet, is quote, quote text) \n",
    "                # as a tuple to the tweet_tx list\n",
    "                all_tweets.append((id_text,user,date,text,0,0,np.nan))\n",
    "    \n",
    "            # if the ID of the tweet is not yet in the original_ids set\n",
    "            if id_text not in original_ids:\n",
    "                \n",
    "                # append the information to the 'original' list: we do this to be able to distinguish between\n",
    "                # retweets and original tweets\n",
    "                original.append((id_text,user,date,text))\n",
    "            \n",
    "            # add the ids to the sets\n",
    "            all_ids.add(id_text)\n",
    "            original_ids.add(id_text)\n",
    "            \n",
    "    # turning the lists into pandas dataframes\n",
    "    all_df = pd.DataFrame(all_tweets, columns = ['id','user','date','text','is_retweet','is_quote','quote'])\n",
    "    original_df = pd.DataFrame(original, columns = ['id','user', 'date', 'text'])\n",
    "\n",
    "    # turn the date column into pandas datetime\n",
    "    all_df['date'] = pd.to_datetime(all_df['date'])\n",
    "    original_df['date'] = pd.to_datetime(original_df['date'])\n",
    "    \n",
    "    return all_df, original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the function to the different datasets\n",
    "\n",
    "# German data\n",
    "de_all_text, de_original_text = tweets(de_final)\n",
    "\n",
    "# Danish data\n",
    "da_all_text, da_original_text = tweets(da_final)\n",
    "\n",
    "# Polish data\n",
    "pl_all_text, pl_original_text = tweets(pl_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if any irrelevant tweets are left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking if there are any irrelevant tweets left\n",
    "\n",
    "# Germany\n",
    "de = 0\n",
    "for tweet_id in de_all_text['id']:\n",
    "    if tweet_id in de_irr:\n",
    "        de += 1\n",
    "print(f'There are {de} issues with irrelevant tweets in the German data.')\n",
    "\n",
    "# Denmark\n",
    "da = 0\n",
    "for tweet_id in da_all_text['id']:\n",
    "    if tweet_id in da_irr:\n",
    "        da += 1\n",
    "print(f'There are {da} issues with irrelevant tweets in the Danish data.')\n",
    "\n",
    "# Poland\n",
    "pl = 0\n",
    "for tweet_id in pl_all_text['id']:\n",
    "    if tweet_id in pl_irr:\n",
    "        pl += 1\n",
    "print(f'There are {pl} issues with irrelevant tweets in the Polish data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if there are any duplicates left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check if we successfully removed all duplicates\n",
    "\n",
    "def check_duplicates(pd_dataframe):\n",
    "        \n",
    "    \"\"\"Takes dataframe as input, prints string comparing the length of the dataframe with the number of unique IDs.\"\"\"\n",
    "        \n",
    "    check_set = set()\n",
    "    \n",
    "    for i in pd_dataframe['id']:\n",
    "        check_set.add(i)\n",
    "        \n",
    "    print(f\"There are {pd_dataframe.shape[0]} tweets in the dataframe and {len(check_set)} of them are unique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the different dataframes\n",
    "print(f\"German data all: {check_duplicates(de_all_text)}\"), print(f\"German data original: {check_duplicates(de_original_text)}\")\n",
    "print(f\"Danish data all: {check_duplicates(da_all_text)}\"), print(f\"Danish data original: {check_duplicates(da_original_text)}\")\n",
    "print(f\"Polish data all: {check_duplicates(pl_all_text)}\"), print(f\"Polish data original: {check_duplicates(pl_original_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the length of the dataframe and displaying the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the length of the df and its first few rows\n",
    "for df in [de_all_text, da_all_text, pl_all_text]:\n",
    "    print(f\"There are {len(df)} tweets incl. retweets in this dataframe.\")\n",
    "    display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the length of the df and its first few rows\n",
    "for df in [de_original_text, da_original_text, pl_original_text]:\n",
    "    print(f\"There are {len(df)} original tweets in this dataframe.\")\n",
    "    display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the time period from which we've collected tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving the time period, i.e. the largest and smallest date\n",
    "time_df_list = [de_original_text, da_original_text, pl_original_text]\n",
    "\n",
    "language_list = ['German', 'Danish','Polish']\n",
    "\n",
    "for i in range(len(time_df_list)):\n",
    "    print(f\"{language_list[i]}: For the original tweets, the time period starts on {min(time_df_list[i]['date'])} and ends on {max(time_df_list[i]['date'])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Twitter interactions (retweets, quotes, mentions, replies)\n",
    "Here we create edgelists for retweets, quotes, mentions and replies. We are not yet sure if we need all of this information, or whether retweets will be enough, but we will extract all communicative interactions just in case we want to use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactions(list_of_final_tweets): \n",
    "    \n",
    "    \"\"\"\n",
    "    Returns edgelist for retweets, quotes, replies and mentions.\n",
    "    \n",
    "        Input: List containing tweets (tweet format: dict of dicts)\n",
    "\n",
    "        Output: Four pandas dataframes. Each dataframe contains the source and target user of a communicative interaction,\n",
    "        i.e. retweets, quotes, replies and mentions respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    # empty lists to store the retweet, quote, mention and reply information in\n",
    "    rt = []\n",
    "    quote = []\n",
    "    reply = []\n",
    "    mention = []\n",
    "\n",
    "    for tweet in list_of_final_tweets:\n",
    "\n",
    "        # retweets\n",
    "        if 'retweeted_status' in tweet.keys():\n",
    "            source = tweet['user']['screen_name']\n",
    "            target = tweet['retweeted_status']['user']['screen_name']\n",
    "\n",
    "            # add info to list\n",
    "            rt.append((source,target))\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # quotes\n",
    "        if 'quoted_status' in tweet:\n",
    "            source = tweet['user']['screen_name']\n",
    "            target = tweet['quoted_status']['user']\n",
    "\n",
    "            # add info to list\n",
    "            quote.append((source,target))\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # replies\n",
    "        if tweet['in_reply_to_screen_name']:\n",
    "            source = tweet['user']['screen_name']\n",
    "            target = tweet['in_reply_to_screen_name']\n",
    "\n",
    "            # add info to list\n",
    "            reply.append((source,target))\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "        # mentions\n",
    "\n",
    "        # check if the 'user_mentions' exists in the 'entities' dict\n",
    "        if tweet['entities']['user_mentions']:\n",
    "\n",
    "            # 'user_mentions' is a dict containing dicts for each user mentioned in the tweet text\n",
    "            # therefore, we retrieve the number of mentions (= length of the 'user_mentions' dict to catch them all)\n",
    "            number_mentions = range(len(tweet['entities']['user_mentions']))\n",
    "\n",
    "            # iterate through the number of mentions\n",
    "            for j in number_mentions:\n",
    "\n",
    "                source = tweet['user']['screen_name']\n",
    "                target = tweet['entities']['user_mentions'][j]['screen_name']\n",
    "\n",
    "                # add info to list\n",
    "                mention.append((source,target)) \n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # turning the lists into pandas dataframes\n",
    "    rt = pd.DataFrame(rt, columns = ['source', 'target'])\n",
    "    quote = pd.DataFrame(quote, columns = ['source', 'target'])\n",
    "    reply = pd.DataFrame(reply, columns = ['source', 'target'])\n",
    "    mention = pd.DataFrame(mention, columns = ['source', 'target'])\n",
    "    \n",
    "    return rt, quote, reply, mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the function to the different datasets\n",
    "\n",
    "# German data\n",
    "# de_edgelist_rt, de_edgelist_quote, de_edgelist_reply, de_edgelist_mention = interactions(de_final)\n",
    "\n",
    "# Danish data\n",
    "da_edgelist_rt, da_edgelist_quote, da_edgelist_reply, da_edgelist_mention = interactions(da_final)\n",
    "\n",
    "# Polish data\n",
    "# pl_edgelist_rt, pl_edgelist_quote, pl_edgelist_reply, pl_edgelist_mention = interactions(pl_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the dataframes\n",
    "We save all the different dataframes we've created as csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the different dataframes as files\n",
    "\n",
    "df_list = [\n",
    "#     de_all_text, de_original_text, de_edgelist_rt, de_edgelist_quote, de_edgelist_reply, de_edgelist_mention,\n",
    "    da_all_text, da_original_text, da_edgelist_rt, da_edgelist_quote, da_edgelist_reply, da_edgelist_mention,\n",
    "#     pl_all_text, pl_original_text, pl_edgelist_rt, pl_edgelist_quote, pl_edgelist_reply, pl_edgelist_mention\n",
    "]\n",
    "\n",
    "fil_name_list = [\n",
    "#     'de_all_text', 'de_original_text', 'de_edgelist_rt', 'de_edgelist_quote', 'de_edgelist_reply', 'de_edgelist_mention',\n",
    "    'da_all_text', 'da_original_text', 'da_edgelist_rt', 'da_edgelist_quote', 'da_edgelist_reply', 'da_edgelist_mention',\n",
    "#     'pl_all_text', 'pl_original_text', 'pl_edgelist_rt', 'pl_edgelist_quote', 'pl_edgelist_reply', 'pl_edgelist_mention'\n",
    "]\n",
    "\n",
    "for i in range(len(df_list)):\n",
    "    df_list[i].to_csv(f\"final_data_prepare1\\\\{fil_name_list[i]}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
