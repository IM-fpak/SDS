{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning - Classifying posts (Predicting sentiment labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.1\n"
     ]
    }
   ],
   "source": [
    "### load packages \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "## for explainer\n",
    "from lime import lime_text\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import nltk\n",
    "import string\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from nltk import word_tokenize, corpus\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "sns.set() # use seaborn plotting style\n",
    "from pactools.grid_search import GridSearchCVProgressBar # to view progressbar for grid search\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>lemma_meaningful</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['app', 'use', 'rh', 'dead']</td>\n",
       "      <td>app use rh dead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>['fud']</td>\n",
       "      <td>fud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>['amc', 'amc', 'amc', 'low', 'let', 'trade']</td>\n",
       "      <td>amc amc amc low let trade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>['serious', 'question', 'short', 'ladder', 'at...</td>\n",
       "      <td>serious question short ladder attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>['til', 'fucking', 'fuck', 'billionaire', 'fuc...</td>\n",
       "      <td>til fucking fuck billionaire fucker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31614</th>\n",
       "      <td>2</td>\n",
       "      <td>['robinhood', 'let', 'buy', 'gme', 'amc', 'fin...</td>\n",
       "      <td>robinhood let buy gme amc fine opened fidelity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31615</th>\n",
       "      <td>2</td>\n",
       "      <td>['help', 'get', 'tendies', 'deepfucking', 'val...</td>\n",
       "      <td>help get tendies deepfucking value part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31616</th>\n",
       "      <td>2</td>\n",
       "      <td>['dont', 'sell', 'look', 'buy', 'dont', 'let',...</td>\n",
       "      <td>dont sell look buy dont let short win easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31617</th>\n",
       "      <td>2</td>\n",
       "      <td>['best', 'yet', 'come', 'got', 'others', 'back...</td>\n",
       "      <td>best yet come got others back let gme amc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31618</th>\n",
       "      <td>2</td>\n",
       "      <td>['gme', 'proud', 'fight', 'beside', 'everyone']</td>\n",
       "      <td>gme proud fight beside everyone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31462 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment                                   lemma_meaningful  \\\n",
       "0              0                       ['app', 'use', 'rh', 'dead']   \n",
       "1              0                                            ['fud']   \n",
       "2              0       ['amc', 'amc', 'amc', 'low', 'let', 'trade']   \n",
       "3              0  ['serious', 'question', 'short', 'ladder', 'at...   \n",
       "4              0  ['til', 'fucking', 'fuck', 'billionaire', 'fuc...   \n",
       "...          ...                                                ...   \n",
       "31614          2  ['robinhood', 'let', 'buy', 'gme', 'amc', 'fin...   \n",
       "31615          2  ['help', 'get', 'tendies', 'deepfucking', 'val...   \n",
       "31616          2  ['dont', 'sell', 'look', 'buy', 'dont', 'let',...   \n",
       "31617          2  ['best', 'yet', 'come', 'got', 'others', 'back...   \n",
       "31618          2    ['gme', 'proud', 'fight', 'beside', 'everyone']   \n",
       "\n",
       "                                               processed  \n",
       "0                                        app use rh dead  \n",
       "1                                                    fud  \n",
       "2                              amc amc amc low let trade  \n",
       "3                   serious question short ladder attack  \n",
       "4                    til fucking fuck billionaire fucker  \n",
       "...                                                  ...  \n",
       "31614  robinhood let buy gme amc fine opened fidelity...  \n",
       "31615            help get tendies deepfucking value part  \n",
       "31616         dont sell look buy dont let short win easy  \n",
       "31617          best yet come got others back let gme amc  \n",
       "31618                    gme proud fight beside everyone  \n",
       "\n",
       "[31462 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "df=pd.read_csv(\"processed_sentiment_data.csv\").dropna()\n",
    "\n",
    "#look at data\n",
    "df\n",
    "\n",
    "# label encoding \n",
    "\n",
    "labels={\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "\n",
    "df = df.replace(labels)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Sentiment\", fontsize=12)\n",
    "df[\"Sentiment\"].reset_index().groupby(\"Sentiment\").count().sort_values(by= \n",
    "       \"index\").plot(kind=\"barh\", legend=False, \n",
    "        ax=ax).grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test split\n",
    "\n",
    "! First split data, then vectorize \n",
    "https://towardsdatascience.com/3-things-you-need-to-know-before-you-train-test-split-869dfabb7e50\n",
    "\n",
    "! Use stratified split, bc. we have imbalanced data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### VERSION 1 \n",
    "\n",
    "## split dataset\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, stratify=df['Sentiment'], random_state=4) \n",
    "\n",
    "## get target\n",
    "y_train = train_data[\"Sentiment\"].values\n",
    "y_test = test_data[\"Sentiment\"].values\n",
    "\n",
    "## get text\n",
    "X_train = train_data[\"processed\"].values\n",
    "X_test = test_data[\"processed\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### small data set\n",
    "\n",
    "smallData=df.loc[1:5000]\n",
    "\n",
    "sX = smallData['processed'] # Collection of documents\n",
    "sy = smallData['Sentiment'] # Target: The sentiment we want to predict (3 classes)\n",
    "\n",
    "sX_train, sX_test, sy_train, sy_test = train_test_split(sX, sy, test_size=0.2, stratify=smallData['Sentiment'], random_state=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piping: Vectorization (using TF-IDF) and multiple classifiers \n",
    "https://stackabuse.com/text-classification-with-python-and-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 10\n",
      "n_required_iterations: 12\n",
      "n_possible_iterations: 10\n",
      "min_resources_: 30\n",
      "max_resources_: 25169\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 3456\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 3456 candidates, totalling 17280 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 1728\n",
      "n_resources: 60\n",
      "Fitting 5 folds for each of 1728 candidates, totalling 8640 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 864\n",
      "n_resources: 120\n",
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 432\n",
      "n_resources: 240\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 216\n",
      "n_resources: 480\n",
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 108\n",
      "n_resources: 960\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 54\n",
      "n_resources: 1920\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 27\n",
      "n_resources: 3840\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "----------\n",
      "iter: 8\n",
      "n_candidates: 14\n",
      "n_resources: 7680\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "----------\n",
      "iter: 9\n",
      "n_candidates: 7\n",
      "n_resources: 15360\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "### build a pipeline\n",
    "tfidf=TfidfVectorizer()\n",
    "\n",
    "mnb_pipe = Pipeline([\n",
    "    ('vectorizer', tfidf),\n",
    "    ('classifier', MultinomialNB()),])\n",
    "\n",
    "# see paramerts for model \n",
    "mnb_pipe.get_params().keys()\n",
    "\n",
    "\n",
    "### define parameters to be tested usign k-fold CV\n",
    "mnb_params = {'vectorizer__max_features': [2000, 5000, 7000, 1000],\n",
    "                  'vectorizer__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
    "                  'vectorizer__max_df': (0.5, 0.6, 0.7),\n",
    "                  'vectorizer__min_df': [3,4],\n",
    "                  'vectorizer__stop_words': ['english', None],\n",
    "                  'vectorizer__smooth_idf': [True, False],\n",
    "                  'vectorizer__use_idf': [True, False],\n",
    "                  'classifier__fit_prior': [True, False],\n",
    "                  'classifier__alpha': [0.1, 0.5, 1],} # (1e-2, 1e-3)\n",
    "\n",
    "\n",
    "mnb_gs = HalvingGridSearchCV(mnb_pipe,mnb_params,cv=5,n_jobs=-1, verbose=1,factor=2)#5-fold, computation will be dispatched on all the CPUs\n",
    "mnb_gs = mnb_gs.fit(X_train, y_train)\n",
    "\n",
    "print('Done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score on training data: 0.8215980834753615\n",
      "\n",
      "Best score on testing data: 0.7620250690822619\n",
      "\n",
      "Best score 0.7311477853266586\n",
      "\n",
      "Best parameters {'classifier__alpha': 0.5, 'classifier__fit_prior': True, 'vectorizer__max_df': 0.7, 'vectorizer__max_features': 5000, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 2), 'vectorizer__smooth_idf': False, 'vectorizer__stop_words': None, 'vectorizer__use_idf': False}\n",
      "\n",
      "Pipeline(steps=[('vectorizer',\n",
      "                 TfidfVectorizer(max_df=0.7, max_features=5000, min_df=3,\n",
      "                                 ngram_range=(1, 2), smooth_idf=False,\n",
      "                                 use_idf=False)),\n",
      "                ('classifier', MultinomialNB(alpha=0.5))])\n",
      "{'classifier__alpha': 0.5, 'classifier__fit_prior': True, 'vectorizer__max_df': 0.7, 'vectorizer__max_features': 5000, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 2), 'vectorizer__smooth_idf': False, 'vectorizer__stop_words': None, 'vectorizer__use_idf': False}\n",
      "Accuracy: 0.76\n",
      "Auc: 0.91\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.70      0.74      1876\n",
      "           1       0.79      0.74      0.76      2123\n",
      "           2       0.73      0.83      0.78      2294\n",
      "\n",
      "    accuracy                           0.76      6293\n",
      "   macro avg       0.77      0.76      0.76      6293\n",
      "weighted avg       0.77      0.76      0.76      6293\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### get the best \n",
    "print('Best score on training data:',mnb_gs.score(X_train, y_train))\n",
    "print('')\n",
    "print('Best score on testing data:',mnb_gs.score(X_test, y_test))\n",
    "print('')\n",
    "print('Best score',mnb_gs.best_score_)\n",
    "print('')\n",
    "print('Best parameters',mnb_gs.best_params_)\n",
    "print('')\n",
    "\n",
    "# interpretation step 5 https://medium.com/@ishan16.d/text-classification-in-python-with-scikit-learn-and-nltk-891aa2d0ac4b\n",
    "\n",
    "# save the best model \n",
    "best_mnb=mnb_gs.best_estimator_\n",
    "mnb_params=mnb_gs.best_params_\n",
    "\n",
    "print(best_mnb)\n",
    "print(mnb_params)\n",
    "\n",
    "\n",
    "\n",
    "## test classifier \n",
    "y_pred = best_mnb.predict(X_test)\n",
    "predicted_prob = best_mnb.predict_proba(X_test)\n",
    "\n",
    "\n",
    "####################################################### Evaluation \n",
    "\n",
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "    \n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comapre to other models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vectorizer', 'classifier', 'vectorizer__analyzer', 'vectorizer__binary', 'vectorizer__decode_error', 'vectorizer__dtype', 'vectorizer__encoding', 'vectorizer__input', 'vectorizer__lowercase', 'vectorizer__max_df', 'vectorizer__max_features', 'vectorizer__min_df', 'vectorizer__ngram_range', 'vectorizer__norm', 'vectorizer__preprocessor', 'vectorizer__smooth_idf', 'vectorizer__stop_words', 'vectorizer__strip_accents', 'vectorizer__sublinear_tf', 'vectorizer__token_pattern', 'vectorizer__tokenizer', 'vectorizer__use_idf', 'vectorizer__vocabulary', 'classifier__C', 'classifier__class_weight', 'classifier__dual', 'classifier__fit_intercept', 'classifier__intercept_scaling', 'classifier__l1_ratio', 'classifier__max_iter', 'classifier__multi_class', 'classifier__n_jobs', 'classifier__penalty', 'classifier__random_state', 'classifier__solver', 'classifier__tol', 'classifier__verbose', 'classifier__warm_start'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### build a pipeline\n",
    "lg_pipe = Pipeline([\n",
    "    ('vectorizer', tfidf),\n",
    "    ('classifier', LogisticRegression(random_state=0)),])\n",
    "\n",
    "# see paramerts for model \n",
    "lg_pipe.get_params().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 10\n",
      "n_required_iterations: 11\n",
      "n_possible_iterations: 10\n",
      "min_resources_: 30\n",
      "max_resources_: 25169\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 1080\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 1080 candidates, totalling 5400 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-56682c8bdc23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mlg_gs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHalvingGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlg_pipe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlg_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f1_weighted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#5-fold, computation will be dispatched on all the CPUs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mlg_gs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlg_gs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_samples_orig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m    317\u001b[0m                             'n_resources': [n_resources] * n_candidates}\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m             results = evaluate_candidates(candidate_params, cv,\n\u001b[0m\u001b[0;32m    320\u001b[0m                                           more_results=more_results)\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### build a pipeline\n",
    "lg_pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', LogisticRegression(random_state=0)),])\n",
    "\n",
    "# see paramerts for model \n",
    "lg_pipe.get_params().keys()\n",
    "\n",
    "\n",
    "### define parameters to be tested usign k-fold CV ###CHANGE###\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n",
    "lg_params = {'classifier__penalty': ['l2'],\n",
    "             'classifier__C': [0.9, 0.8, 0.7],\n",
    "             'vectorizer__max_df': (0.5, 0.6, 0.7),\n",
    "             'vectorizer__min_df': [3,4,5],\n",
    "             'vectorizer__stop_words': ['english', None],\n",
    "             'classifier__solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "             'vectorizer__use_idf': [True, False],\n",
    "             'classifier__class_weight': [None, 'balanced'],\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "lg_gs = HalvingGridSearchCV(lg_pipe,lg_params,cv=5,n_jobs=-1, verbose=1, scoring='accuracy', factor=2)#5-fold, computation will be dispatched on all the CPUs\n",
    "lg_gs = lg_gs.fit(X_train, y_train)\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score on training data: 0.8854145973221026\n",
      "Best score on testing data: 0.8382329572540919\n",
      "Best score 0.8051954227382503\n",
      "Best parameters {'classifier__C': 0.8, 'classifier__class_weight': None, 'classifier__penalty': 'l2', 'classifier__solver': 'saga', 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 4, 'vectorizer__stop_words': 'english', 'vectorizer__use_idf': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### get the best model and params\n",
    "print('Best score on training data:',lg_gs.score(X_train, y_train))\n",
    "print('Best score on testing data:',lg_gs.score(X_test, y_test))\n",
    "print('Best score',lg_gs.best_score_)\n",
    "print('Best parameters',lg_gs.best_params_)\n",
    "\n",
    "print('')\n",
    "\n",
    "# interpretation step 5 https://medium.com/@ishan16.d/text-classification-in-python-with-scikit-learn-and-nltk-891aa2d0ac4b\n",
    "# save the best model \n",
    "best_lg=lg_gs.best_estimator_\n",
    "lg_params=lg_gs.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vectorizer', 'classifier', 'vectorizer__analyzer', 'vectorizer__binary', 'vectorizer__decode_error', 'vectorizer__dtype', 'vectorizer__encoding', 'vectorizer__input', 'vectorizer__lowercase', 'vectorizer__max_df', 'vectorizer__max_features', 'vectorizer__min_df', 'vectorizer__ngram_range', 'vectorizer__norm', 'vectorizer__preprocessor', 'vectorizer__smooth_idf', 'vectorizer__stop_words', 'vectorizer__strip_accents', 'vectorizer__sublinear_tf', 'vectorizer__token_pattern', 'vectorizer__tokenizer', 'vectorizer__use_idf', 'vectorizer__vocabulary', 'classifier__algorithm', 'classifier__leaf_size', 'classifier__metric', 'classifier__metric_params', 'classifier__n_jobs', 'classifier__n_neighbors', 'classifier__p', 'classifier__weights'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "### build a pipeline\n",
    "knn_pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', KNeighborsClassifier()),])\n",
    "\n",
    "# see paramerts for model \n",
    "knn_pipe.get_params().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 8\n",
      "n_required_iterations: 8\n",
      "n_possible_iterations: 8\n",
      "min_resources_: 196\n",
      "max_resources_: 25169\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 144\n",
      "n_resources: 196\n",
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 72\n",
      "n_resources: 392\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 36\n",
      "n_resources: 784\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 18\n",
      "n_resources: 1568\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 9\n",
      "n_resources: 3136\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 5\n",
      "n_resources: 6272\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 3\n",
      "n_resources: 12544\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 2\n",
      "n_resources: 25088\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "### build a pipeline\n",
    "knn_pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', KNeighborsClassifier()),]) #algorithm='auto'\n",
    "\n",
    "# see paramerts for model \n",
    "knn_pipe.get_params().keys()\n",
    "\n",
    "\n",
    "### define parameters to be tested usign k-fold CV ###CHANGE###\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n",
    "knn_params = {'classifier__n_neighbors': [1,10, 2],\n",
    "              'classifier__weights': ['uniform', 'distance'],\n",
    "              'classifier__metric': ['minkowski'],\n",
    "              'classifier__leaf_size': [20, 40, 1],\n",
    "              'classifier__p': [1,2],\n",
    "             'vectorizer__min_df': [4,5],\n",
    "             'vectorizer__use_idf': [True, False],}\n",
    "\n",
    "\n",
    "knn_gs = HalvingGridSearchCV(knn_pipe,knn_params,cv=5,n_jobs=-1, verbose=1, scoring='accuracy', factor=2)#5-fold, computation will be dispatched on all the CPUs\n",
    "knn_gs = knn_gs.fit(X_train, y_train)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score on training data: 0.9932853907584728\n",
      "Best score on testing data: 0.5493405371047195\n",
      "Best score 0.5718802324091709\n",
      "Best parameters {'classifier__leaf_size': 1, 'classifier__metric': 'minkowski', 'classifier__n_neighbors': 1, 'classifier__p': 2, 'classifier__weights': 'distance', 'vectorizer__min_df': 5, 'vectorizer__use_idf': True}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__leaf_size': 1,\n",
       " 'classifier__metric': 'minkowski',\n",
       " 'classifier__n_neighbors': 1,\n",
       " 'classifier__p': 2,\n",
       " 'classifier__weights': 'distance',\n",
       " 'vectorizer__min_df': 5,\n",
       " 'vectorizer__use_idf': True}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the best model and params\n",
    "print('Best score on training data:',knn_gs.score(X_train, y_train))\n",
    "print('Best score on testing data:',knn_gs.score(X_test, y_test))\n",
    "print('Best score',knn_gs.best_score_) \n",
    "print('Best parameters',knn_gs.best_params_)\n",
    "\n",
    "print('')\n",
    "\n",
    "# interpretation step 5 https://medium.com/@ishan16.d/text-classification-in-python-with-scikit-learn-and-nltk-891aa2d0ac4b\n",
    "# save the best model \n",
    "best_knn=knn_gs.best_estimator_\n",
    "knn_params=knn_gs.best_params_\n",
    "\n",
    "knn_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest\n",
    "See exercise 9\n",
    "https://www.kaggle.com/emanueleamcappella/random-forest-hyperparameters-tuning#4.-Machine-learning scroll to 4. \n",
    "https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f\n",
    "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 9\n",
      "n_required_iterations: 9\n",
      "n_possible_iterations: 9\n",
      "min_resources_: 98\n",
      "max_resources_: 25169\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 384\n",
      "n_resources: 98\n",
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 192\n",
      "n_resources: 196\n",
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 96\n",
      "n_resources: 392\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 48\n",
      "n_resources: 784\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 24\n",
      "n_resources: 1568\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 12\n",
      "n_resources: 3136\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 6\n",
      "n_resources: 6272\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 3\n",
      "n_resources: 12544\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----------\n",
      "iter: 8\n",
      "n_candidates: 2\n",
      "n_resources: 25088\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "### build a pipeline\n",
    "rf_pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', RandomForestClassifier(random_state=14)),])\n",
    "\n",
    "# see paramerts for model \n",
    "rf_pipe.get_params().keys()\n",
    "\n",
    "\n",
    "### define parameters to be tested usign k-fold CV ###CHANGE###\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n",
    "rf_params = {'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "             'classifier__n_estimators': [80, 90],\n",
    "             'classifier__max_depth': [3,4],\n",
    "             'vectorizer__min_df': [4,5],\n",
    "             'classifier__min_samples_split': [4, 5],            \n",
    "             'vectorizer__stop_words': ['english', None],\n",
    "             'vectorizer__use_idf': [True, False],\n",
    "             'classifier__bootstrap': [False, True],}\n",
    "\n",
    "\n",
    "\n",
    "rf_gs = HalvingGridSearchCV(rf_pipe,rf_params,cv=5,n_jobs=-1, verbose=1, scoring='accuracy', factor=2)#5-fold, computation will be dispatched on all the CPUs\n",
    "rf_gs = rf_gs.fit(X_train, y_train)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': True,\n",
       " 'classifier__max_depth': 4,\n",
       " 'classifier__max_features': 'sqrt',\n",
       " 'classifier__min_samples_split': 5,\n",
       " 'classifier__n_estimators': 90,\n",
       " 'vectorizer__min_df': 5,\n",
       " 'vectorizer__stop_words': 'english',\n",
       " 'vectorizer__use_idf': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf=rf_gs.best_estimator_\n",
    "best_rf\n",
    "\n",
    "rf_gs.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score on training data: 0.46016925583058527\n",
      "Best score on testing data: 0.45097727633878915\n",
      "Best score 0.4700998979863996\n",
      "Best parameters {'classifier__bootstrap': True, 'classifier__max_depth': 4, 'classifier__max_features': 'sqrt', 'classifier__min_samples_split': 5, 'classifier__n_estimators': 90, 'vectorizer__min_df': 5, 'vectorizer__stop_words': 'english', 'vectorizer__use_idf': False}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier__bootstrap': True,\n",
       " 'classifier__max_depth': 4,\n",
       " 'classifier__max_features': 'sqrt',\n",
       " 'classifier__min_samples_split': 5,\n",
       " 'classifier__n_estimators': 90,\n",
       " 'vectorizer__min_df': 5,\n",
       " 'vectorizer__stop_words': 'english',\n",
       " 'vectorizer__use_idf': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the best model and params\n",
    "print('Best score on training data:',rf_gs.score(X_train, y_train))\n",
    "print('Best score on testing data:',rf_gs.score(X_test, y_test))\n",
    "print('Best score',rf_gs.best_score_)\n",
    "print('Best parameters',rf_gs.best_params_)\n",
    "\n",
    "print('')\n",
    "\n",
    "# interpretation step 5 https://medium.com/@ishan16.d/text-classification-in-python-with-scikit-learn-and-nltk-891aa2d0ac4b\n",
    "# save the best model \n",
    "best_rf=rf_gs.best_estimator_\n",
    "rf_params=rf_gs.best_params_\n",
    "\n",
    "rf_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 6\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 786\n",
      "max_resources_: 25169\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 48\n",
      "n_resources: 786\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 24\n",
      "n_resources: 1572\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 12\n",
      "n_resources: 3144\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 6\n",
      "n_resources: 6288\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 3\n",
      "n_resources: 12576\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 2\n",
      "n_resources: 25152\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "### build a pipeline\n",
    "svm_pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', SVC(random_state=2)),])\n",
    "\n",
    "# see paramerts for model \n",
    "svm_pipe.get_params().keys()\n",
    "\n",
    "\n",
    "### define parameters to be tested usign k-fold CV ###CHANGE###\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n",
    "svm_params = {'classifier__gamma': ['scale'],\n",
    "             'vectorizer__max_df': (0.4, 0.5),\n",
    "             'vectorizer__min_df': [4,5],         \n",
    "             'vectorizer__stop_words': ['english', None],\n",
    "             'vectorizer__use_idf': [True, False], \n",
    "             'classifier__kernel': ['poly', 'rbf', 'sigmoid'],}\n",
    " \n",
    "    \n",
    "\n",
    "svm_gs = HalvingGridSearchCV(svm_pipe,svm_params,cv=5,n_jobs=-1, verbose=1, scoring='accuracy', factor=2)#5-fold, computation will be dispatched on all the CPUs\n",
    "svm_gs = svm_gs.fit(X_train, y_train)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score on training data: 0.9679367475863165\n",
      "Best score on testing data: 0.8676307007786429\n",
      "Best score 0.8599944417804171\n",
      "Best parameters {'classifier__gamma': 'scale', 'classifier__kernel': 'rbf', 'vectorizer__max_df': 0.4, 'vectorizer__min_df': 4, 'vectorizer__stop_words': None, 'vectorizer__use_idf': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### get the best model and params\n",
    "print('Best score on training data:',svm_gs.score(X_train, y_train))\n",
    "print('Best score on testing data:',svm_gs.score(X_test, y_test))\n",
    "print('Best score',svm_gs.best_score_)\n",
    "print('Best parameters',svm_gs.best_params_)\n",
    "\n",
    "print('')\n",
    "\n",
    "# interpretation step 5 https://medium.com/@ishan16.d/text-classification-in-python-with-scikit-learn-and-nltk-891aa2d0ac4b\n",
    "\n",
    "# save the best model \n",
    "best_svm=svm_gs.best_estimator_\n",
    "svm_params=svm_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosted tree\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost classifier\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ (tuning)\n",
    "https://hackernoon.com/want-a-complete-guide-for-xgboost-model-in-python-using-scikit-learn-sc11f31bq \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 9\n",
      "n_required_iterations: 9\n",
      "n_possible_iterations: 9\n",
      "min_resources_: 98\n",
      "max_resources_: 25169\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 432\n",
      "n_resources: 98\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-fcbe5a24c3cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mxgb_gs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHalvingGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb_pipe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxgb_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#5-fold, computation will be dispatched on all the CPUs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mxgb_gs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_gs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_samples_orig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m    317\u001b[0m                             'n_resources': [n_resources] * n_candidates}\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m             results = evaluate_candidates(candidate_params, cv,\n\u001b[0m\u001b[0;32m    320\u001b[0m                                           more_results=more_results)\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### XGBoost Classifier\n",
    "### build a pipeline\n",
    "xgb_pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', XGBClassifier(random_state=14, verbosity=2)),])\n",
    "\n",
    "# see paramerts for model \n",
    "xgb_pipe.get_params().keys()\n",
    "\n",
    "\n",
    "### define parameters to be tested usign k-fold CV ###CHANGE###\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n",
    "xgb_params = {'classifier__learning_rate': [ 0.07, 0.1, 0.2],\n",
    "              'classifier__booster': ['dart', 'gbtree'], \n",
    "              'classifier__max_depth' : [6,7,8],\n",
    "              'vectorizer__max_df': (0.5, 0.6, 0.7),\n",
    "              'vectorizer__min_df': [3,4],\n",
    "              'classifier__use_label_encoder' : [True, False],\n",
    "              'vectorizer__use_idf': [True, False],}\n",
    " \n",
    "\n",
    "\n",
    "xgb_gs = HalvingGridSearchCV(xgb_pipe,xgb_params,cv=5,n_jobs=-1, verbose=1, scoring='accuracy', factor=2)#5-fold, computation will be dispatched on all the CPUs\n",
    "xgb_gs = xgb_gs.fit(X_train, y_train)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the best model and params\n",
    "print('Best score on training data:',xgb_gs.score(X_train, y_train))\n",
    "print('Best score on testing data:',xgb_gs.score(X_test, y_test))\n",
    "print('Best score',xgb_gs.best_score_)\n",
    "print('Best parameters',xgb_gs.best_params_)\n",
    "\n",
    "print('')\n",
    "\n",
    "# interpretation step 5 https://medium.com/@ishan16.d/text-classification-in-python-with-scikit-learn-and-nltk-891aa2d0ac4b\n",
    "# save the best model \n",
    "best_xgb=xgb_gs.best_estimator_\n",
    "xgb_params=xgb_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adaboost Classifier\n",
    "#https://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-bd9de589adf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mbag_gs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHalvingGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbag_pipe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbag_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#5-fold, computation will be dispatched on all the CPUs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mbag_gs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbag_gs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_samples_orig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m    317\u001b[0m                             'n_resources': [n_resources] * n_candidates}\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m             results = evaluate_candidates(candidate_params, cv,\n\u001b[0m\u001b[0;32m    320\u001b[0m                                           more_results=more_results)\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### build a pipeline\n",
    "bag_pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', BaggingClassifier(random_state=14)),])\n",
    "\n",
    "#{'classifier__n_estimators': 100, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 4, 'vectorizer__use_idf': False}\n",
    "\n",
    "# see paramerts for model \n",
    "bag_pipe.get_params().keys()\n",
    "\n",
    "\n",
    "### define parameters to be tested usign k-fold CV ###CHANGE###\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n",
    "bag_params = {'classifier__n_estimators': [100, 200],\n",
    "             'vectorizer__max_df': (0.5, 0.6, 0.7),\n",
    "             'vectorizer__min_df': [4,5],\n",
    "             'vectorizer__use_idf': [True, False],}\n",
    "\n",
    "\n",
    "# s coring='accuracy', verbose=1\n",
    "\n",
    "bag_gs = HalvingGridSearchCV(bag_pipe,bag_params,cv=5,n_jobs=-1, verbose=0, factor=2)#5-fold, computation will be dispatched on all the CPUs\n",
    "bag_gs = bag_gs.fit(X_train, y_train)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HalvingGridSearchCV' object has no attribute 'scorer_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-7714d77b4c27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m### get the best model and params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best score on training data:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbag_gs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best score on testing data:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbag_gs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbag_gs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best parameters'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbag_gs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    472\u001b[0m         \"\"\"\n\u001b[0;32m    473\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscorer_\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m             raise ValueError(\"No score function explicitly defined, \"\n\u001b[0;32m    476\u001b[0m                              \u001b[1;34m\"and the estimator doesn't provide one %s\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HalvingGridSearchCV' object has no attribute 'scorer_'"
     ]
    }
   ],
   "source": [
    "### get the best model and params\n",
    "print('Best score on training data:',bag_gs.score(X_train, y_train))\n",
    "print('Best score on testing data:',bag_gs.score(X_test, y_test))\n",
    "print('Best score',bag_gs.best_score_)\n",
    "print('Best parameters',bag_gs.best_params_)\n",
    "\n",
    "print('')\n",
    "\n",
    "# interpretation step 5 https://medium.com/@ishan16.d/text-classification-in-python-with-scikit-learn-and-nltk-891aa2d0ac4b\n",
    "# save the best model \n",
    "best_bag=bag_gs.best_estimator_\n",
    "bag_params=bag_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HalvingGridSearchCV' object has no attribute 'best_score_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-97e8f8d2fd1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# summarize results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best: %f using %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbag_gs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbag_gs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbag_gs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbag_gs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'std_test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbag_gs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HalvingGridSearchCV' object has no attribute 'best_score_'"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (bag_gs.best_score_, bag_gs.best_params_))\n",
    "means = bag_gs.cv_results_['mean_test_score']\n",
    "stds = bag_gs.cv_results_['std_test_score']\n",
    "params = bag_gs.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best svm score:',svm_gs.best_score_,'with model parameters: ', svm_params)\n",
    "print('')\n",
    "print('Best mnb score:',mnb_gs.best_score_, 'with model parameters: ', mnb_params)\n",
    "print('')\n",
    "print('Best lg score:',lg_gs.best_score_,'with model parameters: ', lg_params)\n",
    "print('')\n",
    "print('Best rf score:',rf_gs.best_score_, 'with model parameters: ', rf_params)\n",
    "print('')\n",
    "print('Best XGBoost score:',xgb_gs.best_score_, 'with model parameters: ', xgb_params)\n",
    "print('')\n",
    "print('Best KNN score:',knn_gs.best_score_, 'with model parameters: ', knn_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=3000,ngram_range=(1, 3), min_df=5, stop_words=stopwords.words('english'))\n",
    "\n",
    "features = tfidf.fit_transform(X_train).toarray()#only fit vestorizer to the training data\n",
    "vectorized_features = tfidf.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### investigating the vocabulary and their tfidf \"values\"\n",
    "vocab = pd.DataFrame(vectorized_features[0].T.todense(), index=tfidf.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "vocab = vocab.sort_values('TF-IDF', ascending=False)\n",
    "print (vocab.head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a simple model \n",
    "https://sanjayasubedi.com.np/machinelearning/nlp/text-classification-with-sklearn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the classifier \n",
    "mnb = MultinomialNB()\n",
    "# transform the list of text to tf-idf before passing it to the model\n",
    "mnb.fit(tfidf.transform(X_train), y_train)\n",
    " \n",
    "# checking accuracy \n",
    " \n",
    "y_pred = mnb.predict(tfidf.transform(X_test))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a pipeline to build the same classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##specify the classifier \n",
    "mnb = MultinomialNB()### Pipeline for NB\n",
    "\n",
    "## pipeline\n",
    "pipe_mnb = Pipeline([(\"vectorizer\", tfidf),  \n",
    "                           (\"classifier\", mnb)])\n",
    "## train classifier\n",
    "pipe_mnb.fit(X_train, y_train)\n",
    "\n",
    "## test \n",
    "y_pred = pipe_mnb.predict(X_test)\n",
    "predicted_prob = pipe_mnb.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search to finetune hyperparameters\n",
    "Almost all the classifiers will have various parameters which can be tuned to obtain optimal performance. \n",
    "\n",
    "https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see paramerts for model \n",
    "pipe_mnb.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paramter tuning \n",
    "\n",
    "https://medium.com/@ishan16.d/text-classification-in-python-with-scikit-learn-and-nltk-891aa2d0ac4b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define parameters to be tested usign k-fold CV\n",
    "\n",
    "parameters_mnb = {'vectorizer__max_features': [2000, 3000, 4000],\n",
    "                  'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'vectorizer__stop_words': ['english', None],\n",
    "                  'vectorizer__fit_prior': [True, False],\n",
    "                  'vectorizer__use_idf': [True, False],\n",
    "                  'classifier__alpha': [0.1, 0.5, 1]\n",
    "                 }\n",
    "\n",
    "gs_mnnb = GridSearchCVProgressBar(pipe_mnb,parameters_mnb,cv=5,n_jobs=-1, verbose=1)#5-fold, computation will be dispatched on all the CPUs\n",
    "gs_mnnb=gs_mnnb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the best \n",
    "\n",
    "print('Best score on training data:',gs_mnnb.score(X_train, y_train))\n",
    "print('Best score on testing data:',gs_mnnb.score(X_test, y_test))\n",
    "print('Best score',gs_mnnb.best_score_)\n",
    "print('Best parameters',gs_mnnb.best_params_)\n",
    "\n",
    "# interpretation step 5 https://medium.com/@ishan16.d/text-classification-in-python-with-scikit-learn-and-nltk-891aa2d0ac4b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set the best params to the model \n",
    "\n",
    "#pipe_mnb.set_params(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation \n",
    "https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation \n",
    "\n",
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "    \n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "    \n",
    "## Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "       yticklabels=classes, title=\"Confusion matrix\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                              metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "    \n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an explainer  \n",
    "https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select observation\n",
    "i = 2749 # cahnge to look for different posts/titles \n",
    "txt_instance = test_data[\"processed\"].iloc[i]\n",
    "## check true value and predicted value\n",
    "print(\"True:\", y_test[i], \"--> Pred:\", y_pred[i], \"| Prob:\", round(np.max(predicted_prob[i]),2))\n",
    "## show explanation\n",
    "explainer = lime_text.LimeTextExplainer(class_names=\n",
    "             np.unique(y_train))\n",
    "explained = explainer.explain_instance(txt_instance, \n",
    "             model.predict_proba, num_features=3)\n",
    "explained.show_in_notebook(text=txt_instance, predict_proba=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a random forrest classifier\n",
    "\n",
    "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "https://www.kaggle.com/emanueleamcappella/random-forest-hyperparameters-tuning \n",
    "https://medium.com/@benfenison/gridsearching-a-random-forest-classifier-fc225609699c\n",
    "https://www.kaggle.com/pepacz/randomforestclassifier-with-sklearn-pipeline\n",
    "\n",
    "Documentation \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the classifier\n",
    "rf_c = RandomForestClassifier()\n",
    "\n",
    "# make pipeline \n",
    "rf_pipe = Pipeline([(\"scaler\", tfidf),  # #StandardScaler()\n",
    "                    (\"rf\", rf_c)])\n",
    "\n",
    "# specify grid search parameters \n",
    "rf_params = [\n",
    "        {'bootstrap': [False, True],\n",
    "         'n_estimators': [10, 100, 500],\n",
    "         'max_features': [0.6, 0.65, 0.7, 0.73, 0.7500000000000001, 0.78, 0.8],\n",
    "         'min_samples_leaf': [10, 12, 14]\n",
    "        },]\n",
    "\n",
    "\n",
    "# create the GridSearchCV object\n",
    "rf_gs = GridSearchCVProgressBar(rf_c, rf_params, cv=2,n_jobs=-1, verbose=1,scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "# fine-tune the hyperparameters\n",
    "rf_gs=rf_gs.fit(X_train, y_train)\n",
    "\n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score on testing data:',gs_mnnb.score(X_test, y_test))\n",
    "print('Best score',gs_mnnb.best_score_)\n",
    "print('Best parameters',gs_mnnb.best_params_)# get the best model\n",
    "print('Best parameters',rf_gs.best_estimator__)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipe = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logisitc regression \n",
    "https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn pipelines to train several models \n",
    "\n",
    "Pipelines allow us to add the necessary steps for a model to do its task. In our case, we need to convert the raw texts into vectorized format and then pass it to the model. Pipeline allows us to group these related steps. We can consider a Pipeline object as a model itself i.e. we can call fit, predict functions.\n",
    "\n",
    "https://sanjayasubedi.com.np/machinelearning/nlp/text-classification-with-sklearn/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://sanjayasubedi.com.np/machinelearning/nlp/text-classification-with-sklearn/\n",
    "\n",
    "# start with the classic\n",
    "# with either pure counts or tfidf features\n",
    "sgd = Pipeline([\n",
    "        (\"count vectorizer\", CountVectorizer(stop_words=\"english\", max_features=3000)),\n",
    "        (\"sgd\", SGDClassifier(loss=\"modified_huber\"))\n",
    "    ])\n",
    "sgd_tfidf = Pipeline([\n",
    "        (\"tfidf_vectorizer\", TfidfVectorizer(stop_words=\"english\", max_features=3000)),\n",
    "        (\"sgd\", SGDClassifier(loss=\"modified_huber\"))\n",
    "    ])\n",
    " \n",
    "svc = Pipeline([\n",
    "        (\"count_vectorizer\", CountVectorizer(stop_words=\"english\", max_features=3000)),\n",
    "        (\"linear svc\", SVC(kernel=\"linear\"))\n",
    "    ])\n",
    "svc_tfidf = Pipeline([\n",
    "        (\"tfidf_vectorizer\", TfidfVectorizer(stop_words=\"english\", max_features=3000)),\n",
    "        (\"linear svc\", SVC(kernel=\"linear\"))\n",
    "    ])\n",
    "   \n",
    "all_models = [\n",
    "    (\"sgd\", sgd),\n",
    "    (\"sgd_tfidf\", sgd_tfidf),\n",
    "    (\"svc\", svc),\n",
    "    (\"svc_tfidf\", svc_tfidf),\n",
    "    ]\n",
    " \n",
    "unsorted_scores = [(name, cross_val_score(model, X_train, y_train, cv=5).mean()) for name, model in all_models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svc\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
