{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiyng tweets with Active Learning: Denmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Label tweets  \n",
    "\n",
    "**`Instructions`**\n",
    "\n",
    "> Start by labelling around 10-15 tweets in following chunk \n",
    "\n",
    "> Press the `retrain` button \n",
    "\n",
    "> Continue untill the `progress bar` is improved **or** the `accuracy` measure is satisfactory \n",
    "\n",
    "> Save the tweets and their labels in a dictionary *(Which will be used to train the supervised classifiers)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules \n",
    "from superintendent import ClassLabeller\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "import random\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import ast\n",
    "from tabulate import tabulate\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data \n",
    "\n",
    "# function to turn the tokenized/lemmatized list into a readable format\n",
    "def string_list(text):\n",
    "    \n",
    "    # we transform the string representation of the list into an actual list\n",
    "    text = ast.literal_eval(text)\n",
    "    \n",
    "    # return the transformed text\n",
    "    return text\n",
    "\n",
    "# import data\n",
    "df = pd.read_csv('da_preprocess.csv')\n",
    "\n",
    "# apply function: YOU NEED TO SPECIFY ALL RELEVANT COLUMNS HERE\n",
    "df['token'] = df['token'].apply(string_list)\n",
    "df['lemma'] = df['lemma'].apply(string_list)\n",
    "df['token_no_mention'] = df['token_no_mention'].apply(string_list)\n",
    "df['lemma_no_mention'] = df['lemma_no_mention'].apply(string_list)\n",
    "\n",
    "df['lemma_no_mention'] = df['lemma_no_mention'].astype(str)\n",
    "# print the dataframe\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Log.reg params:** \n",
    "\n",
    "https://machinelearningmastery.com/multinomial-logistic-regression-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initiate a model using multinomial logistic regression \n",
    "ALmodel = Pipeline([\n",
    "    ('tfidf_vectorizer', TfidfVectorizer()),\n",
    "    ('logistic_regression', LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\"))]) # max_iter=5000 + solver=\"lbfgs\"])\n",
    "\n",
    "\n",
    "### define display function to show the preprocessed text for manual labelling\n",
    "def display_text(df):\n",
    "    display(Markdown(df[\"preprocess\"]))\n",
    "    \n",
    "    \n",
    "### define preprocessor to only train the model on the lemmas (no mention)\n",
    "def preprocessor(x,y):\n",
    "    return x['lemma_no_mention'], y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create widget for labelling \n",
    "widget = ClassLabeller(\n",
    "    features=df, # specify column \n",
    "    model=ALmodel,\n",
    "    display_func=display_text, # use the display function to show the text\n",
    "    model_preprocess=preprocessor,\n",
    "    options=['vaxx','anti-vaxx','neutral','trash'], # specify the label options (r=remove)\n",
    "    acquisition_function='certainty') # specify sampling strategy \n",
    "\n",
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the indexes and labels to dictionary \n",
    "labels = widget.queue.labels\n",
    "\n",
    "### map the dict to a dataframe \n",
    "labelframe = pd.DataFrame.from_dict(labels, orient='index')\n",
    "labelframe.columns = ['label']\n",
    "\n",
    "labelframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    " 1. Inspect a random tweet by first providing a label to the widget **AND** make sure you can recognize the text later\n",
    " 2. Then, run the chunk above to make sure the label is saved with its index \n",
    " 3. Run the chunk below and inspect if the tweet has the same index in the original dataframe. Below, **Remember to sepcify the tweet index in line 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### inspect that the tweets are shuffelled at each retrianing of the model BUT KEEPS THEIR INDEX \n",
    "print(labelframe.tail(10))\n",
    "      \n",
    "# label a tweet in the widget (and remember it) > find index in dictionary > find tweet in df using index > same tweet? \n",
    "df.iloc[1379]['preprocess'] #change to the index number we want to inspect + column we want to inspect \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dataframe with the manually labelled tweets** (We will later split this and use as both training and testing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge the dataframes \n",
    "labeldf = df.merge(labelframe, left_index=True, right_index=True)\n",
    "labeldf\n",
    "\n",
    "\n",
    "### drop the columns we don't need \n",
    "labeldf = labeldf.drop(['date', 'preprocess','preprocess_no_mention',\n",
    "       'token', 'lemma', 'token_no_mention'], axis = 1)\n",
    "\n",
    "\n",
    "### drop the rows that aren't danish (marked 'r' during active learning)\n",
    "# Get indexes where label column has value 'r'\n",
    "#indexes= labeldf[(labeldf['label'] == 'r')].index\n",
    "# Delete these row indexes from dataFrame\n",
    "#labeldf.drop(indexes, inplace=True)\n",
    "\n",
    "# inspect df\n",
    "labeldf.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the labelled data as a csv-file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the df \n",
    "labeldf.to_csv(r'C:\\Users\\Frederikke\\OneDrive\\MSc. Social Data Science\\Exam\\da_labelsNew.csv') # change to own directory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter tuning and model training \n",
    "\n",
    "In this part, we will use the labelled tweets *(from step 1)* to train several supervised classifiers `(SVM, Multinomial Logistic Regression, Baggin, Boosting, Multinomial NaÃ¯ve Bayes)`\n",
    "\n",
    "\n",
    "Moreover, we will use `grid search` to tune the hyperparameters of the classifiers. This will be performed as an integrated part of the training, using cross-validation.\n",
    "\n",
    "> Which hyperparameters to tune, and which vlaues to consider, is dependent on the respective classifier and the datasets. Hence, the **search space of the `grid-search` will differ**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 : Load classifiers, define search spaces, create pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "###  preprocessing and classifiers \n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk import word_tokenize, corpus\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "from sklearn.svm import (SVC, LinearSVC)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "# If you have trouble importing the following two functions: \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "# evaluation \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, make train/test split, prepare for pipeline \n",
    "data = pd.read_csv('da_labels.csv')\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize class proprotions:** Use `stratified split` if imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the class proportions \n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Label\", fontsize=12)\n",
    "data[\"label\"].reset_index().groupby(\"label\").count().sort_values(by= \n",
    "       \"index\").plot(kind=\"bar\", legend=False, \n",
    "        ax=ax).grid(axis='x') \n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(data.groupby(\"label\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a stratified train/test split (on label) to ensure representative class proportions in the train/test sets\n",
    "X = data['lemma_no_mention'] # Collection of documents (tweets)\n",
    "y = data['label'] # labels  (4 classes)\n",
    "\n",
    "# split in to train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=data['label'],random_state=24) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 : Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build a pipeline\n",
    "lg_pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', LogisticRegression(random_state=1, multi_class='auto', penalty='l2')),])\n",
    "\n",
    "\n",
    "\n",
    "### define parameters to be tested usign k-fold CV\n",
    "lg_params = {'classifier__C': [0.5,1,2,3,4,5], # try [0.1, 1, 10, 100, 1000] 0.01,0.1,\n",
    "             'vectorizer__max_df': (0.6,0.7,0.9,0.99),\n",
    "             'vectorizer__min_df': [0.01,0.001,0.0001],\n",
    "             'vectorizer__ngram_range' : [(1,1),(1,2),(2,2), (1,3), (2,3)],\n",
    "             'classifier__solver' : ['newton-cg', 'lbfgs', 'liblinear'], # 'sag', 'saga' not for danish\n",
    "             'vectorizer__use_idf': [True, False],\n",
    "             'classifier__class_weight': [None, 'balanced'],\n",
    "            }\n",
    "\n",
    "### perform gridsearch using clf and params ! \n",
    "lg_gs = HalvingGridSearchCV(lg_pipe,lg_params,cv=5,n_jobs=-1, verbose=1,scoring='f1_micro', factor=2) #  #5-fold, n_jobs =-1 :computation will be dispatched on all the CPUs\n",
    "\n",
    "### train best estimator on traning data \n",
    "lg_gs = lg_gs.fit(X_train, y_train)\n",
    "#print('Best score on training data:',lg_gs.score(X_train, y_train))\n",
    "#print('Best score on testing data:',lg_gs.score(X_test, y_test))\n",
    "#print('Best score',lg_gs.best_score_)\n",
    "print('Best parameters',lg_gs.best_params_)\n",
    "\n",
    "\n",
    "## test classifier \n",
    "y_pred = lg_gs.best_estimator_.predict(X_test)\n",
    "predicted_prob = lg_gs.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "\n",
    "############################################## Evaluation ####################################################\n",
    "\n",
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "\n",
    "\n",
    "### Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class='ovr') # or ovo \n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Logistic Regression Details:\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    \n",
    "### Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Predicted label\", ylabel=\"True label\", xticklabels=classes, \n",
    "       yticklabels=classes, title=\"Confusion matrix: Logistic Regression\")\n",
    "plt.yticks(rotation=0)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "### Plot ROC-curve (to illustrate trade-off between sensitivity (or TPR) and specificity (1 â FPR) )\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                              metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "\n",
    "    \n",
    "### Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### Confusion matrix vol 2.0\n",
    "labels = classes \n",
    "print(classification_report(y_test, y_pred, labels)) #classification report from sklearn\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "plt.imshow(cnf_matrix, cmap=plt.cm.Blues) #plot confusion matrix grid\n",
    "threshold = cnf_matrix.max() / 2 #threshold to define text color\n",
    "for i in range(cnf_matrix.shape[0]): #print text in grid\n",
    "    for j in range(cnf_matrix.shape[1]): \n",
    "        plt.text(j, i, cnf_matrix[i,j], color=\"w\" if cnf_matrix[i,j] > threshold else 'black')\n",
    "tick_marks = np.arange(len(labels)) #define labeling spacing based on number of classes\n",
    "plt.xticks(tick_marks, labels, rotation=45)\n",
    "plt.yticks(tick_marks, labels)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 :  Multinomial NaÃ¯ve Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build a pipeline\n",
    "mnb_pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB()),])\n",
    "\n",
    "# see paramerts for model \n",
    "mnb_pipe.get_params().keys()\n",
    "\n",
    "\n",
    "### define parameters to be tested usign k-fold CV\n",
    "mnb_params = {'vectorizer__max_features': [1000,1500,2000,2500], #5000, 7000\n",
    "                  'vectorizer__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
    "                  'vectorizer__max_df': (0.6, 0.8, 0.9, 0.99),\n",
    "                 # 'vectorizer__min_df': [3,4],\n",
    "                  'vectorizer__stop_words': ['english', None],\n",
    "                  'vectorizer__smooth_idf': [True, False],\n",
    "                  'vectorizer__use_idf': [True, False],\n",
    "                  'classifier__fit_prior': [True, False],\n",
    "                  'classifier__alpha': [0.6, 0.7, 0.8],} # (1e-2, 1e-3) \n",
    "\n",
    "\n",
    "mnb_gs = HalvingGridSearchCV(mnb_pipe,mnb_params,cv=5,n_jobs=-1,scoring='f1_micro',verbose=1,factor=2)#5-fold, computation will be dispatched on all the CPUs\n",
    "\n",
    "\n",
    "### train best estimator on traning data \n",
    "mnb_gs = mnb_gs.fit(X_train, y_train)\n",
    "print('Best score on training data:',mnb_gs.score(X_train, y_train))\n",
    "print('Best score on testing data:',mnb_gs.score(X_test, y_test))\n",
    "print('Best score',mnb_gs.best_score_)\n",
    "print('Best parameters',mnb_gs.best_params_)\n",
    "\n",
    "\n",
    "## test classifier \n",
    "y_pred = mnb_gs.predict(X_test)\n",
    "predicted_prob = mnb_gs.predict_proba(X_test)\n",
    "\n",
    "####################################################### Evaluation \n",
    "\n",
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "    \n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Multinomial NaÃ¯ve Bayes Details:\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "    \n",
    "## Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "       yticklabels=classes, title=\"Confusion matrix: Multinomial NaÃ¯ve Bayes\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                              metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "    \n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()\n",
    "\n",
    "#### Other confusion matrix\n",
    "\n",
    "labels = classes \n",
    "print(classification_report(y_test, y_pred, labels)) #classification report from sklearn\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "plt.imshow(cnf_matrix, cmap=plt.cm.Blues) #plot confusion matrix grid\n",
    "threshold = cnf_matrix.max() / 2 #threshold to define text color\n",
    "for i in range(cnf_matrix.shape[0]): #print text in grid\n",
    "    for j in range(cnf_matrix.shape[1]): \n",
    "        plt.text(j, i, cnf_matrix[i,j], color=\"w\" if cnf_matrix[i,j] > threshold else 'black')\n",
    "tick_marks = np.arange(len(labels)) #define labeling spacing based on number of classes\n",
    "plt.xticks(tick_marks, labels, rotation=45)\n",
    "plt.yticks(tick_marks, labels)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 :  Support vector machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build a pipeline\n",
    "svm_pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', SVC(random_state=2,probability=True)),]) # probability must be = True, otherwise no 'predict_proba' \n",
    "\n",
    "# see paramerts for model \n",
    "svm_pipe.get_params().keys()\n",
    "\n",
    "\n",
    "### define parameters to be tested usign k-fold CV ###CHANGE###\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n",
    "svm_params = {'classifier__gamma': [0.6,0.5,0.4,0.3,0.1], # ['scale', 'auto'] 0.01,0.001 \n",
    "             'vectorizer__max_df': [0.5, 0.6, 0.7],\n",
    "            # 'vectorizer__min_df': [4,5],\n",
    "            # 'classifier__class_weight' : ['balanced'],\n",
    "             'classifier__degree':[1,2,3,4,5,6],\n",
    "            # 'classifier__probability': [True, False],     # probability must be = True, otherwise no 'predict_proba' \n",
    "             'vectorizer__stop_words': ['english', None],\n",
    "             'vectorizer__use_idf': [True, False], \n",
    "             'classifier__kernel': ['poly', 'rbf', 'sigmoid', 'linear'],\n",
    "             'classifier__C': [4,6,8,10]} # 0.1, 1, 2, 100, 1000\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "svm_gs = HalvingGridSearchCV(svm_pipe,svm_params,cv=5,n_jobs=-1, verbose=1, scoring='f1_micro', factor=2)#5-fold, computation will be dispatched on all the CPUs\n",
    "\n",
    "\n",
    "### train best estimator on traning data \n",
    "svm_gs = svm_gs.fit(X_train, y_train)\n",
    "print('Best score on training data:',svm_gs.score(X_train, y_train))\n",
    "print('Best score on testing data:',svm_gs.score(X_test, y_test))\n",
    "print('Best score',svm_gs.best_score_)\n",
    "print('Best parameters',svm_gs.best_params_)\n",
    "\n",
    "## test classifier \n",
    "y_pred = svm_gs.predict(X_test)\n",
    "predicted_prob = svm_gs.predict_proba(X_test)\n",
    "\n",
    "####################################################### Evaluation \n",
    "\n",
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "    \n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"SVM Details:\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "    \n",
    "## Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n",
    "       yticklabels=classes, title=\"Confusion matrix: SVM\")\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "## Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                              metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "    \n",
    "## Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()\n",
    "\n",
    "#### Other confusion matrix\n",
    "labels = classes \n",
    "print(classification_report(y_test, y_pred, labels)) #classification report from sklearn\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "plt.imshow(cnf_matrix, cmap=plt.cm.Blues) #plot confusion matrix grid\n",
    "threshold = cnf_matrix.max() / 2 #threshold to define text color\n",
    "for i in range(cnf_matrix.shape[0]): #print text in grid\n",
    "    for j in range(cnf_matrix.shape[1]): \n",
    "        plt.text(j, i, cnf_matrix[i,j], color=\"w\" if cnf_matrix[i,j] > threshold else 'black')\n",
    "tick_marks = np.arange(len(labels)) #define labeling spacing based on number of classes\n",
    "plt.xticks(tick_marks, labels, rotation=45)\n",
    "plt.yticks(tick_marks, labels)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 :  Bagging *(Bootstrapped aggregattion )*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build a pipeline\n",
    "bag_pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', BaggingClassifier(random_state=14, verbose=True)),])\n",
    "# see paramerts for model \n",
    "bag_pipe.get_params().keys()\n",
    "\n",
    "\n",
    "\n",
    "### define parameters to be tested usign k-fold CV\n",
    "bag_params = {'classifier__n_estimators': [1400,1500,1600],\n",
    "              'classifier__max_features' : [150,200,220], #250 is too much\n",
    "             'vectorizer__max_df': (0.8,0.9, 0.99),\n",
    "            # 'vectorizer__min_df': [4,5,6,7],\n",
    "             'vectorizer__use_idf': [True, False],}\n",
    "\n",
    "\n",
    "### perform gridsearch using clf and params\n",
    "bag_gs = HalvingGridSearchCV(bag_pipe,bag_params,cv=5,n_jobs=-1, scoring='f1_micro',verbose=0, factor=2) #5-fold, n_jobs =-1 :computation will be dispatched on all the CPUs\n",
    "                             \n",
    "                             \n",
    "### train best estimator on traning data \n",
    "bag_gs = bag_gs.fit(X_train, y_train)\n",
    "print('Best score on testing data:',bag_gs.score(X_test, y_test))\n",
    "print('Best score on training data:',bag_gs.score(X_train, y_train))\n",
    "print('Best score',bag_gs.best_score_)\n",
    "print('Best parameters',bag_gs.best_params_)\n",
    "\n",
    "\n",
    "## test classifier \n",
    "y_pred = bag_gs.best_estimator_.predict(X_test)\n",
    "predicted_prob = bag_gs.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "\n",
    "\n",
    "############################################## Evaluation ####################################################\n",
    "\n",
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "\n",
    "\n",
    "### Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Bagging Classifier Details:\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    \n",
    "### Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Predicted label\", ylabel=\"True label\", xticklabels=classes, \n",
    "       yticklabels=classes, title=\"Confusion matrix: Bagging Classifier\")\n",
    "plt.yticks(rotation=0)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "### Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                              metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "\n",
    "    \n",
    "### Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### Confusion matrix vol 2.0\n",
    "labels = classes\n",
    "print(classification_report(y_test, y_pred, labels)) #classification report from sklearn\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "plt.imshow(cnf_matrix, cmap=plt.cm.Blues) #plot confusion matrix grid\n",
    "threshold = cnf_matrix.max() / 2 #threshold to define text color\n",
    "for i in range(cnf_matrix.shape[0]): #print text in grid\n",
    "    for j in range(cnf_matrix.shape[1]): \n",
    "        plt.text(j, i, cnf_matrix[i,j], color=\"w\" if cnf_matrix[i,j] > threshold else 'black')\n",
    "tick_marks = np.arange(len(labels)) #define labeling spacing based on number of classes\n",
    "plt.xticks(tick_marks, labels, rotation=45)\n",
    "plt.yticks(tick_marks, labels)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5 :  Gradient boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build a pipeline\n",
    "gb = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()), #max_df=0.7, min_df=4,use_idf=False)\n",
    "    ('classifier', GradientBoostingClassifier(random_state=14)),])\n",
    "\n",
    "\n",
    "### define parameters to be tested usign k-fold CV\n",
    "gb_params = {'classifier__learning_rate': [0.4,0.5,0.6,0.7],\n",
    "          #   'classifier__loss': ['deviance', 'exponential'], \n",
    "             'classifier__n_estimators' : [175,200,250],\n",
    "              'classifier__max_depth' : [10,12,15],\n",
    "              'classifier__subsample' : [0.6,0.7,0.8,0.9],\n",
    "              'vectorizer__use_idf': [True, False],}\n",
    " \n",
    "### perform gridsearch using clf and params\n",
    "gb_gs = HalvingGridSearchCV(gb,gb_params,cv=5, scoring='f1_micro',verbose=1,n_jobs=-1,factor=2)#5-fold, computation will be dispatched on all the CPUs\n",
    "\n",
    "\n",
    "### train best estimator on traning data \n",
    "gb_gs = gb_gs.fit(X_train, y_train)\n",
    "print('Best score on testing data:',gb_gs.score(X_test, y_test))\n",
    "print('Best score on training data:',gb_gs.score(X_train, y_train))\n",
    "print('Best score',gb_gs.best_score_)\n",
    "print('Best parameters',gb_gs.best_params_)\n",
    "\n",
    "\n",
    "## test classifier \n",
    "y_pred = gb_gs.best_estimator_.predict(X_test)\n",
    "predicted_prob = gb_gs.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "\n",
    "\n",
    "############################################## Evaluation ####################################################\n",
    "\n",
    "classes = np.unique(y_test)\n",
    "y_test_array = pd.get_dummies(y_test, drop_first=False).values\n",
    "\n",
    "\n",
    "### Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Gradient Boosting Classifier Details:\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    \n",
    "### Plot confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
    "            cbar=False)\n",
    "ax.set(xlabel=\"Predicted label\", ylabel=\"True label\", xticklabels=classes, \n",
    "       yticklabels=classes, title=\"Confusion matrix: Gradient Boosting Classifier\")\n",
    "plt.yticks(rotation=0)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "### Plot roc\n",
    "for i in range(len(classes)):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n",
    "                           predicted_prob[:,i])\n",
    "    ax[0].plot(fpr, tpr, lw=3, \n",
    "              label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                              metrics.auc(fpr, tpr))\n",
    "               )\n",
    "ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n",
    "          xlabel='False Positive Rate', \n",
    "          ylabel=\"True Positive Rate (Recall)\", \n",
    "          title=\"Receiver operating characteristic\")\n",
    "ax[0].legend(loc=\"lower right\")\n",
    "ax[0].grid(True)\n",
    "\n",
    "    \n",
    "### Plot precision-recall curve\n",
    "for i in range(len(classes)):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "                 y_test_array[:,i], predicted_prob[:,i])\n",
    "    ax[1].plot(recall, precision, lw=3, \n",
    "               label='{0} (area={1:0.2f})'.format(classes[i], \n",
    "                                  metrics.auc(recall, precision))\n",
    "              )\n",
    "ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n",
    "          ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### Confusion matrix vol 2.0\n",
    "labels = classes\n",
    "print(classification_report(y_test, y_pred, labels)) #classification report from sklearn\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "plt.imshow(cnf_matrix, cmap=plt.cm.Blues) #plot confusion matrix grid\n",
    "threshold = cnf_matrix.max() / 2 #threshold to define text color\n",
    "for i in range(cnf_matrix.shape[0]): #print text in grid\n",
    "    for j in range(cnf_matrix.shape[1]): \n",
    "        plt.text(j, i, cnf_matrix[i,j], color=\"w\" if cnf_matrix[i,j] > threshold else 'black')\n",
    "tick_marks = np.arange(len(labels)) #define labeling spacing based on number of classes\n",
    "plt.xticks(tick_marks, labels, rotation=45)\n",
    "plt.yticks(tick_marks, labels)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selecting best model > refit on all labels >  predict on unlabelled tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "As we don't want to predict labels for tweets the classifier has been trained on, \n",
    "we subset the dataset to only contain unlabbled data and thereby avoid any spill-over from training. \n",
    "At the same time, this allow us to retrain the classifier on the full labelled dataset (X_train,y_train,X_test,y_test), \n",
    "thereby making use of the sparse amount of labelled data in the attempt to improve the predictions on the unlabbled data. \n",
    "\n",
    "Subsequently, the classifier predictions will be evaluated manually by drawing a stratified random sample\n",
    "from the label predictions and manually assessing the lables and creating a confusion matrix. \n",
    "\n",
    "'''\n",
    "\n",
    "### create dataset for testing and manual evaluation of the classifier predictions \n",
    "# merge the full and the labelled dataframes \n",
    "evaldf = pd.merge(df, data, how=\"outer\") #keep all data on index\n",
    "evaldf.fillna('unknown', inplace=True) # 'inknown' for the tweets without lables \n",
    "\n",
    "### drop the columns we don't need \n",
    "evaldf = evaldf.drop(['date', 'preprocess','preprocess_no_mention',\n",
    "       'token', 'lemma', 'token_no_mention', 'Unnamed: 0'], axis = 1)\n",
    "\n",
    "### drop the rows that already have labels (we don't want spill-over from fitting the classifier) \n",
    "# Get indexes of tweets to remove \n",
    "rows= evaldf[(evaldf['label'] != 'unknown')].index # tweets that are not 'unknown'\n",
    "# Delete these row indexes from dataFrame\n",
    "evaldf.drop(rows, inplace=True)\n",
    "evaldf=evaldf.reset_index()\n",
    "\n",
    "### drop the 'label' column, as we now don't have lables \n",
    "evaldf = evaldf.drop(['label'], axis = 1)\n",
    "\n",
    "# create validation set (to be used later for manual evaluation)\n",
    "XVal = evaldf['lemma_no_mention'] # \n",
    "len(XVal) # should be 2404 for danish (for Pl and GER: should be len(DF) minus len(Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select best classifier with optimal hyperparameters \n",
    "clf_pipe= Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(use_idf=False, max_df=0.6)), \n",
    "    ('classifier', SVC(C=8, degree=4,gamma=0.4, kernel='sigmoid',probability=True, random_state=10)),]) \n",
    "\n",
    "\n",
    "# fit the classifier on the full labelled dataset(X,y) \n",
    "clf = clf_pipe.fit(X, y)\n",
    "\n",
    "\n",
    "# predict labels for the unlabbled dataset (XVal, yVal)\n",
    "yVal = clf.predict(XVal)\n",
    "predict_prob = clf.predict_proba(XVal)\n",
    "\n",
    "# save predicted lables with the tweets in the df\n",
    "evaldf['prediction']=pd.Series(yVal)\n",
    "evaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the df to use for network\n",
    "evaldf.to_csv(r'C:\\Users\\Frederikke\\OneDrive\\MSc. Social Data Science\\Exam\\da_labels_pred.csv') # change to own directory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Draw random sample to evaluate classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Inspecting class proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noStrat = evaldf.groupby('prediction', group_keys=False).apply(lambda x: x.sample(50).sample(frac=1)).reset_index(drop=True)\n",
    "print(noStrat.groupby(\"prediction\").count())\n",
    "noStrat.loc[noStrat['prediction'] == 'anti-vaxx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initiate manual tweet labelling widget \n",
    "\n",
    "# define display function to show the preprocessed text for manual labelling\n",
    "def display_text(noStrat):\n",
    "    display(Markdown(noStrat[\"text\"]))\n",
    "    \n",
    "# label widget \n",
    "labellerNS = ClassLabeller(\n",
    "    features=noStrat, # change this to the sub-sample dataframe and correct column \n",
    "    display_func=display_text,\n",
    "    options=['vaxx', 'anti-vaxx', 'neutral','trash'],\n",
    ")\n",
    "\n",
    "labellerNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save labels to list and df \n",
    "# save index and lables in a dictionary \n",
    "manlabsNS = labellerNS.queue.labels\n",
    "\n",
    "# map the dict to a dataframe \n",
    "targetdfNS = pd.DataFrame.from_dict(manlabsNS, orient='index')\n",
    "targetdfNS.columns = ['target']\n",
    "\n",
    "### inspect that the tweets are shuffelled at each retrianing of the model BUT KEEPS THEIR INDEX \n",
    "print(targetdfNS.tail(10))\n",
    "      \n",
    "# label a tweet in the widget (and remember it) > find index in dictionary > find tweet in df using index > same tweet? \n",
    "noStrat.iloc[5]['text'] #change to the index number we want to inspect + to the sub-sample dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge the dataframes  \n",
    "targetdfNS = noStrat.merge(targetdfNS, left_index=True, right_index=True) \n",
    "targetdfNS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### identify correct and wrong labels \n",
    "\n",
    "#define conditions\n",
    "conditions = [targetdfNS['target'] == targetdfNS['prediction'], # the predicted label is equal to the real label\n",
    "              targetdfNS['target'] != targetdfNS['prediction']] # the predicted label is NOT equal to the real label\n",
    "#define choices\n",
    "choices = ['correct', 'wrong'] \n",
    "\n",
    "#create new column in DataFrame that displays results of comparisons\n",
    "targetdfNS['evaluation'] = np.select(conditions, choices, default='Tie')\n",
    "targetdfNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### list of targets \n",
    "targetsNS = targetdfNS['target']\n",
    "\n",
    "### list of predictions \n",
    "predictionsNS = targetdfNS['prediction']\n",
    "\n",
    "\n",
    "### Plot confusion matrix\n",
    "classes = np.unique(targetsNS)\n",
    "labels = classes \n",
    "print(classification_report(targetsNS, predictionsNS, labels)) #classification report from sklearn\n",
    "cnf_matrix = confusion_matrix(targetsNS, predictionsNS, labels=labels)\n",
    "plt.imshow(cnf_matrix, cmap=plt.cm.Blues) #plot confusion matrix grid\n",
    "threshold = cnf_matrix.max() / 2 #threshold to define text color\n",
    "for i in range(cnf_matrix.shape[0]): #print text in grid\n",
    "    for j in range(cnf_matrix.shape[1]): \n",
    "        plt.text(j, i, cnf_matrix[i,j], color=\"w\" if cnf_matrix[i,j] > threshold else 'black')\n",
    "tick_marks = np.arange(len(labels)) #define labeling spacing based on number of classes\n",
    "plt.xticks(tick_marks, labels, rotation=45)\n",
    "plt.yticks(tick_marks, labels)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the df \n",
    "targetdf.to_csv(r'C:\\Users\\Frederikke\\OneDrive\\MSc. Social Data Science\\Exam\\da_labels_done.csv') # change to own directory "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
