{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic network analysis\n",
    "\n",
    "This notebook exemplifies how we build the semantic networks. The notebook was run on all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Danish: !python -m spacy download da_core_news_sm\n",
    "# for Polish: !python -m spacy download pl_core_news_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import ast\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from community import community_louvain\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "os.chdir(r'C:\\Users\\maril\\Documents\\20-21 KU\\block 4\\DM\\twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(r'final_data_preprocess\\de_preprocess.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to turn the tokenized/lemmatized list into a readable format\n",
    "def string_list(text):\n",
    "    \n",
    "    # we transform the string representation of the list into an actual list\n",
    "    text = ast.literal_eval(text)\n",
    "    \n",
    "    # return the transformed text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function: YOU NEED TO SPECIFY ALL RELEVANT COLUMNS HERE\n",
    "df['token'] = df['token'].apply(string_list)\n",
    "df['lemma'] = df['lemma'].apply(string_list)\n",
    "df['token_no_mention'] = df['token_no_mention'].apply(string_list)\n",
    "df['lemma_no_mention'] = df['lemma_no_mention'].apply(string_list)\n",
    "\n",
    "# print the dataframe\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy model\n",
    "\n",
    "# for Danish: nlp = spacy.load('da_core_news_sm')\n",
    "# for Polish: nlp = spacy.load('pl_core_news_sm')\n",
    "nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to conduct the POS tagging\n",
    "\n",
    "def pos_tagging(keep_types):\n",
    "    \n",
    "    \"\"\"Takes a list of POS types to keep. Returns a list of tuples (lemma, POS type).\"\"\"\n",
    "    \n",
    "    # words to keep\n",
    "    keep = []\n",
    "    \n",
    "    # iterate through the column containing preprocessed text without mentions\n",
    "    for doc in df['preprocess_no_mention']:\n",
    "        \n",
    "        # apply the spacy pipeline\n",
    "        doc = nlp(doc)\n",
    "        \n",
    "        # iterate through the list of tokens\n",
    "        for w in doc:\n",
    "            \n",
    "            # remove stopwords\n",
    "            if w.is_stop == False:\n",
    "                \n",
    "                # get the POS type\n",
    "                typ = w.pos_\n",
    "\n",
    "                # get the lemma\n",
    "                w = w.lemma_.lower()\n",
    "\n",
    "                # if the POS type is defined as one we want to keep\n",
    "                if typ in keep_types:\n",
    "\n",
    "                    # append the keep list with a tuple of the lemma and the type\n",
    "                    keep.append((w,typ))\n",
    "        \n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the function: we specify that we want to keep nouns, verbs and adjectives\n",
    "\n",
    "# for Danish: only keep nouns and adjectives \n",
    "\n",
    "keep_output = pos_tagging(keep_types=set(['NOUN','VERB','ADJ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list that contains only the words we want to keep (without the POS tag)\n",
    "keep_output_words = set(word[0] for word in keep_output)\n",
    "print(keep_output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now onto word frequencies: we need to get an overview of the most frequent words in order to be able to adjust \n",
    "# for very rare and very frequent words\n",
    "\n",
    "# set up a counter\n",
    "count = Counter()\n",
    "\n",
    "# iterate through the list of lemmas in our dataframe\n",
    "# NOTE: this list of lemmas comes from an earlier preprocessing step in a different notebook and it therefore\n",
    "# contains more words than the ones we want to keep - we will handle that issue in the cells below\n",
    "for line in df['lemma_no_mention']:\n",
    "    \n",
    "    # update the counter\n",
    "    count.update(line)\n",
    "\n",
    "# print the 60 most frequent words \n",
    "print(count.most_common(60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove very rare words\n",
    "\n",
    "# we create a copy of the count\n",
    "count_final = count.copy()\n",
    "\n",
    "# iterate through the count dict\n",
    "for i in count:\n",
    "    \n",
    "    # if the value is below a certain value\n",
    "    if count[i] < 20:\n",
    "        \n",
    "        # delete the entry from the count_final dict\n",
    "        del count_final[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the co-occurrence matrix (here: in the form of a dict)\n",
    "\n",
    "# initiate defaultdict\n",
    "com = defaultdict(lambda : defaultdict(int))\n",
    " \n",
    "# iterate through the list of lemmas in the 'lemma_no_mention' column\n",
    "for line in df['lemma_no_mention']: \n",
    "    \n",
    "    # build co-occurrence matrix\n",
    "    # the -1 and +1 here make sure that we only get co-occurences of the token with all other tokens in the \n",
    "    # tweet, but not for itself (this makes sense - I promise:))\n",
    "    for i in range(len(line)-1): \n",
    "        for j in range(i+1, len(line)):\n",
    "            \n",
    "            # w1 and w2 are two words which co-occur in a tweet together\n",
    "            w1, w2 = sorted([line[i], line[j]]) \n",
    "            \n",
    "            # now we filter: if w1 and w2 fulfill all our criteria for being kept, we add them to the com dict\n",
    "            # and set/update their value by 1\n",
    "            if w1 in keep_output_words:\n",
    "                if w1 in count_final:\n",
    "                    if w2 in keep_output_words:\n",
    "                        if w2 in count_final:\n",
    "                            com[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-symmetric association values\n",
    "\n",
    "...we create symmetric association values inspired by Fuhse et al. (2020) further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of 'com' because we want to use the original 'com' for the symmetric association values further down\n",
    "com_uns = com.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are interested in the words that most frequently co-occur with the word 'impfung' (ENG: 'vaccine')\n",
    "# retrieve the 50 most co-occuring words\n",
    "\n",
    "term_of_interest = 'impfung'\n",
    "\n",
    "co_occur_uns = sorted(com_uns[term_of_interest].items(), key=lambda x:x[1], reverse=True)[:60]\n",
    "co_occur_uns = [tup[0] for tup in co_occur_uns]\n",
    "print(co_occur_uns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the network, we need a submatrix for these terms and their respective co-occurences among each other\n",
    "\n",
    "# empty list to save edges and weights in\n",
    "edgelist_uns = []\n",
    "\n",
    "# iterate through the co_occur list\n",
    "for term in co_occur_uns:\n",
    "    \n",
    "    # retrive the subdict which is saved for this term in the com dict\n",
    "    edges = com_uns[term]\n",
    "    \n",
    "    # iterate through the keys in this subdict\n",
    "    for node in edges:\n",
    "        \n",
    "        # if the node appears in the co_occur list\n",
    "        if node in co_occur_uns:\n",
    "            \n",
    "            # add the following information to the edgelist: node, node, weight\n",
    "            edgelist_uns.append([term, node, edges[node]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the edgelist into a dataframe\n",
    "edge_df_uns = pd.DataFrame(edgelist_uns, columns=['source', 'target', 'weight'])\n",
    "edge_df_uns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an undirected, weighted graph\n",
    "G_uns = nx.from_pandas_edgelist(edge_df_uns, source='source', target='target', edge_attr='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community detection (Louvain algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Louvain community detection on weighted graph\n",
    "partition_uns = community_louvain.best_partition(G_uns, weight='weight', random_state=40)\n",
    "\n",
    "# check the communities\n",
    "partition_uns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the community as a node attribute\n",
    "for c in partition_uns:\n",
    "    G_uns.nodes[c]['community'] = partition_uns[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change wd\n",
    "os.chdir(r'C:\\Users\\maril\\Documents\\20-21 KU\\block 4\\DM\\twitter\\semantic_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to Gephi\n",
    "nx.write_gexf(G_uns, 'semantic_net_unsymmetric.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling tweet from the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample tweets from the clusters\n",
    "\n",
    "# for Danish: there were six clusters, so c5 needs ot be added to the loop\n",
    "\n",
    "c0 = []\n",
    "c1 = []\n",
    "c2 = []\n",
    "c3 = []\n",
    "c4 = []\n",
    "\n",
    "for term in partition_uns:\n",
    "    if partition_uns[term] == 0:\n",
    "        c0.append(term)\n",
    "    \n",
    "    elif partition_uns[term] == 1:\n",
    "        c1.append(term)\n",
    "    \n",
    "    elif partition_uns[term] == 2:\n",
    "        c2.append(term)\n",
    "        \n",
    "    elif partition_uns[term] == 3:\n",
    "        c3.append(term)\n",
    "        \n",
    "    else:\n",
    "        c4.append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweets from all the clusters\n",
    "\n",
    "c0_indices = set()\n",
    "c1_indices = set()\n",
    "c2_indices = set()\n",
    "c3_indices = set()\n",
    "c4_indices = set()\n",
    "\n",
    "\n",
    "# iterate through the list of lemmas in the 'lemma_no_mention' column\n",
    "for i in range(len(df['lemma_no_mention'])):\n",
    "    \n",
    "    # match all lists of lemmas that contain the word 'impfung'\n",
    "    if 'impfung' in df['lemma_no_mention'][i]:\n",
    "        \n",
    "        # for each word in the cluster\n",
    "        for word in c0:\n",
    "            \n",
    "            # if the word from the cluster appears in the lemma list as well\n",
    "            if word in df['lemma_no_mention'][i]:\n",
    "                \n",
    "                # save index\n",
    "                c0_indices.add(i)\n",
    "                \n",
    "                # for each word in the cluster\n",
    "        for word in c1:\n",
    "            \n",
    "            # if the word from the cluster appears in the lemma list as well\n",
    "            if word in df['lemma_no_mention'][i]:\n",
    "                \n",
    "                # save index\n",
    "                c1_indices.add(i)\n",
    "                \n",
    "                # for each word in the cluster\n",
    "        for word in c2:\n",
    "            \n",
    "            # if the word from the cluster appears in the lemma list as well\n",
    "            if word in df['lemma_no_mention'][i]:\n",
    "                \n",
    "                # save index\n",
    "                c2_indices.add(i)\n",
    "                \n",
    "                # for each word in the cluster\n",
    "        for word in c3:\n",
    "            \n",
    "            # if the word from the cluster appears in the lemma list as well\n",
    "            if word in df['lemma_no_mention'][i]:\n",
    "                \n",
    "                # save index\n",
    "                c3_indices.add(i)\n",
    "                \n",
    "                # for each word in the cluster\n",
    "        for word in c4:\n",
    "            \n",
    "            # if the word from the cluster appears in the lemma list as well\n",
    "            if word in df['lemma_no_mention'][i]:\n",
    "                \n",
    "                # save index\n",
    "                c4_indices.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the dataframe by these indices and draw a random sample of 100 tweets\n",
    "c0_df = df.iloc[[index for index in c0_indices]].sample(25)\n",
    "c1_df = df.iloc[[index for index in c1_indices]].sample(25)\n",
    "c2_df = df.iloc[[index for index in c2_indices]].sample(25)\n",
    "c3_df = df.iloc[[index for index in c3_indices]].sample(25)\n",
    "c4_df = df.iloc[[index for index in c4_indices]].sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the sampled dataframes\n",
    "display(c0_df.head(3))\n",
    "display(c1_df.head(3))\n",
    "display(c2_df.head(3))\n",
    "display(c3_df.head(3))\n",
    "display(c4_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save them to csv files\n",
    "c0_df['text'].to_excel('c0_cluster_de.xlsx', index=False)\n",
    "c1_df['text'].to_excel('c1_cluster_de.xlsx', index=False)\n",
    "c2_df['text'].to_excel('c2_cluster_de.xlsx', index=False)\n",
    "c3_df['text'].to_excel('c3_cluster_de.xlsx', index=False)\n",
    "c4_df['text'].to_excel('c4_cluster_de.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric association values\n",
    "\n",
    "Fuhse et al. (2020) create \"symmetric association values\" which are the \"number of co-occurences $C_i{_j}$ divided by the overall frequencies of the two terms $F_i$ and $F_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate symmetric association values \n",
    "\n",
    "# iterate through the keys in com\n",
    "for i in com:\n",
    "    for j in com[i]:\n",
    "        \n",
    "        # if the column term is in count_final\n",
    "        if i in count_final:\n",
    "            \n",
    "            # if the row term is in count_final\n",
    "            if j in count_final:\n",
    "                \n",
    "                # update the co-occurence value by dividing the old value by the overall frequency of the two terms\n",
    "                com[i][j] = com[i][j] / (count_final[i] * count_final[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are interested in the words that most frequently co-occur with the word \"impfung\" (ENG: vaccine)\n",
    "# retrieve the 50 most co-occuring words\n",
    "\n",
    "term_of_interest = 'impfung'\n",
    "\n",
    "co_occur = sorted(com[term_of_interest].items(), key=lambda x:x[1], reverse=True)[:35]\n",
    "co_occur = [tup[0] for tup in co_occur]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the network, we need a submatrix for these terms and their respective co-occurences among each other\n",
    "\n",
    "# empty list to save edges and weights in\n",
    "edgelist = []\n",
    "\n",
    "# iterate through the co_occur list\n",
    "for term in co_occur:\n",
    "    \n",
    "    # retrive the subdict which is saved for this term in the com dict\n",
    "    edges = com[term]\n",
    "    \n",
    "    # iterate through the keys in this subdict\n",
    "    for node in edges:\n",
    "        \n",
    "        # if the node appears in the co_occur list\n",
    "        if node in co_occur:\n",
    "            \n",
    "            # add the following information to the edgelist: node, node, weight\n",
    "            edgelist.append([term, node, edges[node]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the edgelist into a dataframe\n",
    "edge_df = pd.DataFrame(edgelist, columns=['source', 'target', 'weight'])\n",
    "edge_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an undirected, weighted graph\n",
    "G = nx.from_pandas_edgelist(edge_df, source='source', target='target', edge_attr='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Louvain community detection on weighted graph\n",
    "partition = community_louvain.best_partition(G, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the communities\n",
    "partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the community as a node attribute\n",
    "for c in partition:\n",
    "    G.nodes[c]['community'] = partition[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to Gephi\n",
    "nx.write_gexf(G, 'semantic_net_symmetric.gexf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
