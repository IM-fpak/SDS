{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis for r/WallStreetBets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading packages\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "from nltk import word_tokenize, corpus\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "### advanced below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b53cdfe434ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'darkgrid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffline\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpyo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sns.set_style('darkgrid')\n",
    "import plotly.express as ex\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "from plotly.subplots import make_subplots\n",
    "pyo.init_notebook_mode()\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "nltk.download('vader_lexicon')\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.ar_model import AR,AutoReg\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy as sp\n",
    "nlps = sp.load('en')\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data \n",
    "df=pd.read_csv(\"reddit_wsb.csv\")\n",
    "\n",
    "#dropping columns we don't need \n",
    "df.drop(columns=['id', 'url', 'created'], inplace=True)\n",
    "\n",
    "#look at data\n",
    "print(df.info())  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the text \n",
    "\n",
    "* Data quality: https://www.kaggle.com/gpreda/wallstreetbets-reddit-posts-analysis#Sentiment-analysis\n",
    " > remove Nans\n",
    " > special characters \n",
    " > upper/lower\n",
    " > lemma/token\n",
    " \n",
    " \n",
    "\n",
    "https://www.kaggle.com/thomaskonstantin/reddit-wallstreetbets-posts-sentiment-analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean it up \n",
    "\n",
    "title = df['title'].dropna()\n",
    "body = df['body'].dropna()\n",
    "\n",
    "\n",
    "def cleaner(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replacing Handlers with Empty String\n",
    "    text = re.sub('@[^\\s]+','',text)\n",
    "\n",
    "    # Replacing URLs with Empty String\n",
    "    text = re.sub(r\"http\\S+\", \"\",text)\n",
    "\n",
    "    # Remove all the special characters\n",
    "    text = ' '.join(re.findall(r'\\w+', text))\n",
    "\n",
    "    # Replacing Single Characters with Empty String\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    # Removing Extra Spaces\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    \n",
    "    return text\n",
    "\n",
    "    \n",
    "# Text Preprocessing\n",
    "title = title.apply(lambda x : cleaner(x))\n",
    "body = body.apply(lambda x : cleaner(x))\n",
    "\n",
    "\n",
    "#put into original df\n",
    "df['clean_title']=title\n",
    "df['clean_body']=body\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### investigate lengths of titles and bodies \n",
    "\n",
    "title_length = [len(word_tokenize(text)) for text in title]\n",
    "body_length = [len(word_tokenize(text)) for text in body]\n",
    "\n",
    "fig, (axis1, axis2) = plt.subplots(1,2, figsize=(16,6))\n",
    "\n",
    "sns.histplot(title_length, bins=50, kde=True, ax=axis1)\n",
    "sns.histplot(body_length, bins=40, kde=True, ax=axis2)\n",
    "\n",
    "axis1.set_xlabel(\"Length of Title\")\n",
    "axis2.set_xlabel(\"Length of Body\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### THIS MIGHT BE IMPORTANT FOR NB ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud for Title and Body of the Post\n",
    "\n",
    "https://www.kaggle.com/sprakshith/beginner-s-guide-to-sentiment-analysis\n",
    "\n",
    "For prevalent words\n",
    "\n",
    "https://www.kaggle.com/gpreda/wallstreetbets-reddit-posts-analysis#Sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Title Version 1 ###\n",
    "\n",
    "def show_wordcloud(df, title=\"\"):\n",
    "    text = \" \".join(t for t in df.dropna())\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update([\"t\", \"co\", \"https\", \"amp\", \"U\", \"fuck\", \"fucking\"])\n",
    "    wordcloud = WordCloud(stopwords=stopwords, scale=4, max_font_size=50, max_words=500,background_color=\"white\").generate(text)\n",
    "    fig = plt.figure(1, figsize=(16,16))\n",
    "    plt.axis('off')\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    fig.subplots_adjust(top=2.3)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_wordcloud(df['title'], title = 'Prevalent words in titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Title Version 2 ###\n",
    "\n",
    "english_words = set(corpus.words.words())\n",
    "word_tokens = [word_tokenize(text) for text in title]\n",
    "\n",
    "word_cloud_string = \"\"\n",
    "\n",
    "for word_list in word_tokens:\n",
    "    for word in word_list:\n",
    "        if word.lower() in english_words:\n",
    "            word_cloud_string += word + \" \"\n",
    "        \n",
    "# Updating some of the Words into Stopwords \n",
    "description_stopwords = set(STOPWORDS)\n",
    "\n",
    "my_word_cloud = WordCloud(background_color='white',stopwords=description_stopwords).generate(word_cloud_string)\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.imshow(my_word_cloud, interpolation='bilinear')\n",
    "plt.title(\"Word Cloud for Post Title\", fontsize=20)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Body Version 1 ###\n",
    "\n",
    "show_wordcloud(df['body'], title = 'Prevalent words in post bodies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Body Version 2 ###\n",
    "\n",
    "word_tokens = [word_tokenize(text) for text in body]\n",
    "\n",
    "word_cloud_string = \"\"\n",
    "\n",
    "for word_list in word_tokens:\n",
    "    for word in word_list:\n",
    "        if word.lower() in english_words:\n",
    "            word_cloud_string += word + \" \"\n",
    "        \n",
    "# Updating some of the Words into Stopwords \n",
    "description_stopwords = set(STOPWORDS)\n",
    "\n",
    "my_word_cloud = WordCloud(background_color='white',stopwords=description_stopwords).generate(word_cloud_string)\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.imshow(my_word_cloud, interpolation='bilinear')\n",
    "plt.title(\"Word Cloud for Post Body\", fontsize=20)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis using 'SentimentIntensityAnalyzer' from nltk.sentiment\n",
    "\n",
    "https://www.kaggle.com/sprakshith/beginner-s-guide-to-sentiment-analysis\n",
    "https://www.kaggle.com/gpreda/wallstreetbets-reddit-posts-analysis#Sentiment-analysis\n",
    "\n",
    "* WordCloud for Different Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the sentiment analysier from nltk.sentiment \n",
    "sia = SentimentIntensityAnalyzer() \n",
    "\n",
    "#make function \n",
    "def sentiment_score(sia, text): #takes the SentimentIntensityAnalyzer and some text \n",
    "    if sia.polarity_scores(text)[\"compound\"] > 0: # if score above 0, label as posittive \n",
    "        return \"Positive\"\n",
    "    elif sia.polarity_scores(text)[\"compound\"] < 0: # if score below zero label negative \n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\" # else, label Neutral\n",
    "\n",
    "    \n",
    "\n",
    "### use the sentiment_score fundction \n",
    "\n",
    "# create a df with the titels and the found sentiment lables \n",
    "title_df = title.to_frame(name='Title')\n",
    "title_df['Sentiment'] = title_df['Title'].apply(lambda x : sentiment_score(sia, x))\n",
    "\n",
    "\n",
    "# create a df with the bodies and the found sentiment lables \n",
    "body_df = body.to_frame(name='Body')\n",
    "body_df['Sentiment'] = body_df['Body'].apply(lambda x : get_sentiment(sia, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### looking at sentiment \n",
    "#title \n",
    "title_sent_df=pd.DataFrame(title_df.groupby(\"Sentiment\")[\"Sentiment\"].count()).rename(columns={\"Sentiment\":\"Count\"}).reset_index().style.set_caption(\"Title sentiments\")\n",
    "\n",
    "title_sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#body\n",
    "body_sent_df=pd.DataFrame(body_df.groupby(\"Sentiment\")[\"Sentiment\"].count()).rename(columns={\"Sentiment\":\"Count\"}).reset_index().style.set_caption(\"Body sentiments\")\n",
    "#body_sent_df=bodye_sent_df.style.set_caption(\"Title sentiments\")\n",
    "\n",
    "body_sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualizations of the distribution of sentiment \n",
    "\n",
    "fig, (axis1, axis2) = plt.subplots(1,2, figsize=(12,5))\n",
    "order = ['Positive', 'Neutral', 'Negative']\n",
    "\n",
    "xs = title_df['Sentiment'].value_counts().index\n",
    "ys = title_df['Sentiment'].value_counts().values\n",
    "sns.barplot(x=xs, y=ys, order=order, ax=axis1)\n",
    "\n",
    "xs = body_df['Sentiment'].value_counts().index\n",
    "ys = body_df['Sentiment'].value_counts().values\n",
    "sns.barplot(x=xs, y=ys, order=order, ax=axis2)\n",
    "\n",
    "axis1.set_title(\"For Title\")\n",
    "axis2.set_title(\"For Body\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordclouds for different sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Title clouds \n",
    "\n",
    "fig, (axis1, axis2, axis3) = plt.subplots(3, 1, figsize=(12,18))\n",
    "\n",
    "axes = [axis1, axis2, axis3]\n",
    "sentiments = ['Positive', 'Neutral', 'Negative']\n",
    "\n",
    "for i in range(3):\n",
    "    word_tokens = [word_tokenize(text) for text in title_df[title_df['Sentiment'] == sentiments[i]]['Title']]\n",
    "    \n",
    "    word_cloud_string = \"\"\n",
    "    \n",
    "    for word_list in word_tokens:\n",
    "        for word in word_list:\n",
    "            if word.lower() in english_words:\n",
    "                word_cloud_string += word + \" \"\n",
    "    \n",
    "    description_stopwords = set(STOPWORDS)\n",
    "\n",
    "    my_word_cloud = WordCloud(background_color='white',stopwords=description_stopwords).generate(word_cloud_string)\n",
    "    axes[i].imshow(my_word_cloud, interpolation='bilinear')\n",
    "    axes[i].set_title(f\"Word Cloud for Post Title with {sentiments[i]} Sentiment\", fontsize=20)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Body clouds\n",
    "\n",
    "fig, (axis1, axis2, axis3) = plt.subplots(3, 1, figsize=(12,18))\n",
    "\n",
    "axes = [axis1, axis2, axis3]\n",
    "sentiments = ['Positive', 'Neutral', 'Negative']\n",
    "\n",
    "for i in range(3):\n",
    "    word_tokens = [word_tokenize(text) for text in body_df[body_df['Sentiment'] == sentiments[i]]['Body']]\n",
    "    \n",
    "    word_cloud_string = \"\"\n",
    "    \n",
    "    for word_list in word_tokens:\n",
    "        for word in word_list:\n",
    "            if word.lower() in english_words:\n",
    "                word_cloud_string += word + \" \"\n",
    "    \n",
    "    description_stopwords = set(STOPWORDS)\n",
    "\n",
    "    my_word_cloud = WordCloud(background_color='white',stopwords=description_stopwords).generate(word_cloud_string)\n",
    "    axes[i].imshow(my_word_cloud, interpolation='bilinear')\n",
    "    axes[i].set_title(f\"Word Cloud for Post Body with {sentiments[i]} Sentiment\", fontsize=20)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect text\n",
    "\n",
    "* Avg. score for pos/neg/neu\n",
    "* Avg. # for pos/neg/neu \n",
    "* when are the pos/negative posts made?\n",
    "* Avg. number of comments per pos/neg/neu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and saving pdfs for titles and bodies with sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df\n",
    "body_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6abc089e0126>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m### adding sentiment and dropping columns for the title dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtitle_sentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtitle_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# dropping the columns we dont need\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtitle_sentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Title'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'clean_body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "### adding sentiment and dropping columns for the title dataframe\n",
    "title_sentiment = pd.concat([title_df, df], axis=1)\n",
    "\n",
    "# dropping the columns we dont need\n",
    "title_sentiment.drop(columns=['title', 'body', 'Title','clean_body'], inplace=True)\n",
    "\n",
    "# check for NaNs\n",
    "title_sentiment.info() # no NaNs\n",
    "\n",
    "# look at the data\n",
    "title_sentiment\n",
    "\n",
    "# create csv file \n",
    "#title_sentiment.to_csv('title_sentiment.csv',index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 19244 entries, 2 to 38635\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Sentiment   19244 non-null  object\n",
      " 1   score       19244 non-null  int64 \n",
      " 2   comms_num   19244 non-null  int64 \n",
      " 3   timestamp   19244 non-null  object\n",
      " 4   clean_body  19244 non-null  object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 902.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>score</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>clean_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>2021-01-28 21:30:35</td>\n",
       "      <td>the ceo of nasdaq pushed to halt trading to gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Negative</td>\n",
       "      <td>317</td>\n",
       "      <td>53</td>\n",
       "      <td>2021-01-28 21:26:27</td>\n",
       "      <td>hedgefund whales are spreading disinfo saying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Negative</td>\n",
       "      <td>405</td>\n",
       "      <td>178</td>\n",
       "      <td>2021-01-28 21:19:31</td>\n",
       "      <td>life isn fair my mother always told me that wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Positive</td>\n",
       "      <td>222</td>\n",
       "      <td>70</td>\n",
       "      <td>2021-01-28 21:18:25</td>\n",
       "      <td>i believe right now is one of those rare oppor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2021-01-28 21:17:10</td>\n",
       "      <td>you guys are champs gme who would have thought...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment  score  comms_num            timestamp  \\\n",
       "2   Negative      0         47  2021-01-28 21:30:35   \n",
       "6   Negative    317         53  2021-01-28 21:26:27   \n",
       "7   Negative    405        178  2021-01-28 21:19:31   \n",
       "10  Positive    222         70  2021-01-28 21:18:25   \n",
       "12  Positive      0         16  2021-01-28 21:17:10   \n",
       "\n",
       "                                           clean_body  \n",
       "2   the ceo of nasdaq pushed to halt trading to gi...  \n",
       "6   hedgefund whales are spreading disinfo saying ...  \n",
       "7   life isn fair my mother always told me that wh...  \n",
       "10  i believe right now is one of those rare oppor...  \n",
       "12  you guys are champs gme who would have thought...  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### combining the sentiment lables for the bodies to the df\n",
    "body_sentiment = pd.concat([body_df, df], axis=1).dropna()\n",
    "\n",
    "#dropping columns for the body dataframe\n",
    "body_sentiment.drop(columns=['title','Body','body','clean_title'], inplace=True)\n",
    "\n",
    "#checking for NaNs\n",
    "body_sentiment.info() # no NaNs\n",
    "\n",
    "#looking at data \n",
    "body_sentiment.head()\n",
    "\n",
    "# create csv file \n",
    "#body_sentiment.to_csv('body_sentiment.csv',index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_body</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's not about the money, it's about sending a...</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:37:41</td>\n",
       "      <td>it not about the money it about sending message</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Math Professor Scott Steiner says the numbers ...</td>\n",
       "      <td>110</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:32:10</td>\n",
       "      <td>math professor scott steiner says the numbers ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exit the system</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>The CEO of NASDAQ pushed to halt trading “to g...</td>\n",
       "      <td>2021-01-28 21:30:35</td>\n",
       "      <td>exit the system</td>\n",
       "      <td>the ceo of nasdaq pushed to halt trading to gi...</td>\n",
       "      <td>the ceo of nasdaq pushed to halt trading to gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEW SEC FILING FOR GME! CAN SOMEONE LESS RETAR...</td>\n",
       "      <td>29</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:28:57</td>\n",
       "      <td>new sec filing for gme can someone less retard...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not to distract from GME, just thought our AMC...</td>\n",
       "      <td>71</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-28 21:26:56</td>\n",
       "      <td>not to distract from gme just thought our amc ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38636</th>\n",
       "      <td>NCLH 💎🙌🚀🌕 YOLO update!!!</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-04 01:27:07</td>\n",
       "      <td>nclh yolo update</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38637</th>\n",
       "      <td>Adding to $RKT 300 more to 1000 on morning pul...</td>\n",
       "      <td>59</td>\n",
       "      <td>76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-04 01:19:25</td>\n",
       "      <td>adding to rkt 300 more to 1000 on morning pull...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38638</th>\n",
       "      <td>Started at 5k a few weeks ago. Now it’s time f...</td>\n",
       "      <td>44</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-04 01:19:01</td>\n",
       "      <td>started at 5k few weeks ago now it time for th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38639</th>\n",
       "      <td>Posted yesterday about buying GME 104 @ $122.9...</td>\n",
       "      <td>864</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-04 01:17:22</td>\n",
       "      <td>posted yesterday about buying gme 104 122 94 f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38640</th>\n",
       "      <td>GME 🚀 Proud to fight beside each and everyone ...</td>\n",
       "      <td>15268</td>\n",
       "      <td>552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-04 01:12:42</td>\n",
       "      <td>gme proud to fight beside each and everyone of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38641 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  score  comms_num  \\\n",
       "0      It's not about the money, it's about sending a...     55          6   \n",
       "1      Math Professor Scott Steiner says the numbers ...    110         23   \n",
       "2                                        Exit the system      0         47   \n",
       "3      NEW SEC FILING FOR GME! CAN SOMEONE LESS RETAR...     29         74   \n",
       "4      Not to distract from GME, just thought our AMC...     71        156   \n",
       "...                                                  ...    ...        ...   \n",
       "38636                           NCLH 💎🙌🚀🌕 YOLO update!!!     19          5   \n",
       "38637  Adding to $RKT 300 more to 1000 on morning pul...     59         76   \n",
       "38638  Started at 5k a few weeks ago. Now it’s time f...     44         26   \n",
       "38639  Posted yesterday about buying GME 104 @ $122.9...    864         22   \n",
       "38640  GME 🚀 Proud to fight beside each and everyone ...  15268        552   \n",
       "\n",
       "                                                    body            timestamp  \\\n",
       "0                                                    NaN  2021-01-28 21:37:41   \n",
       "1                                                    NaN  2021-01-28 21:32:10   \n",
       "2      The CEO of NASDAQ pushed to halt trading “to g...  2021-01-28 21:30:35   \n",
       "3                                                    NaN  2021-01-28 21:28:57   \n",
       "4                                                    NaN  2021-01-28 21:26:56   \n",
       "...                                                  ...                  ...   \n",
       "38636                                                NaN  2021-03-04 01:27:07   \n",
       "38637                                                NaN  2021-03-04 01:19:25   \n",
       "38638                                                NaN  2021-03-04 01:19:01   \n",
       "38639                                                NaN  2021-03-04 01:17:22   \n",
       "38640                                                NaN  2021-03-04 01:12:42   \n",
       "\n",
       "                                             clean_title  \\\n",
       "0        it not about the money it about sending message   \n",
       "1      math professor scott steiner says the numbers ...   \n",
       "2                                        exit the system   \n",
       "3      new sec filing for gme can someone less retard...   \n",
       "4      not to distract from gme just thought our amc ...   \n",
       "...                                                  ...   \n",
       "38636                                   nclh yolo update   \n",
       "38637  adding to rkt 300 more to 1000 on morning pull...   \n",
       "38638  started at 5k few weeks ago now it time for th...   \n",
       "38639  posted yesterday about buying gme 104 122 94 f...   \n",
       "38640  gme proud to fight beside each and everyone of...   \n",
       "\n",
       "                                              clean_body  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2      the ceo of nasdaq pushed to halt trading to gi...   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "38636                                                NaN   \n",
       "38637                                                NaN   \n",
       "38638                                                NaN   \n",
       "38639                                                NaN   \n",
       "38640                                                NaN   \n",
       "\n",
       "                                              clean_text  \n",
       "0                                                    NaN  \n",
       "1                                                    NaN  \n",
       "2      the ceo of nasdaq pushed to halt trading to gi...  \n",
       "3                                                    NaN  \n",
       "4                                                    NaN  \n",
       "...                                                  ...  \n",
       "38636                                                NaN  \n",
       "38637                                                NaN  \n",
       "38638                                                NaN  \n",
       "38639                                                NaN  \n",
       "38640                                                NaN  \n",
       "\n",
       "[38641 rows x 8 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### make a clean text df \n",
    "\n",
    "df['clean_text'] = df['clean_body'] + df['clean_title']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
