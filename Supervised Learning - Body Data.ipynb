{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Comparison : Body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### packages \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "## for explainer\n",
    "from lime import lime_text\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import nltk\n",
    "import string\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from nltk import word_tokenize, corpus\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>lemma_meaningful</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>['money', 'sending', 'message']</td>\n",
       "      <td>money sending message</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>['math', 'professor', 'scott', 'steiner', 'say...</td>\n",
       "      <td>math professor scott steiner say number spell ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>['exit', 'system']</td>\n",
       "      <td>exit system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>['new', 'sec', 'filing', 'gme', 'someone', 'le...</td>\n",
       "      <td>new sec filing gme someone le retarded please ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>['distract', 'gme', 'thought', 'amc', 'brother...</td>\n",
       "      <td>distract gme thought amc brother aware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57663</th>\n",
       "      <td>0</td>\n",
       "      <td>['got', 'fuck', 'wife', 'morning', 'moon']</td>\n",
       "      <td>got fuck wife morning moon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57664</th>\n",
       "      <td>2</td>\n",
       "      <td>['rkt', 'tech', 'platform', 'real', 'estate', ...</td>\n",
       "      <td>rkt tech platform real estate doubled auto dou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57665</th>\n",
       "      <td>2</td>\n",
       "      <td>['vonage', 'largest', 'ucaas', 'unified', 'com...</td>\n",
       "      <td>vonage largest ucaas unified communication ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57666</th>\n",
       "      <td>2</td>\n",
       "      <td>['know', 'ape', 'autist', 'retard', 'sub', 'ye...</td>\n",
       "      <td>know ape autist retard sub year first exposure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57667</th>\n",
       "      <td>2</td>\n",
       "      <td>['guy', 'rocket', 'ha', 'like', 'spacex', 'boo...</td>\n",
       "      <td>guy rocket ha like spacex booster get close sp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57149 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment                                   lemma_meaningful  \\\n",
       "0              1                    ['money', 'sending', 'message']   \n",
       "1              0  ['math', 'professor', 'scott', 'steiner', 'say...   \n",
       "2              1                                 ['exit', 'system']   \n",
       "3              0  ['new', 'sec', 'filing', 'gme', 'someone', 'le...   \n",
       "4              2  ['distract', 'gme', 'thought', 'amc', 'brother...   \n",
       "...          ...                                                ...   \n",
       "57663          0         ['got', 'fuck', 'wife', 'morning', 'moon']   \n",
       "57664          2  ['rkt', 'tech', 'platform', 'real', 'estate', ...   \n",
       "57665          2  ['vonage', 'largest', 'ucaas', 'unified', 'com...   \n",
       "57666          2  ['know', 'ape', 'autist', 'retard', 'sub', 'ye...   \n",
       "57667          2  ['guy', 'rocket', 'ha', 'like', 'spacex', 'boo...   \n",
       "\n",
       "                                               processed  \n",
       "0                                  money sending message  \n",
       "1      math professor scott steiner say number spell ...  \n",
       "2                                            exit system  \n",
       "3      new sec filing gme someone le retarded please ...  \n",
       "4                 distract gme thought amc brother aware  \n",
       "...                                                  ...  \n",
       "57663                         got fuck wife morning moon  \n",
       "57664  rkt tech platform real estate doubled auto dou...  \n",
       "57665  vonage largest ucaas unified communication ser...  \n",
       "57666  know ape autist retard sub year first exposure...  \n",
       "57667  guy rocket ha like spacex booster get close sp...  \n",
       "\n",
       "[57149 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Load data \n",
    "\n",
    "#look at data\n",
    "df=pd.read_csv(\"processed_data.csv\").dropna()\n",
    "\n",
    "###  label encoding \n",
    "labels={\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "df = df.replace(labels)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dataset\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, stratify=df['Sentiment'], random_state=14) \n",
    "\n",
    "## get target\n",
    "y_train = train_data[\"Sentiment\"].values\n",
    "y_test = test_data[\"Sentiment\"].values\n",
    "\n",
    "## get text\n",
    "X_train = train_data[\"processed\"].values\n",
    "X_test = test_data[\"processed\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clssification and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n",
      "Auc: 0.92\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.70      0.75      1876\n",
      "           1       0.87      0.92      0.90      3504\n",
      "           2       0.78      0.80      0.79      2293\n",
      "\n",
      "    accuracy                           0.83      7673\n",
      "   macro avg       0.82      0.81      0.81      7673\n",
      "weighted avg       0.83      0.83      0.83      7673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## make pipeline \n",
    "\n",
    "mnb = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_df=0.5, max_features=5000, min_df=5, ngram_range=(1,1))),\n",
    "    ('classifier', MultinomialNB(alpha=0.1,fit_prior=True)),])\n",
    "\n",
    "## train classifier\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "## test classifier \n",
    "y_pred = mnb.predict(X_test)\n",
    "predicted_prob = mnb.predict_proba(X_test)\n",
    "\n",
    "\n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n",
      "Auc: 0.96\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.75      0.81      3138\n",
      "           1       0.85      0.98      0.91      4057\n",
      "           2       0.88      0.85      0.87      4235\n",
      "\n",
      "    accuracy                           0.87     11430\n",
      "   macro avg       0.87      0.86      0.86     11430\n",
      "weighted avg       0.87      0.87      0.87     11430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_df=0.6, max_features=5000, min_df=4, ngram_range=(1,1))),\n",
    "    ('classifier', LogisticRegression(random_state=0, C=0.9, penalty='l2', solver='newton-cg')),])\n",
    "\n",
    "\n",
    "## train classifier\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "## test classifier \n",
    "y_pred = logreg.predict(X_test)\n",
    "predicted_prob = logreg.predict_proba(X_test)\n",
    "\n",
    "\n",
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_prob, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:32:41] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:33:41] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "### Define all the classifiers and get evaluation metrics \n",
    "\n",
    "# define pipelines for classifiers (with optimal parameters)\n",
    "mnb = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_df=0.5, max_features=5000, min_df=5, ngram_range=(1,1))),\n",
    "    ('classifier', MultinomialNB(alpha=0.1,fit_prior=True)),])\n",
    "\n",
    "logreg = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_df=0.6, max_features=5000, min_df=4, ngram_range=(1,1))),\n",
    "    ('classifier', LogisticRegression(random_state=0, C=0.9, penalty='l2', solver='newton-cg')),])\n",
    "\n",
    "knn = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_df=0.6, max_features=5000, min_df=4,use_idf=False)), #max_df?\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=1,weights='uniform')),])\n",
    "\n",
    "rf = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_df=0.7, max_features=5000, min_df=5,use_idf=False)),\n",
    "    ('classifier', RandomForestClassifier(random_state=14,bootstrap=False,max_depth=5,max_features='sqrt', \n",
    "                                          min_samples_split=3,n_estimators=110)),])\n",
    "\n",
    "svm = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_df=0.5, max_features=5000, min_df=4,use_idf=True )),\n",
    "    ('classifier', SVC(random_state=2, gamma='scale',kernel='rbf')),])\n",
    "\n",
    "xgb = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_df=0.7, min_df=4,use_idf=False)),\n",
    "    ('classifier', XGBClassifier(random_state=14, verbosity=1, booster='dart',learning_rate=0.05,\n",
    "                                 max_depth=8,use_label_encoder=True)),]) #max_features=5000\n",
    "\n",
    "bag = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_df=0.5, max_features=5000, min_df=4,use_idf=False )),\n",
    "    ('classifier', BaggingClassifier(random_state=14, n_estimators=100)),])\n",
    "\n",
    "\n",
    "    \n",
    "##### put in all models \n",
    "all_models = [\n",
    "    (\"mnb\", mnb),\n",
    "    (\"knn\", knn),\n",
    "    (\"rf\", rf),\n",
    "    (\"svm\", svm),\n",
    "    (\"xgb\", xgb),\n",
    "    (\"bag\", bag),\n",
    "    (\"logreg\", logr eg),]\n",
    "\n",
    "\n",
    " \n",
    "unsorted_scores = [(name, cross_val_score(model, X_train, y_train, cv=2).mean()) for name, model in all_models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('svm', 0.849974848664281), ('logreg', 0.8435880149589114), ('bag', 0.8194405424758275), ('mnb', 0.7483540619914308), ('xgb', 0.7234192596960858), ('rf', 0.5435596180426263), ('knn', 0.49016817580421984)]\n"
     ]
    }
   ],
   "source": [
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "print(scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 8\n",
      "n_required_iterations: 8\n",
      "n_possible_iterations: 8\n",
      "min_resources_: 357\n",
      "max_resources_: 45719\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 144\n",
      "n_resources: 357\n",
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 72\n",
      "n_resources: 714\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 36\n",
      "n_resources: 1428\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 18\n",
      "n_resources: 2856\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 9\n",
      "n_resources: 5712\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 5\n",
      "n_resources: 11424\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 3\n",
      "n_resources: 22848\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 2\n",
      "n_resources: 45696\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[20:21:49] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "### XGBoost Classifier\n",
    "### build a pipeline\n",
    "xgb2 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()), #max_df=0.7, min_df=4,use_idf=False)\n",
    "    ('classifier', XGBClassifier(random_state=14, verbosity=1)),])\n",
    "\n",
    "\n",
    "\n",
    "### define parameters to be tested usign k-fold CV ###CHANGE###\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n",
    "xgb2_params = {'classifier__learning_rate': [0.03, 0.05, 0.07],\n",
    "              'classifier__booster': ['dart', 'gbtree'], \n",
    "              'classifier__max_depth' : [8,9,10],\n",
    "              'vectorizer__min_df': [3,4],\n",
    "              'classifier__use_label_encoder' : [True, False],\n",
    "              'vectorizer__use_idf': [True, False],}\n",
    " \n",
    "\n",
    "\n",
    "xgb_gs2 = HalvingGridSearchCV(xgb2,xgb2_params,cv=5,n_jobs=-1, verbose=1, factor=2)#5-fold, computation will be dispatched on all the CPUs\n",
    "xgb_gs2 = xgb_gs2.fit(X_train, y_train)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score on training data: 0.8048951201907303\n",
      "Best score on testing data: 0.7566054243219598\n",
      "Best score 0.7574297771311974\n",
      "Best parameters {'classifier__booster': 'dart', 'classifier__learning_rate': 0.07, 'classifier__max_depth': 10, 'classifier__use_label_encoder': True, 'vectorizer__min_df': 4, 'vectorizer__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "## Best model and parameters \n",
    "\n",
    "### get the best model and params\n",
    "print('Best score on training data:',xgb_gs2.score(X_train, y_train))\n",
    "print('Best score on testing data:',xgb_gs2.score(X_test, y_test))\n",
    "print('Best score',xgb_gs2.best_score_)\n",
    "print('Best parameters',xgb_gs2.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "Auc: 0.9\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.50      0.62      3138\n",
      "           1       0.70      0.98      0.81      4057\n",
      "           2       0.81      0.74      0.77      4235\n",
      "\n",
      "    accuracy                           0.76     11430\n",
      "   macro avg       0.78      0.74      0.74     11430\n",
      "weighted avg       0.77      0.76      0.75     11430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Accuracy, Precision, Recall\n",
    "\n",
    "## train classifier\n",
    "#xgb_gs2.fit(X_train, y_train)\n",
    "\n",
    "## test classifier \n",
    "y_predt = xgb_gs2.predict(X_test)\n",
    "predicted_probt = xgb_gs2.predict_proba(X_test)\n",
    "\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_predt)\n",
    "auc = metrics.roc_auc_score(y_test, predicted_probt, \n",
    "                            multi_class=\"ovr\")\n",
    "print(\"Accuracy:\",  round(accuracy,2))\n",
    "print(\"Auc:\", round(auc,2))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test, y_predt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
