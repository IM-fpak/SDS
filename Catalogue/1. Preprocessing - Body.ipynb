{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load packages \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from autocorrect import spell \n",
    "import matplotlib\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stopwords.words('english')\n",
    "import warnings\n",
    "from nltk import word_tokenize, corpus\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "sns.set() # use seaborn plotting style\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19244 entries, 0 to 19243\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Sentiment   19244 non-null  object\n",
      " 1   score       19244 non-null  int64 \n",
      " 2   comms_num   19244 non-null  int64 \n",
      " 3   timestamp   19244 non-null  object\n",
      " 4   clean_text  19018 non-null  object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 751.8+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>score</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>2021-01-28 21:30:35</td>\n",
       "      <td>the ceo of nasdaq pushed to halt trading to gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>317</td>\n",
       "      <td>53</td>\n",
       "      <td>2021-01-28 21:26:27</td>\n",
       "      <td>hedgefund whales are spreading disinfo saying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>405</td>\n",
       "      <td>178</td>\n",
       "      <td>2021-01-28 21:19:31</td>\n",
       "      <td>life isn fair my mother always told me that wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>222</td>\n",
       "      <td>70</td>\n",
       "      <td>2021-01-28 21:18:25</td>\n",
       "      <td>i believe right now is one of those rare oppor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2021-01-28 21:17:10</td>\n",
       "      <td>you guys are champs gme who would have thought...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19239</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>130</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-03-04 01:54:44</td>\n",
       "      <td>x200b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19240</th>\n",
       "      <td>Positive</td>\n",
       "      <td>105</td>\n",
       "      <td>72</td>\n",
       "      <td>2021-03-04 01:51:52</td>\n",
       "      <td>rkt as tech platform real estate doubled auto ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19241</th>\n",
       "      <td>Positive</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>2021-03-04 01:41:49</td>\n",
       "      <td>vonage is the 2nd largest ucaas unified commun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19242</th>\n",
       "      <td>Positive</td>\n",
       "      <td>296</td>\n",
       "      <td>46</td>\n",
       "      <td>2021-03-04 01:36:29</td>\n",
       "      <td>i don know if am an ape autist or retard ve be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19243</th>\n",
       "      <td>Positive</td>\n",
       "      <td>2564</td>\n",
       "      <td>319</td>\n",
       "      <td>2021-03-04 01:31:51</td>\n",
       "      <td>guys this rocket has been like the spacex boos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19244 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiment  score  comms_num            timestamp  \\\n",
       "0      Negative      0         47  2021-01-28 21:30:35   \n",
       "1      Negative    317         53  2021-01-28 21:26:27   \n",
       "2      Negative    405        178  2021-01-28 21:19:31   \n",
       "3      Positive    222         70  2021-01-28 21:18:25   \n",
       "4      Positive      0         16  2021-01-28 21:17:10   \n",
       "...         ...    ...        ...                  ...   \n",
       "19239   Neutral    130         13  2021-03-04 01:54:44   \n",
       "19240  Positive    105         72  2021-03-04 01:51:52   \n",
       "19241  Positive     18         11  2021-03-04 01:41:49   \n",
       "19242  Positive    296         46  2021-03-04 01:36:29   \n",
       "19243  Positive   2564        319  2021-03-04 01:31:51   \n",
       "\n",
       "                                              clean_text  \n",
       "0      the ceo of nasdaq pushed to halt trading to gi...  \n",
       "1      hedgefund whales are spreading disinfo saying ...  \n",
       "2      life isn fair my mother always told me that wh...  \n",
       "3      i believe right now is one of those rare oppor...  \n",
       "4      you guys are champs gme who would have thought...  \n",
       "...                                                  ...  \n",
       "19239                                              x200b  \n",
       "19240  rkt as tech platform real estate doubled auto ...  \n",
       "19241  vonage is the 2nd largest ucaas unified commun...  \n",
       "19242  i don know if am an ape autist or retard ve be...  \n",
       "19243  guys this rocket has been like the spacex boos...  \n",
       "\n",
       "[19244 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load data and merge them  \n",
    "df=pd.read_csv(\"body_sentiment.csv\")\n",
    "\n",
    "# change column name \n",
    "df.rename({'clean_body': 'clean_text'}, axis=1, inplace=True)\n",
    "#look at ddta\n",
    "print(df.info()) \n",
    "sents_count=pd.DataFrame(df.groupby(\"Sentiment\")[\"Sentiment\"].count()).rename(columns={\"Sentiment\":\"Entries\"}).reset_index().style.set_caption(\"Sentiment overview\")\n",
    "sents_count\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment', 'clean_text']\n"
     ]
    }
   ],
   "source": [
    "# dropping the columns we dont need\n",
    "df.drop(columns=['score', 'comms_num', 'timestamp'], inplace=True)\n",
    "df\n",
    "print (list(df))\n",
    "df['clean_text']=df['clean_text'].apply(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokens(row):\n",
    "    posts = row['clean_text']\n",
    "    tokens = nltk.word_tokenize(posts)\n",
    "    # taken only words (not punctuation)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "df['words'] = df.apply(make_tokens, axis=1)\n",
    "\n",
    "#dropping the rows where the string is empty \n",
    "df=df[df['words'].map(lambda d: len(d)) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lemmatize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def Lemmatizer(row):\n",
    "    my_list = row['words']\n",
    "    lemma_list = [lemmatizer.lemmatize(word) for word in my_list]\n",
    "    return (lemma_list)\n",
    "\n",
    "df['lemmatized_words'] = df.apply(Lemmatizer, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### removing stopwords \n",
    "\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['lemmatized_words']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "df['lemma_meaningful'] = df.apply(remove_stops, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### make back into string \n",
    "\n",
    "def make_string(row):\n",
    "    my_list = row['lemma_meaningful']\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    return joined_words\n",
    "\n",
    "df['processed'] = df.apply(make_string, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop irrelavant columns \n",
    "df.drop(columns=['clean_text', 'words', 'lemmatized_words'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create csv \n",
    "\n",
    "# create csv file \n",
    "df.to_csv('processed_body_data.csv',index=False )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
