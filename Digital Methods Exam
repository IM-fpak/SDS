### ASDS2 EXAM

## Research Questions

The overarching aim of the project is to explore similarities and differences in the Covid-19 anti-vaccine debate between Denmark, Germany and Poland (see section XX for details).

In this section, the scope is to investigate the following sub questions:

**RQ 1: What is the topical distribution of Tweets on the Covid-19 vaccine debate?**

The analysis aims to examine the topical distributions in each dataset, exploring both the &#39;themes&#39; of the topics and degree of meaningfulness. Moreover, the performance of a HSBM (Hierarchical Stochastic Block Model) will be compared to a LDA (Latent Dirichlet Allocation) to examine and evaluate possible differences in their results.

**RQ 2: What type of users partake in the covid-19 vaccine debate on Twitter?**

This sub-question aims to utilize a combination of Active Learning and Supervised Learning to label Twitter users by classifying tweets as either being pro or anti vaccine. In addition to exploring the proportions of user-types, the predicted labels will be integrated in the automated retweet-network (section XX).

## Methods and analytical choices

**Dataset**

**Word2Vec**

Word2Vec was integrated in the data collection process to extend the search queries in the attempt to improve dataset quality and decrease sampling bias. By utilizing the vector-representation of words, search queries can be extended based on word embeddings generated using a shallow neural network. Due to the (relatively) small corpuses (DK: 3.431, GER: 100.314, PL: 46.310), this paper used C-bow and a vector-size of 32 as these yielded more meaningful results. However, the Word2Vec did not work equally well for the datasets, which is most possibly due to the size of the corpuses.

**Preprocessing**

Prior to conducting the analyses, the text was preprocessed through the following steps: removing URLs, emojis, mentions &#39;@&#39;, &amp;amp; and replacing dashes &#39;-&#39; , colons &#39;:&#39;, &#39;\*&#39; and &#39;\_&#39; by empty strings. Then, all non-alphanumeric characters (excl. # for hashtags), numbers, single characters, double/triple/multiple whitespaces, and leading and trailing whitespaces were removed. Finally, all text was lowercase and stopwords (language specific) were removed before tokenizing and lemmatizing using the language specific versions of SpaCy (Ref).

**Topic models**

It is hypothesized that the HSBM will outperform the LDA in terms of quality, here meaningfulness of the detected topics, and therefore both models are run for comparison. Among the advantages of the HSBM is the ability to identify the optimal number of topics and best parameters. This is not supported in the LDA, so prior to running the LDA, the HSBM is used to select the number of topics, while the remaining parameters are defaulted. Another drawback of the LDA is that topics are drawn from a dirichlet distribution which introduces an assumption of homogeneity, hence assuming that all documents are characterized by the mixture of topics, and that all topics are characterized by the mixture of words, which is not always supported in the data.

Both of the topic models are run on bi-grams from the lemmatized tweets, as the tokens yielded topics that clustered together different word forms of the same lexeme. Moreover, both models were run on the entire danish dataset, while a random subset of 20.000 samples were taken from each of the german and polish datasets, as the HSBM does not scale well. Moreover, as the german and polish corpuses were considerably larger than the danish, a threshold of a minimum of two occurrences was set for a word to be included in the corpus.

**Text Classification**

Initially, Active Learning is deployed to efficiently label tweets using optimized query strategies for each of the three datasets using the &#39;superintendent&#39; module (ref). Active learning relies on and makes use of predicted probabilities and can be used to mitigate bias. The procedure involves creating a labeling widget based on a learning strategy, which takes as input unlabeled data, a classifier, and a learning strategy, which then returns the labels to be manually labeled depending on the learning strategy. By iteratively retraining the model, the data is automatically re-ordered to prioritise the optimal subsequent data points for labelling. The main analytical choices of the Active Learning procedure involved specifying learning sampling (certainty), training model (multinomial logistic regression model using a lbfgs-solver), and scoring of the 3-fold cross validation (accuracy, default). Furthermore, rather than opting for a binary classification task, inspection of the datasets identified both &#39;trash&#39; tweets (spam, irrelevant, not vaccine related), and &#39;neutral&#39; tweets and hence, four class options were available during the manual labeling. Lastly, an arbitrary minimum threshold of 1200 tweets to be manually labeled in each dataset was introduced.

Subsequent to Active Learning, the labelled datasets are used for text classification, as the outcome variable (label) is categorical. The text classification is conducted through supervised learning, comparing several classifiers with the goal of finding the optimal model for each language-specific dataset. As seen in figure 1, the labeled datasets revealed unbalanced classes and thus, stratified 80:20 splits of the datasets is done to preserve the class proportion of the datasets in the training and test data, attempting to avoid overfitting. After creating the train and test sets for each county, the following multinomial classifiers are implemented; Logistic Regression, Na√Øve Bayes, SVM, Bagging, and Boosting. The classifiers&#39; hyperparameters are tuned on each of the country-specific dataset to ensure optimal performance. To enable the classifiers to process natural language, the raw data is vectorized to create numerical representations using either document-term matrix or tf-idf, as specified in the vectorizer parameters in the model pipelines.

The overall strategy for the hyperparameter search is a &#39;bounded exploration&#39;, in which the initial search consists of searching (exponentially) for fewer parameters with a wider range of values. Subsequently, an iterative process of parameter-tuning is performed to narrow in on the optimal values in the search space, and is performed for all classifiers, for all datasets. The evaluation criteria of the 5-fold cross-validation is assessed on the f1\_micro score, as the default of evaluating on accuracy could potentially lead to models biased towards the majority class. Moreover, the weighted f1-score balances precision and recall and accounts for the expected imbalanced class proportions by aggregating the contributions of all classes to compute the average metric. Lastly, it should be noted that the vectorizer parameter of whether to remove english stop words was included as manual inspection of the datasets indicated that &#39;trash&#39; tweets were often irrelevant english tweets, and thus removing stop words might improve classification of these.

## Results

## **Topic models**

Based on manual inspection and qualitative evaluations of the meaningfulness of the topics of each of the datasets, the HSBM was found to outperform the LDA which generally produced conglomerates and &#39;junk&#39; topics ( appendix XX).While the qualitative assessment revealed that junk topics were also present in the HSBM, the majority of the topics were rather meaningful. Further details of the exact topic summarizations and country specific evaluations should be found in appendix XX. The topical distributions and qualitative evaluation of the individual topics are visualized in figure XX. In general, the HSBM appeared to yield better results for the German dataset, with the majority of the topics being meaningful. In contrast, in the Danish and Polish topic models, the non-meaningful topics were among the most prevalent topics. However, the qualitative evaluation of the individual topics predominantly yielded meaningful insights of the topics associated with the covid-19 vaccine debate on twitter for all three countries.

![](RackMultipart20210617-4-1m4g54l_html_c927deaefb79d7a8.png)

**Text classification**
The best models were chosen based on an evaluation combining performance metrics and the aim of the task. Thus, keeping the objective of labelling vaxxers and anti-vaxxers in mind, in addition to accuracy and AUC the best models were selected in relation to best balance between the f1-scores for the vaxx and anti-vaxx labels. The specific search grids, final parameters of the best models, and evaluation metrics are found in appendix XX. The best models for each of the countries were fitted on the entire labeled datasets (respective to the country) prior to classifying the unlabeled tweets. The predicted class proportions are depicted in figure XX. The classifier performance was evaluated by randomly sampling from the predictions and manually labeling the sampled tweets to assess the quality of the predictions, yielding the following metrics:

**Figure XX: class proportions ACTIVE LEARNING**

**Denmark**![](RackMultipart20210617-4-1m4g54l_html_1dc3b0509c3ec2d4.png)

![](RackMultipart20210617-4-1m4g54l_html_b2cd9e28a2f90c78.png)

The Danish SVM-classifier had an accuracy of .58 and a weighted f1-score of .62, possibly due to unbalanced classes. Precision, recall, and f1-scores indicate a tendency of predicting the label &#39;trash&#39; with a high recall score of .94, but only a precision of .62, while the &#39;neutral&#39; class appears to be impacted by this, having a recall of only .32 and precision of .14. Hence, the poor performance reflected in the accuracy might result from the classifier predictions being biased towards the &#39;trash&#39; class. However, in relation to the main objective to predict &#39;anti-vaxx&#39; and &#39;vaxx&#39; labels, the f1-scores of .65 and .59 are relatively good.

**Germany**![](RackMultipart20210617-4-1m4g54l_html_3fecc56217e82546.png)

![](RackMultipart20210617-4-1m4g54l_html_3fecc56217e82546.png)

The German SVM-classifier had an accuracy of .73 and a weighted f1-score of .72, thus approximately even. The f1-score for both the &#39;vaxx&#39; and &#39;anti-vaxx&#39; classes were .80, thereby indicating superior performance of the classifier on these classes in comparison to &#39;neutral&#39; (.57) and &#39;trash&#39; (.76).

**Poland**![](RackMultipart20210617-4-1m4g54l_html_5cd3bdb0a18a619e.png)

![](RackMultipart20210617-4-1m4g54l_html_5cd3bdb0a18a619e.png)

The Polish SVM-classifier yielded both a low accuracy (.36) and a low weighted f1-score (.39). Inspection of the evaluation metrics indicate unbalanced classes and i.e. the weighted precision is .57 in comparison to the weighted recall of .35. Moreover, the &#39;neutral&#39; class had a high precision of 0.68 and a low recall of .26 and in combination with visual inspection of the confusion matrix, this may suggest a bias towards the &#39;neutral&#39; class, supported further by the highly unbalanced class proportions of the predicted labels (figure XX).

Overall, the results of the classifier evaluations were mixed and especially the polish classifier performed poorly. This could be caused by various factors including (bad) data quality, manual errors during Active Learning, and bias. Thus, future steps should prioritize bias detection and correction. This should focus on correcting proportional estimates aggregated from the respective classifiers by estimating the misclassification probabilities and using these to correct the raw estimates of the class proportions. After using a new labelled test set to calculate the specific misclassification probabilities between each pair of classifications, the resulting confusion matrix should be used to find the corrective frequency (with respective to the individual classes) by taking the number of positive predictions (pos^) and multiplying it with the true positive rate (TPR=TP/(TP+FP)), then subtract the number of negative predictions (neg^) multiplied by the false negative rate (FNR=FN/(FN+TN), and finally dividing by the total number of predictions in the new test set. However, as this was not within the scope of the initial project, the results should be interpreted with caution.

To summarize, for all three datasets, the SVM-classifier yielded the best results. However, the performance was generally not great, suggesting that future steps should involve bias correction and possibly include transfer learning.
