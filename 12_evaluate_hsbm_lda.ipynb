{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the hSBM and the LDA model\n",
    "\n",
    "This notebook exemplifies how we evaluated the hSBM and the LDA topic models. We ran this code for all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import graph_tool.all as gt\n",
    "from hSBM_Topicmodel.sbmtm import sbmtm\n",
    "import pylab as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframe\n",
    "df = pd.read_csv('de_bigrams_data2.csv',  lineterminator='\\n')\n",
    "df.head()\n",
    "\n",
    "# the hSBM was run on random sample and we reproduce this sample here\n",
    "df = df.sample(n=20000, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to turn the tokenized list into a readable format\n",
    "def string_list(text):\n",
    "    \n",
    "    # we transform the string representation of the list into an actual list\n",
    "    text = ast.literal_eval(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to the lemma_no_mention column (since we'll use that to construct bigrams)\n",
    "df['token'] = df['token'].apply(string_list)\n",
    "df['lemma'] = df['lemma'].apply(string_list)\n",
    "df['token_no_mention'] = df['token_no_mention'].apply(string_list)\n",
    "df['lemma_no_mention'] = df['lemma_no_mention'].apply(string_list)\n",
    "df['lemma_uni_bi'] = df['lemma_uni_bi'].apply(string_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the hSBM model \n",
    "Here we inspect some properties of the model and visualize the topic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "hsbm_model = pickle.load(open(r'/home/root/model_sav/de_hsbm_bigram_sample20_nmin2.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ``hSBM`` package, the method ``.topics()`` returns all the topics that the model detected and their most predictive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of the topics and their most predictive words\n",
    "topics = hsbm_model.topics()\n",
    "num_topics = len(topics)\n",
    "print(f'The model detected {num_topics} topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the topics\n",
    "for t in topics:\n",
    "    print(f\"\\nTopic {t}: \\n\")\n",
    "    for tup in topics[t]:\n",
    "        print(tup[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to analyse the results, we need to access the probability of each word-group (= topic) in each document (= tweet).\n",
    "\n",
    "We use the ``.get_groups()`` method which returns a dictionary that contains different information on word-group and topic-groups. We are interested in the ``p_tw_d`` part of the dictionary which contains a ``numpy`` array that specifies the probability of each word-group (= topic) in each document (= tweet). \n",
    "\n",
    "For reference, in the ``sbmtm.py`` file, the ``.getgroups()`` method is described as follows:\n",
    "```\n",
    "'''\n",
    "extract statistics on group membership of nodes form the inferred state.\n",
    "return dictionary\n",
    "- B_d, int, number of doc-groups\n",
    "- B_w, int, number of word-groups\n",
    "- p_tw_w, array B_w x V; word-group-membership:\n",
    "     prob that word-node w belongs to word-group tw: P(tw | w)\n",
    "- p_td_d, array B_d x D; doc-group membership:\n",
    "     prob that doc-node d belongs to doc-group td: P(td | d)\n",
    "- p_w_tw, array V x B_w; topic distribution:\n",
    "     prob of word w given topic tw P(w | tw)\n",
    "- p_tw_d, array B_w x d; doc-topic mixtures:\n",
    "     prob of word-group tw in doc d P(tw | d)\n",
    "'''\n",
    "```\n",
    "\n",
    "Once we've accessed this ``numpy`` array, we turn it into a ``pandas`` dataframe and we will retrieve the tweets that have the highest probabilities of each word-group (= topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to access the numpy array which is stored in \n",
    "# model.get_groups()['p_tw_d']\n",
    "dist_dict = hsbm_model.get_groups()['p_tw_d']\n",
    "\n",
    "# we turn the numpy array into a pandas dataframe and transpose it\n",
    "# so that the topics are in the columns and the tweets are in the rows\n",
    "topic_docs = pd.DataFrame(dist_dict).transpose()\n",
    "\n",
    "# we drop all rows that contain NaN values: since we only considered words that occur at least 5 times, there are some\n",
    "# tweets which were not used in the topic model and they therefore contain NaN values\n",
    "topic_docs.dropna(inplace=True)\n",
    "\n",
    "# display the topic_docs dataframe\n",
    "topic_docs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting the most prevalent topics\n",
    "# we select all columns except for the last one since the last column contains the number of the dominant topic\n",
    "# we sum up all weights and divide by the total number of tweets\n",
    "topic_sum = pd.DataFrame(topic_docs.sum() / len(df), columns=['prevalence'])\n",
    "\n",
    "# get the 25 most prevalent topics\n",
    "prev_topics = topic_sum.sort_values(by='prevalence', ascending=False)\n",
    "prev_topics.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data to be plotted\n",
    "plot_df = prev_topics.head(25)\n",
    "plot_df.reset_index(inplace=True)\n",
    "plot_df.to_csv('de_topicmodel_plot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the prevalent topics\n",
    "prev_index = prev_topics.head(25).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the most representative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to retrieve the most 10 most representative tweets for each topic\n",
    "\n",
    "# empty list to store the indices in\n",
    "index_list = []\n",
    "\n",
    "# we iterate through the number of topics\n",
    "for i in range(len(topics)):\n",
    "    \n",
    "    # we retrieve the index of the 10 most representative tweets (returns a pandas index object)\n",
    "    index = topic_docs.sort_values(by=[i], ascending=False).iloc[:10].index\n",
    "    index_list.append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets for the most prevalent topics\n",
    "#### ...in descending order, i.e. stariting with the most prevalent topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the text of these most representive tweets for all topics\n",
    "\n",
    "# iterate through the number of topics\n",
    "for i in prev_index:\n",
    "    \n",
    "    # print the Topic number, the words of each topic and its most representative tweets   \n",
    "    \n",
    "    print(f\"\"\"*************************************************************************************************'\n",
    "          \\n \\033[1m --- Topic {i} --- \\033[0m \\n \n",
    "          \\n \\033[1m --- Words: --- \\033[0m \\n \n",
    "          \\n {[j[0] for j in topics[i]]} \\n \n",
    "          \\n \\033[1m --- Most representative tweets: --- \\033[0m \\n\\n\"\"\")\n",
    "    \n",
    "    for text in df.iloc[index_list[i]]['text']:\n",
    "        print(f'>> {text} \\n')\n",
    "    \n",
    "    print(f'\\n \\033[1m --- Most representative list of lemmas: --- \\033[0m \\n\\n')\n",
    "    \n",
    "    for lemma in df.iloc[index_list[i]]['lemma_uni_bi']:\n",
    "              print(f'>> {lemma} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets for all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the text of these most representive tweets for all topics\n",
    "\n",
    "# iterate through the number of topics\n",
    "for i in range(len(topics)):\n",
    "    \n",
    "    # print the Topic number, the words of each topic and its most representative tweets\n",
    "    print(f\"\"\"*************************************************************************************************'\n",
    "          \\n \\033[1m --- Topic {i} --- \\033[0m \\n \n",
    "          \\n \\033[1m --- Words: --- \\033[0m \\n \n",
    "          \\n {[j[0] for j in topics[i]]} \\n \n",
    "          \\n \\033[1m --- Most representative tweets: --- \\033[0m \\n\\n\"\"\")\n",
    "    \n",
    "    for text in df.iloc[index_list[i]]['text']:\n",
    "        print(f'>> {text} \\n')\n",
    "    \n",
    "    print(f'\\n \\033[1m Most representative list of lemmas: --- \\033[0m \\n\\n')\n",
    "    \n",
    "    for lemma in df.iloc[index_list[i]]['lemma_uni_bi']:\n",
    "              print(f'>> {lemma} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we visualize the topics_sum dataframe (which we created further up) in a barplot\n",
    "\n",
    "# either do topic_sum.bar.plot() or try seaborn\n",
    "ax = sns.barplot(x=topic_sum.index, y = 'prevalence', color = '#175985', data=topic_sum)\n",
    "xtickslocs = ax.get_xticks()\n",
    "_ = ax.set_xticks(np.arange(xtickslocs[0], xtickslocs[-1], 50))\n",
    "_ = ax.set_xticklabels(np.arange(xtickslocs[0], xtickslocs[-1], 50))\n",
    "_ = ax.set_xlabel(\"Topics\")\n",
    "_ = ax.set_ylabel(\"Weighted contribution of each topic\")\n",
    "_ = ax.set_title(\"hSBM: Distribution of topics by weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the topic-document distribution\n",
    "The code for this heatmap is taken from the ``sbmtm.py`` file of the hSBM package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the distribution of topics over documents\n",
    "\n",
    "# select all columns except for the last one (because it contains the number of the dominant topic)\n",
    "p_tw_d = topic_docs\n",
    "\n",
    "# figure\n",
    "fig=plt.figure(figsize=(12,10))\n",
    "\n",
    "# get color map: see https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "# for reversed cmaps do: \n",
    "# color_map = plt. cm. get_cmap('PuBuGn')\n",
    "# reversed_color_map = color_map. reversed() \n",
    "\n",
    "# plot the heatmap\n",
    "plt.imshow(p_tw_d,origin='lower',aspect='auto',interpolation='none', cmap='Reds')\n",
    "plt.title(r'hSBM: Probability of each word-group (= topic) in each document (= tweet)')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Document')\n",
    "_ = plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "lda_model = pickle.load(open(r'/home/root/model_sav/de_lda_sample20_nmin2.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a id2word dictionary\n",
    "\n",
    "#Insert the column where you saved unigram and bigram tokens between the parentheses\n",
    "id2word = Dictionary(df['lemma_uni_bi']) \n",
    "\n",
    "#Viewing how many words are in our vocabulary\n",
    "print(len(id2word))\n",
    "\n",
    "# Use filter_extremes to remove very frequent (those that appear in more than 99.9% of the \n",
    "# documents) and very infrequent words (those that appear in less than 10 documents)\n",
    "id2word.filter_extremes(no_below=2, no_above=1)\n",
    "\n",
    "#Viewing how many words are in our vocabulary\n",
    "print(len(id2word))\n",
    "\n",
    "# creating a corpus object\n",
    "corpus = [id2word.doc2bow(doc) for doc in df['lemma_uni_bi']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic overview\n",
    "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in lda_model.print_topics(num_topics,10)]\n",
    "topic_words = []\n",
    "\n",
    "# Printing the topics in a nice format\n",
    "for id, t in enumerate(words): \n",
    "    print(f\"------ Topic {id} ------\")\n",
    "    print(' '.join(t), end=\"\\n\\n\")\n",
    "    topic_words.append(' '.join(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the most representative tweets for teach topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the gamma probabilities show how prevalent a topic is in a given document\n",
    "\n",
    "# gamma_list is a list containing another list for each doc within the nested list are tuples (topic, probability)\n",
    "gamma_list = list(lda_model.get_document_topics(corpus, minimum_probability = 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list to store probabilities in in\n",
    "gamma_df_list = []\n",
    "\n",
    "# iterating through the gamma_list\n",
    "for doc in gamma_list:\n",
    "    \n",
    "    # retrieving all probabilities and storing them in a list\n",
    "    prob = [tup[1] for tup in doc]\n",
    "    \n",
    "    # adding list to the gamma_df_list that we'll turn into a pandas dataframe\n",
    "    gamma_df_list.append(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most prevalent topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the list into a dataframe\n",
    "gamma_df = pd.DataFrame(gamma_df_list)\n",
    "\n",
    "# turn into dataframe\n",
    "gamma_sum = pd.DataFrame(gamma_df.sum() / len(gamma_df), columns=['prevalence'])\n",
    "\n",
    "# get the 25 most prevalent topics\n",
    "lda_prev_topics = gamma_sum.sort_values(by='prevalence', ascending=False)\n",
    "lda_prev_topics.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the prevalent topics\n",
    "lda_prev_index = lda_prev_topics.head(25).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets for the most prevalent topics\n",
    "#### ...in descending order, i.e. stariting with the most prevalent topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to retrieve the most 10 most representative tweets for each topic\n",
    "\n",
    "# empty list to store the indices in\n",
    "lda_index_list = []\n",
    "\n",
    "# we iterate through the number of topics\n",
    "for i in range(num_topics):\n",
    "    \n",
    "    # we retrieve the index of the 10 most representative tweets (returns a pandas index object)\n",
    "    index = gamma_df.sort_values(by=[i], ascending=False).iloc[:10].index\n",
    "    lda_index_list.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_df.sort_values(by=[i], ascending=False).iloc[:10].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the text of these most representive tweets for all topics\n",
    "\n",
    "# iterate through the number of topics\n",
    "for i in range(len(lda_prev_index)):\n",
    "    \n",
    "    # print the Topic number, the words of each topic and its most representative tweets   \n",
    "    \n",
    "    print(f\"\"\"*************************************************************************************************'\n",
    "          \\n \\033[1m --- Topic {lda_prev_index[i]} --- \\033[0m \\n \n",
    "          \\n \\033[1m --- Words: --- \\033[0m \\n \n",
    "          \\n {topic_words[i]} \\n \n",
    "          \\n \\033[1m --- Most representative tweets: --- \\033[0m \\n\\n\"\"\")\n",
    "    \n",
    "    for text in df.iloc[lda_index_list[i]]['text']:\n",
    "        print(f'>> {text} \\n')\n",
    "    \n",
    "    print(f'\\n \\033[1m --- Most representative list of lemmas: --- \\033[0m \\n\\n')\n",
    "    \n",
    "    for lemma in df.iloc[lda_index_list[i]]['lemma_uni_bi']:\n",
    "              print(f'>> {lemma} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets for all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the text of these most representive tweets for all topics\n",
    "\n",
    "# iterate through the number of topics\n",
    "for i in range(num_topics):\n",
    "    \n",
    "    # print the Topic number, the words of each topic and its most representative tweets   \n",
    "    \n",
    "    print(f\"\"\"*************************************************************************************************'\n",
    "          \\n \\033[1m --- Topic {i} --- \\033[0m \\n \n",
    "          \\n \\033[1m --- Words: --- \\033[0m \\n \n",
    "          \\n {topic_words[i]} \\n \n",
    "          \\n \\033[1m --- Most representative tweets: --- \\033[0m \\n\\n\"\"\")\n",
    "    \n",
    "    for text in df.iloc[lda_index_list[i]]['text']:\n",
    "        print(f'>> {text} \\n')\n",
    "    \n",
    "    print(f'\\n \\033[1m --- Most representative list of lemmas: --- \\033[0m \\n\\n')\n",
    "    \n",
    "    for lemma in df.iloc[lda_index_list[i]]['lemma_uni_bi']:\n",
    "              print(f'>> {lemma} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "\n",
    "# either do topic_sum.bar.plot() or try seaborn\n",
    "ax = sns.barplot(x=gamma_sum.index, y = 'prevalence', color = '#175985', data=gamma_sum)\n",
    "xtickslocs = ax.get_xticks()\n",
    "_ = ax.set_xticks(np.arange(xtickslocs[0], xtickslocs[-1], 50))\n",
    "_ = ax.set_xticklabels(np.arange(xtickslocs[0], xtickslocs[-1], 50))\n",
    "_ = ax.set_xlabel(\"Topics\")\n",
    "_ = ax.set_ylabel(\"Weighted contribution of each topic\")\n",
    "_ = ax.set_title(\"LDA: Distribution of topics by weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the topic-document distribution\n",
    "The code for this heatmap is taken from the ``sbmtm.py`` file of the hSBM package, but as soon as we have the gamma probabilities, the code works equally on the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the distribution of topics over documents\n",
    "\n",
    "# select all columns except for the last one (because it contains the number of the dominant topic)\n",
    "p_tw_d = gamma_df\n",
    "\n",
    "# figure\n",
    "fig=plt.figure(figsize=(12,10))\n",
    "\n",
    "# get color map: see https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "# for reversed cmaps do: \n",
    "# color_map = plt. cm. get_cmap('PuBuGn')\n",
    "# reversed_color_map = color_map. reversed() \n",
    "\n",
    "# plot the heatmap\n",
    "plt.imshow(p_tw_d,origin='lower',aspect='auto',interpolation='none', cmap='Blues')\n",
    "plt.title(r'hSBM: Probability of each word-group (= topic) in each document (= tweet)')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Document')\n",
    "_ = plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
