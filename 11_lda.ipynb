{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import pickle \n",
    "\n",
    "#Importing packages for LDA\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "os.chdir(r'C:\\Users\\maril\\Documents\\20-21 KU\\block 4\\DM\\twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "The hSBM found the following number of topics in each of the datasets:\n",
    "* German dataset: 60 topics (note: random sample of 20,000 tweets, seed=40, n_min=2)\n",
    "* Polish dataset: 66 topics (note: random sample of 20,000 tweets, seed=40, n_min=2)\n",
    "* Danish dataset: 17 topics (note: full sample, n_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "de = pd.read_csv(r'topic model\\de_hsbm_data.csv')\n",
    "print(de.shape)\n",
    "de.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "da = pd.read_csv(r'topic model\\da_hsbm_data.csv')\n",
    "print(da.shape)\n",
    "da.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "pl = pd.read_csv(r'topic model\\pl_hsbm_data.csv')\n",
    "print(pl.shape)\n",
    "pl.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to turn the tokenized list into a readable format\n",
    "def string_list(text):\n",
    "    \n",
    "    # we transform the string representation of the list into an actual list\n",
    "    text = ast.literal_eval(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to all relevant columns\n",
    "de['lemma_uni_bi'] = de['lemma_uni_bi'].apply(string_list)\n",
    "da['lemma_uni_bi'] = da['lemma_uni_bi'].apply(string_list)\n",
    "pl['lemma_uni_bi'] = pl['lemma_uni_bi'].apply(string_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the German data: subsample to match the hSBM subsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "de = de.sample(n=20000, random_state=40)\n",
    "pl = pl.sample(n=20000, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a id2word dictionary\n",
    "\n",
    "# Insert the column where you saved unigram and bigram tokens between the parentheses\n",
    "de_id2word = Dictionary(de['lemma_uni_bi']) \n",
    "da_id2word = Dictionary(da['lemma_uni_bi'])\n",
    "pl_id2word = Dictionary(pl['lemma_uni_bi'])\n",
    "\n",
    "# Viewing how many words are in our vocabulary\n",
    "print('German data:', len(de_id2word))\n",
    "print('Danish data:', len(da_id2word))\n",
    "print('Polish data:', len(pl_id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use filter_extremes to remove very frequent (those that appear in more than 99.9% of the \n",
    "# documents) and very infrequent words (those that appear in less than 10 documents)\n",
    "de_id2word.filter_extremes(no_below=2, no_above=1)\n",
    "da_id2word.filter_extremes(no_below=0, no_above=1)\n",
    "pl_id2word.filter_extremes(no_below=2, no_above=1)\n",
    "\n",
    "#Viewing how many words are in our vocabulary\n",
    "print('German data:', len(de_id2word))\n",
    "print('Danish data:', len(da_id2word))\n",
    "print('Polish data:', len(pl_id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a corpus object\n",
    "de_corpus = [de_id2word.doc2bow(doc) for doc in de['lemma_uni_bi']]\n",
    "da_corpus = [da_id2word.doc2bow(doc) for doc in da['lemma_uni_bi']]\n",
    "pl_corpus = [pl_id2word.doc2bow(doc) for doc in pl['lemma_uni_bi']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the number of topics\n",
    "de_topics = 60\n",
    "da_topics = 21\n",
    "pl_topics = 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the LDA with number of topics specified by the hSBM\n",
    "de_lda_model = LdaMulticore(corpus=de_corpus, num_topics=de_topics, id2word=de_id2word, passes = 1, iterations = 50)\n",
    "da_lda_model = LdaMulticore(corpus=da_corpus, num_topics=da_topics, id2word=da_id2word, passes = 1, iterations = 50)\n",
    "pl_lda_model = LdaMulticore(corpus=pl_corpus, num_topics=pl_topics, id2word=pl_id2word, passes = 1, iterations = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "pickle.dump(de_lda_model, open(r'topic model\\de_lda_sample20_nmin2.sav', 'wb'))\n",
    "pickle.dump(da_lda_model, open(r'topic model\\da_lda_all_nmin0.sav', 'wb'))\n",
    "pickle.dump(pl_lda_model, open(r'topic model\\pl_lda_sample20_nmin2.sav', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
