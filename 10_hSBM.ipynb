{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEENV54M8_a9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyfqTUGGBva0"
   },
   "source": [
    "# Hierarchical Stochastic Block Model\n",
    "\n",
    "This is the only notebook that we ran on Google Colab. The rest of the code was run locally in order to comply with GDPR requirements, but since the hSBM is very computing-heavy it was only feasible to run this notebook on Google Colab. \n",
    "\n",
    "To mitigate data protection concerns that arise when uploading and processing person-sensitive data (such as tweets and user names) on Google Colab, we took the following precautions: We only uploaded a dataset containing preprocessed, tokenized and lemmatized tweet texts without further metadata (i.e. these datasets did not contain a user handle or tweet creation date). Moreover, all user mentions were removed from the tweet texts during the preprocessing. Since these lists of lemmas do not contain any personally identifiable information, we argue that these datasets are not covered by the data protection rules layed out in the GDPR and that we can therefore safely upload and process them on Google Drive and Google Colab.\n",
    "\n",
    "Once the hSBM has been fitted, we saved and downloaded the models. All further investigation of results in which we, for example, read the original tweet texts in order to validate the model results, were conducted on our local computers and in accordance with the rules specified by the GDPR and the UCPH data protection guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IiS5m5Pc39R2",
    "outputId": "5004f575-ee59-4f0d-c874-7514ed5fdbb2"
   },
   "outputs": [],
   "source": [
    "# access files in google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6LFMoZsjOAX"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = '/content/gdrive/MyDrive/Colab Notebooks/de_lemma.csv'\n",
    "df = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hPkFc16Ftx8"
   },
   "outputs": [],
   "source": [
    "# function to turn the tokenized list into a readable format\n",
    "def string_list(text):\n",
    "    \n",
    "    # we transform the string representation of the list into an actual list\n",
    "    text = ast.literal_eval(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_1_3BikFueK",
    "outputId": "3df51a15-cf30-4b81-ea5a-a93b28cce00f"
   },
   "outputs": [],
   "source": [
    "# apply function to all relevant columns\n",
    "df['lemma_no_mention'] = df['lemma_no_mention'].apply(string_list)\n",
    "\n",
    "# display dataframe\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tHJjriV8mhH",
    "outputId": "7a2b066e-4a41-485e-9c09-4d32b50ea3e8"
   },
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install -q condacolab\n",
    "\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "\n",
    "import condacolab\n",
    "\n",
    "! conda config --add channels conda-forge\n",
    "! conda config --add channels ostrokach-forge\n",
    "! conda config --add channels pkgw-forge\n",
    " \n",
    "! conda install gtk3 \n",
    "! conda install pygobject graph-tool cairo\n",
    "! conda install -c conda-forge graph-tool \n",
    "! git clone https://github.com/martingerlach/hSBM_Topicmodel.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjvR8MBhGlaO"
   },
   "outputs": [],
   "source": [
    "# import the packages we just installed\n",
    "import graph_tool.all as gt\n",
    "from hSBM_Topicmodel.sbmtm import sbmtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fV00Mwa7BqS"
   },
   "source": [
    "# Run the HSBM model\n",
    "\n",
    "we ran the models on the three datasets with the following specifications:\n",
    "\n",
    "* Danish dataset: Since the Danish dataset is so small, the model is run on the full sample. The ``n_min`` paramater was set to 0.\n",
    "* German dataset: We randomly subsample 20,000 tweets and set the ``n_min`` paramater was set to 2.\n",
    "* German dataset: We randomly subsample 20,000 tweets and set the ``n_min`` paramater was set to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h0f4E5mDG3aq",
    "outputId": "20805612-8063-489f-d875-1d4e2175e140"
   },
   "outputs": [],
   "source": [
    "## the sampling step is only relevant for the Polish and the German dataset\n",
    "\n",
    "# randomly sample \n",
    "sample = df.sample(n=20000, random_state=3)\n",
    "\n",
    "# turn the 'lemma_no_mention' series into list\n",
    "texts = sample['lemma_no_mention'].tolist()\n",
    "\n",
    "# see it if worked\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2X1qy9vA_Cyj"
   },
   "outputs": [],
   "source": [
    "# create an instance of the class\n",
    "model = sbmtm()\n",
    "\n",
    "# seed for graph-tool's random number generator --> same results\n",
    "gt.seed_rng(40) \n",
    "\n",
    "# create the graph\n",
    "# for German and Polish, we specify n_min = 5; since the Danish corpus is so small we do not specify it in the Danish case\n",
    "model.make_graph(texts,documents=['%d'%i for i in range(len(texts))], n_min=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ducDtQ9I_FKb"
   },
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZjidY9CjXWz"
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'de_hsbm_sample20_nmin5.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hSBM_model",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
