{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import emoji\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve query terms using word embeddings\n",
    "\n",
    "This notebook exemplifies how we used word frequencies and word2vec to improve our query terms. We ran this notebook for all three datasets, but to keep the appendix concise we only show the example for one of the datasets here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "os.chdir(r'C:\\Users\\maril\\Documents\\20-21 KU\\block 4\\DM\\twitter\\preprocess_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_csv('de_preprocess.csv', encoding='utf8')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to turn the tokenized list into a readable format\n",
    "def string_list(text):\n",
    "    \n",
    "    # we transform the string representation of the list into an actual list\n",
    "    text = ast.literal_eval(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function\n",
    "df['preprocess_token'] = df['preprocess_token'].apply(string_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the word frequency\n",
    "\n",
    "# empty dict to store the word frequency information in\n",
    "freq_dict = dict()\n",
    "\n",
    "# iterate through the list of token in the 'pre_process' column\n",
    "for tweet in df['preprocess_token']:\n",
    "    \n",
    "    # iterate through the words in each list\n",
    "    for word in tweet:\n",
    "        \n",
    "        # if the word is not yet dict, add the word as the key and 1 as its value\n",
    "        if word not in freq_dict:\n",
    "            freq_dict[word] = 1\n",
    "        \n",
    "        # if word is already in the dict, update its value by 1\n",
    "        else:\n",
    "            freq_dict[word] += 1\n",
    "            \n",
    "# order dict: sort the dictionary by its values\n",
    "freq_dict = dict(sorted(freq_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# display the 200 most common words\n",
    "# .items() returns the dict items as tuples, we turn that into a list in order to retrieve the first 150 words\n",
    "# then we turn the list back into a dict (because I think that looks nicer)\n",
    "dict(list(freq_dict.items())[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing the tweet text, we check the most common words in order to see if there are words we missed in our query so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent words: Results\n",
    "Among the most frequent 200 words, there are a lot of words that are Covid-19 vaccine specific. We manually read through the list and select all keywords that seem relevant and specific enough. An example for the German query term list is noted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save most relevant frequent words in a list\n",
    "freq_keywords = ['impfung', 'geimpft', 'impfstoff', 'biontech', 'impfzentrum', 'impfen', 'impftermin', 'moderna', \n",
    "                 'pfizer', 'astrazeneca', 'impfpass', 'dosis', 'nebenwirkungen', 'impf', 'stiko', 'impfungen', \n",
    "                 'zweitimpfung', 'impfpflicht', 'astra', 'impftermine', 'prio', 'impfzwang', 'impfschutz', 'mrna', \n",
    "                 'geimpfte', 'impfstoffe',  'priorisierung', 'erstimpfung',  'coronaimpfung',  'geimpften',  'dosen',  \n",
    "                 'impfzentren',  'impfneid',  'j&j',  'thrombose', 'curevac',  'erstimpfungen',  'impfdosen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python implementation gensim\n",
    "\n",
    "We use the package ``gensim`` and its implementation of word2vec. \n",
    "\n",
    "There are two ways to approach the word2vec training:\n",
    "\n",
    "**OPTION 1**\n",
    "Use \n",
    "```python \n",
    "Word2Vec(dataset, **keywordarguments)\n",
    "``` \n",
    "to set all parameters, build the vocabulary and train the model all in one go.\n",
    "\n",
    "**OPTION 2** \n",
    "Split the different steps (set parameters, build vocab and train the model) to get a better overview of what is happening where and how different settings influence the outcomes. We specify:\n",
    "```python\n",
    "model = Word2Vec(**keywordarguments)\n",
    "model.build_vocab(**keywordarguments)\n",
    "model.train(dataset, **keywordarguments)\n",
    "```\n",
    "\n",
    "When passing the same hyperparameters, both options yield exactly the same results. They are included here because both of these approaches appear in tutorials and Stackoverflow; and it is an important point to not mix them up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using word2vec**\n",
    "\n",
    "Generally, we are interested in the word similarities:\n",
    "```python\n",
    "model.wv.most_similar(keyword, **keywordarguments)\n",
    "```\n",
    "\n",
    "There are a lot of other things we can do with the results, e.g. access the vectors, but since they are not that relevant for our query building purposes, we won't go into detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many words we have in the corpus\n",
    "\n",
    "# set counter to 0\n",
    "c = 0\n",
    "\n",
    "# iterate through the token list in the 'preprocess' column\n",
    "for word_list in df['preprocess_token']:\n",
    "    \n",
    "    # iterate through the tokens in the list\n",
    "    for word in word_list:\n",
    "        \n",
    "        # update the counter for each word\n",
    "        c+=1\n",
    "\n",
    "# amount of words we have\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "\n",
    "# initiate and train the model\n",
    "w2v_model = Word2Vec(df['preprocess_token'], \n",
    "                     min_count=15,\n",
    "                     window=3,\n",
    "                     vector_size=20,\n",
    "                     negative=10,\n",
    "                     epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check manually if the results make sense: you can put any keyword from the corpus and look up the most similar words\n",
    "w2v_model.wv.most_similar(['impfung'], topn=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most similar words to the word for specific terms\n",
    "\n",
    "# we decided to check synonyms for the most frequent, vaccine-relevant words that I saved in the list\n",
    "# 'freq_keywords' above\n",
    "\n",
    "for keyword in freq_keywords:\n",
    "    display(keyword,\n",
    "            w2v_model.wv.most_similar([keyword], topn=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is again an example for the German corpus, but we conducted these steps for all three datasets. It is important to note, however, that the Danish corpus was so small that word2vec did not produce any useful results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving all relevant keywords from the 'most similar' terms provided through word2vec in a list\n",
    "\n",
    "similar_keywords = ['titer', 'impfdosis', 'impfungen', 'coronaimpfung', 'kreuzimpfung', 'durchgeimpft', 'geimpften', \n",
    "                    'impfstoffe', 'impfstoffes', 'impfstoffen', 'vakzine', 'impfstoffs', 'vakzin', 'impfdosen', \n",
    "                    'biontec', 'moderna', 'comirnaty', 'vaxzevria', 'astrazeneka', 'mrna', 'bnt', 'impfteam', \n",
    "                    'impfzentrums', 'biontech', 'biontec', 'pfizer', 'comirnaty', 'bnt', 'astrazeneca', \n",
    "                    'astrazeneka', 'moderna', 'covaxin', 'johnsonjohnson', 'astra', 'vaxzevria', 'astrazenaca', \n",
    "                    'astrazenica', 'johnson&johnson', 'impfausweis', 'impfpaß', 'impfnachweis', 'impfstatus', \n",
    "                    'zweitimpfung', 'erstimpfung', 'impfdosis', 'zweitgeimpft', 'impfschema', 'nebenwirkungen', \n",
    "                    'nebenwirkung', 'impfnebenwirkungen', 'impfreaktion', 'vaxxer', 'impfkommission', \n",
    "                    'schutzimpfungen', 'impfstoffdosen', 'impfquote', 'impfung', 'impfstoffe', 'erstimpfung', \n",
    "                    'dosis', 'impfabstand', 'zweitdosis', 'erstgeimpften', 'impfzwang', 'zwangsimpfung', \n",
    "                    'astrazeneca', 'astraz', 'vaxzevria', 'astrazeneka', 'impftermin', 'impfberechtigt', 'priogruppe', \n",
    "                    'impfprio', 'impfpflicht', 'zwangsimpfung', 'impfprivilegien', 'impfwirkung', 'geimpften', \n",
    "                    'geimpfter', 'impfstoffes', 'impfstoffs', 'impfpriorisierung', 'impfenschuetzt', \n",
    "                    'impfenrettetleben', 'impfenschützt', 'coronaschutzimpfung', 'aermelhoch', 'ungeimpften', \n",
    "                    'verimpft', 'impflinge', 'j&j', 'vaxzevria', 'blutgerinnsel', 'thrombosen', 'sinusvenenthrombose', \n",
    "                    'sanofi', 'impfstopp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a final keyword list (both old and new keywords)\n",
    "\n",
    "# change to the place the keyword csv file is saved in\n",
    "os.chdir(r'C:\\Users\\maril\\Documents\\20-21 KU\\block 4\\DM\\twitter')\n",
    "\n",
    "# read keyword file\n",
    "# empty list to stored the updated (old) keywords in\n",
    "old_keywords = []\n",
    "\n",
    "with open('keywords_updated.csv', 'r', encoding='utf8') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    \n",
    "    # iterating through the reader object and adding all keywords to the old_keywords list\n",
    "    for row in reader:\n",
    "        old_keywords.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the keywords and remove duplicates\n",
    "final_keywords = []\n",
    "\n",
    "for f in freq_keywords:\n",
    "    if f not in final_keywords:\n",
    "        final_keywords.append(f)\n",
    "    \n",
    "for w in similar_keywords:\n",
    "     if w not in final_keywords:\n",
    "        final_keywords.append(w)\n",
    "    \n",
    "for o in old_keywords:\n",
    "     if o not in final_keywords:\n",
    "        final_keywords.append(o)\n",
    "\n",
    "print(final_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final_keywords list to a csv file\n",
    "with open('final_keywords.csv', 'w', newline=\"\", encoding='utf8') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    for word in final_keywords:\n",
    "        writer.writerow([word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Keywords & Parameters**\n",
    "\n",
    "Overview of keywords which can be passed:\n",
    "  \n",
    "* ``sentences=None``: The data the model is trained on. Has to be an iterable (such as a list of tokens). If this argument is not passend, the model is left uninitialized.\n",
    "* ``corpus_file=None`` Path to file containing the corpus (this can be used if the vocabulary is build separately from training the model). Either pass the path to the corpus file or the ``sentences`` parameter; but not both.\n",
    "* ``vector_size=100``: Size of the word vectors. At some point, adding more dimension diminishes the quality (Mikolov et al., 2019, p. 6), and the vector size is usually recommended to be around 100-300.\n",
    "* ``alpha=0.025``: Initial learning rate.\n",
    "* ``window=5``: Amount of words before and after the word in question which are to be taken into account.\n",
    "* ``min_count=5``: We don't want very infrequent words to be taken into account. All words with a frequency below the value specified will be ignored.\n",
    "* ``max_vocab_size=None`` Limits the RAM during vocabulary building. Not relevant for our puroses.\n",
    "* ``sample=0.001``* To make training faster, the number of training examples is decreased according to how frequently words appear in the corpus. That means that very frequent words are more likely to be delete as training examples (in some instances).\n",
    "* ``seed=1``: Used to ensure reproducibility. The default is 1 and we'll leave it at that.\n",
    "* ``workers=3``: Used for parallel computing. This specifies the number CPU cores to use for training in parallel. \n",
    "* ``min_alpha=0.0001`` The learning rate (specified by the alpha parameter) will linearly drop to ``min_alpha``during training.\n",
    "* ``sg=0``: Which model to use: 0 = CBOW (Continuous Bag of Words), 1 = Skip-gram.\n",
    "* ``hs=0``: How the training works: 0 = negative sampling will be used, 1 = hierarchical softmax will be used.\n",
    "* ``negative=5``: If negative sampling is used, this parameter specifies how many noise words should be drawn.\n",
    "* ``ns_exponent=0.75``: Value shapes the negative sampling distribution.\n",
    "* ``cbow_mean=1``: Idk what this is.\n",
    "* ``hashfxn=<built-in function hash>``: Hash function can be provided to increase training reproducibility. Probably not super important for our purposes, but it is an option.\n",
    "* ``epochs=5``: Number of iterations (epochs) that are run over the entire corpus. The keyword was formerly called ``iter`` and this old keyword appears in a lot of tutorials, so be cautious that the name changed.\n",
    "* ``null_word=0``: Keyword is not explained in the documentation. \n",
    "* ``trim_rule=None``: Rule specifying how to handle infrequent words. The default is: discard if word count < min_count; this is a straightforward and usefule rule and we will therefore stick with the default.\n",
    "* ``sorted_vocab=1`` The default sorts the vocabulary in descending frequency before assigning word indexes. The documentation of this keyword is sparse, but it seems that for the keyword does not make a difference for the quality of the results, so we will stick with the default here.\n",
    "* ``batch_words=10000``: Batch size.\n",
    "* ``compute_loss=False``: Computes and stores the loss value if the keyword is set to ``True``.\n",
    "* ``callbacks=()`` If you want to print the loss after each epoch, you have to add callbacks.\n",
    "* ``comment=None``: Not specified in the documentation and I couldn't find anything on Google either.\n",
    "* ``max_final_vocab=None``: Limits the target size of the vocabulary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
