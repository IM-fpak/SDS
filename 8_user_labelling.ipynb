{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label twitter accounts\n",
    "\n",
    "This notebook exemplifies how we labelled users based on the predicted tweet labels from the supervised learning classification. We ran this code on all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "os.chdir(r'C:\\Users\\maril\\Documents\\20-21 KU\\block 4\\DM\\twitter\\network_analysis\\automated_network\\labelled_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the datasets and join them \n",
    "\n",
    "# load manual lables (from AL) and prepare to merge \n",
    "dataM = pd.read_csv(r'pl_labels.csv', lineterminator='\\r', dtype={'id':str})\n",
    "dataM = dataM.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "# load manual labels (from testing clf) and prepare to merge \n",
    "dataMT = pd.read_csv(r'pl_labels_done.csv', lineterminator='\\r', dtype={'id':str})\n",
    "dataMT = dataMT.drop(['Unnamed: 0', 'prediction', 'evaluation'], axis = 1)\n",
    "dataMT.rename(columns = {'target':'label'}, inplace = True)\n",
    "\n",
    "# load the predictions and prepare to merge \n",
    "dataP = pd.read_csv(r'pl_labels_pred.csv', lineterminator='\\r', dtype={'id':str})\n",
    "dataP = dataP.drop(['Unnamed: 0'], axis = 1)\n",
    "dataP.rename(columns = {'prediction':'label'}, inplace = True)\n",
    "\n",
    "# merge the df's\n",
    "dfs=[dataM,dataMT,dataP]\n",
    "df = pd.concat(dfs, axis=0, join='outer', ignore_index=True)\n",
    "\n",
    "# Find duplicates (if you wanna have a look at them, remove the hashtags)\n",
    "#duplicates = df[df.duplicated(keep='last')]\n",
    "#print(duplicates) \n",
    "\n",
    "# drop duplicates \n",
    "df=df.drop_duplicates(subset=\"id\")\n",
    "df = df.drop(['index'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the length of the dataframe\n",
    "print(len(df))\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory\n",
    "os.chdir(r'C:\\Users\\maril\\Documents\\20-21 KU\\block 4\\DM\\twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all info to all the retweets\n",
    "df_rt = pd.read_csv('final_data_prepare1\\pl_all_text.csv', dtype={'id':str})\n",
    "df_rt = df_rt[df_rt['is_retweet'] == 1]\n",
    "df_rt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add labels to this dataframe\n",
    "rt_labels = []\n",
    "c=0\n",
    "\n",
    "for text in df_rt['text']:\n",
    "    c += 1\n",
    "    tup = tuple(df.loc[df['text'].str.contains(re.escape(text))==True]['label'])\n",
    "    \n",
    "    try:\n",
    "        rt_lab = tup[0]\n",
    "        rt_labels.append(rt_lab)\n",
    "    \n",
    "    except IndexError:\n",
    "        rt_labels.append('error')\n",
    "        print(tup)\n",
    "        \n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the list back to the df\n",
    "df_rt['label'] = rt_labels\n",
    "df_rt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframes back together\n",
    "df_final = pd.concat([df_rt, df], join='outer', ignore_index=True)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick labels by majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### make decision rule for vaxx/anti-vaxx/neutral/trash\n",
    "group_df = pd.DataFrame(df_final.groupby(['user','label'])['text'].count()).unstack()\n",
    "group_df.columns = group_df.columns.droplevel(level=0)\n",
    "group_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store the labels in\n",
    "labels = []\n",
    "\n",
    "# we iterate through the rows of the topic_docs dataframe\n",
    "for row in group_df.iterrows():\n",
    "    \n",
    "    # use the .idxmax() method which returns the index (= topic) of the\n",
    "    # topic with the highest probability; we add the index to the list\n",
    "    labels.append(row[1].idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the users and the topics together in a dataframe\n",
    "user_labels = pd.DataFrame(group_df.index)\n",
    "user_labels['labels'] = labels\n",
    "\n",
    "# check the results\n",
    "print(user_labels.shape)\n",
    "user_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "user_labels.to_csv(r'network_analysis\\automated_network\\pl_labelled_users.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
